In this Learning Module, we will introduce cloud systems architecture. We will quickly cover many areas that will be explored in further detail in other Learning Modules. Cloud architectures can change regularly, with many potential designs to fit an organizations needs or desires. We will introduce the balancing act of cloud design and introduce many common patterns and controls for cloud architecture.

There is no one-size-fits-all for cloud architecture. Organizations typically need to iterate on a design, so that it evolves with the organization's needs. If an organization deviates too much from this design, including the security controls, its ability to protect against and respond to incidents can become extremely time consuming and expensive, or even impossible.

In this Learning Module we will cover the following Learning Units:

- Cloud Architecture Definitions
- Cloud Architecture Design Goals
- Cloud Designs and Patterns
- Common Security Controls
- CNCF Landscape

Each learner moves at their own pace, but this Learning Module should take about 5 hours to complete.

## 1.1. Cloud Architecture Design Goals

While it may be easy to use cloud providers, that doesn't mean we have a cloud system. In order to make a cloud system that has security and supportability, we must consider the design of the cloud. There may be an overwhelming amount of choices to make when building a cloud, but making the most difficult choices ahead of time enables engineers and developers to work cohesively within the agreed upon rules.

Without the rules of the architecture established, well understood, and followed, clouds may become extremely costly, insecure, and difficult to support. Each organization must identify the budgets and goals, then build designs that align with them. We will begin to explore cloud architecture design evaluation processes and goals in this Learning Unit.

This Learning Unit covers the following Learning Objectives:

1. Identify the balance of in-house, community, and proprietary components
2. Introduce SLO and measuring success
3. Understand resilience and reliability by design
4. Learn about balancing security, availability, and experience
5. Design for iteration on applications and systems
6. Design with defense in depth

## 1.1.1. Component Selection

Selection of components isn't just a one-time decision, but a continuous one that takes place as various systems and software are evaluated. The architecture of a system can be considered the choices that are harder to change, the patterns we follow as we make implementation decisions. There are components that are architecture components that require group consensus, the elements of the shared understanding between employees on the way systems work, and work together. There are also components that are not so architecturally bound. These non-architecturally bound components are often specific to a function or vendor, fitting well within the architecture.

An example of an architectural component would be the logging systems. We need all components to be aligned to the logging format and storage requirements, sending logs in the appropriate format to the appropriate destination. An example of a non-architecturally bound component might be an open source software library used for custom error formatting. While the library should be used in alignment with the rest of the architecture, that doesn't mean it can't be replaced with another library that accomplishes the same function.

Selecting the architecture components is a critical set of decisions for the organization. We can choose to develop components within the organization (in-house), use open source components, or purchase proprietary components. Open source components are generally the lowest cost, allowing us to leverage the work of the open source project as our _upstream_,[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1335-1) and implement those components in our systems. Using just any open source project can be recipe for disaster. However, Cloud Native applications exist that can help with this.

_Cloud Native_ means that a particular application or system was designed for distributed systems. _Certified Cloud Native_ means that the _Cloud Native Compute Foundation_ (CNCF)[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1335-2) has reviewed the software, announced its availability, and the software aligns to the community-desired standards. It is not required that software is certified as cloud native with the CNCF for it to be cloud native or for it to be successful; however, if the intent is that the software becomes part of the larger cloud ecosystem, then getting it into the CNCF process is a good idea. We'll discuss CNCF in detail later.

Cloud native code should be fault tolerant, designed to run in a distributed manner, use secure coding practices, be partially or fully open source, avoid design choices that go against our goals, allow for interoperability with existing cloud standards, and offer well-maintained documentation and security updates. Proprietary cloud native software does exist; however, its rate of adoption may be lower than open source software. Cloud providers commonly include proprietary software that is built on top of open source code in their service offerings.

If we are choosing to build with open-source projects, it is a good idea to primarily stick with CNCF components, so we have some assurance of quality and compatibility. That doesn't mean we need to use 100% CNCF components only, but leveraging those components can be efficient and help reduce time to market.

While paid proprietary products and services can be tempting for managers and decision makers, they create architectural inertia, meaning they pull the design in a direction, potentially giving control and visibility away to a third party. There are situations when third parties make sense, especially when it comes to resources that the organization doesn't have. If the organization doesn't have datacenters or resources to build and maintain datacenters, then it will be required to pay a third party to provide and manage them. These third-parties may provide a service offering for a particular need of an organization.

These service providers and IT vendors have been leveraging virtualization and containerization technologies along with automation to create _Infrastructure-as-a-Service_ (IaaS) offerings. IaaS provides infrastructure (such as virtual machines) to a customer with something like a click of a button or a simple request.

While platforms and infrastructure are commonly offered as-a-service, anything we imagine could be as-a-service. All businesses provide a service at some level, whether manufacturing, development, logistics, consulting, etc. What typically distinguishes x-as-a-service from simply "selling x to customers" is that the "service" is already fully automated and available on demand, often allowing customization and controls by the customer. Typically this means that IaaS providers develop web GUI and/or APIs to allow customers to create, customize, and destroy that infrastructure when they want to, without the need for people to be directly involved.

The idea of having an administrative team delicately configuring the systems made much more sense when operations were focused around the hardware itself because a mistake could be very expensive. As systems designs have progressed to virtual machines, containers, and now IaaS, operations are further removed from hardware administration. This separation enables more aspects that used to be only represented physically to be represented in code that can on-demand construct the virtual machines, containers, as well as bare-metal (direct to hardware) in some cases. This is commonly referred to as _Infrastructure-as-Code_ (IaC). IaC means that we are writing all of the configuration and using the chosen tools to create the infrastructure from code.

What we are commonly talking about is IaC, which has the operating systems and their configurations as code. Let's review an example of an IaC tools stack. These tools have much of the hard work done for us, so we can write in configuration files instead of having to engineer these IaC components from scratch.

|Systems Component|Example IaC tool|
|---|---|
|Boot medium|Hashicorp Packer|
|Provisioning|Hashicorp Terraform|
|Configuration|Ansible|
|State enforcement|Monit|

> Table 1 - Example IaC component stack

The _boot medium_ is the install disk that installs the operating system to hardware or virtual machine. To make the boot medium as-code, we need to write up the automation to generate the boot medium file as code. A tool like _Hashicorp Packer_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1335-3) does exactly this.

The _provisioning_ is the process of allocating hardware to an operating system and installing the operating system. To make the provisioning as code, we need to write the automation for the installation process of the operating system as code. _Hashicorp Terraform_[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1335-4) can be used for this.

The _configuration_ is the installation and configuration of files within the operating system after it is installed. To make the configuration as code we need to write the automation of the configuration of applications and operating systems as code. A tool like _Ansible_[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1335-5) can be used for configuration.

The _state enforcement_ is ensuring that the desired state of the machine is maintained and does not drift. To make state-enforcement as code, we need to write the automation for enforcing the state of our operating systems and applications. A tool like _Monit_[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1335-6) may be used for this.

All of these layers combined provide a full picture of infrastructure as code, allowing entire datacenters to be represented in text files that are the code and configuration files that are needed to build everything in that infrastructure from the operating systems up.

As an organization grows, its cloud components need to be continually re-evaluated. As the resources within the organization grow, some components that used to be outsourced to third parties may be able to be dropped as the organization can afford to maintain those. The cost of a component such as a software platform or datacenter is typically several times higher for bringing the development in-house, and even some smaller components can end up costing millions in maintenance over time.

To explain this further, we can use GNU/Linux as an operating system for free, but if we wanted to build our own operating system from scratch, that could cost millions of dollars to do well. Another example is cloud infrastructure, including the datacenters, hardware, networking, and maintenance. We can leverage public cloud infrastructure for a few dollars per month, but building and maintaining our own cloud infrastructure could cost millions.

Short term and long term costs are difficult to estimate; however, estimating those costs is a great way to evaluate whether a given component should be in-house or vendor-provided. Along with costs and available resources to the organization, another critical consideration is the _Software Development Life Cycle_ (SDLC)[7](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1335-7) of the component. The SDLC is the writing and testing of code, publishing and distribution, maintenance, and deprecation of components. If a component doesn't have a healthy SDLC, it carries a much higher risk to security vulnerabilities, supply chain attacks, and other issues.

A component with a good SDLC has engaged developers, ensuring that the component is meeting the needs of the users. While it is possible that a component with no SDLC will do fine untouched for the rest of its use, long into the future, that type of component is likely to be a simple, well-designed program with a more trivial code base. A component that doesn't have an SDLC at all may also be _End-of-Life_ (EOL), meaning it is no longer maintained.

The SDLC of proprietary products might be better or worse than a given open source equivalent, so we should investigate how frequently patches and bug fixes are developed and delivered, as well as the operational costs of making changes to that component for updates.

Large open source projects often have the best SDLC, but well-funded proprietary products can sometimes also do very well. Even some small open source projects do well in terms of SDLC, although there is some risk if only one person is able to make a type of patch or bug fix. When we select and review components, identifying weak spots in the SDLC and starting planning for alternative components, or creating an SDLC for it, can save millions in long term costs.

Not only do external components have SDLC, but internal components also need SDLC. We will rarely be in a position with cloud architecture where we can create something and never touch it again. There are some components that may be like this, such as small shell scripts, but more often than not, components need to be actively reviewed and maintained. Each internal component or product should have a defined SDLC and release process, not only to ensure that changes are auditable, but so bugs and vulnerabilities are being fixed appropriately.

1

(Redhat 2022), [https://docs.fedoraproject.org/en-US/package-maintainers/Staying_Close_to_Upstream_Projects/](https://docs.fedoraproject.org/en-US/package-maintainers/Staying_Close_to_Upstream_Projects/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1335-1)

2

(CNCF, 2022), [https://www.cncf.io/](https://www.cncf.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1335-2)

3

(Hashicorp, 2022), [https://www.packer.io/](https://www.packer.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1335-3)

4

(Hashicorp, 2022), [https://www.terraform.io/](https://www.terraform.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1335-4)

5

(RedHat, 2022), [https://www.ansible.com/](https://www.ansible.com/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1335-5)

6

(Tildeslash, 2022), [https://mmonit.com/monit/](https://mmonit.com/monit/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1335-6)

7

(NIST, 2022), [https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-218.pdf](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-218.pdf) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1335-7)

#### Labs

1. What does SDLC stand for?

Answer

## 1.1.2. Integration and Deployment

Once components are selected, we'll need to find a way to integrate and deploy the application to various environments. Commonly we would have a development, testing, staging, and production environment or cluster. Developers work in the development environment where the application is running the latest version, which might also be more buggy. _Quality Assurance_ (QA) testing is typically done in the testing environment. These two environments will typically deviate from production to lower costs and make it easier to test. The staging environment typically mimics production as closely as possible but is still a testing environment. Production, of course, is the environment the end users interact with.

Manually creating and maintaining these environments can become a major hassle for systems engineers. In addition, manual changes and upgrades can lead to human errors, which can cause security vulnerabilities as well. To remedy this, many organizations utilize a _Continuous Integration and Continuous Deployment_ (CICD) pipeline to build the environments.

_Integration_ is the act of incorporating changes and new components into the systems. _Deployment_ is the movement and configuration of applications and systems from developer code, to running production systems. CI and CD are very popular because they include automation and testing of changes. An organization doesn't have to have CICD implemented, but likely wants to. We may choose to use just CI and not CD, or just CD and not CI; however, we most commonly want them both.

CICD (also written CI/CD) is not a single tool or standard, although some single tools can provide CICD. More commonly, CICD will be accomplished by a selection of tools and services that are used together to test, measure, and scan code changes, as well as deploy and validate code changes into different systems. CI builds and tests, while CD deploys. There are also vendor systems that provide mostly ready-to-go CICD solutions.

Confusingly, there are two slightly different CD abbreviations: continuous deployment and continuous delivery. When we refer to CICD, we are often really referring to the second abbreviation: continuous delivery. While they are both closely related, continuous delivery also includes strategy for how the application is deployed. Continuous delivery may involve mechanisms like production testing, green/blue deployments, and other aspects that comprise a strategic application delivery system.

_GitOps_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1287-1) is a type of continuous delivery strategy. There are a few standard ways to accomplish GitOps, but in short, GitOps automatically integrates and deploys code based on the contents of a git code repository. Using a technique called _Source Code Management_ (SCM),[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1287-2) automation systems reach into the git repository, check for changes, then automatically deploy and test any changes found.

![[OffSec/Cloud/Cloud Essentials/z. images/d1d7980b5b42b5f424c276f72d11dac5_MD5.jpg]]

Figure 1: Example CICD diagram

An organization doesn't have to use GitOps to be successful; however, it is a very common design pattern because it pushes the delivery automation to do everything, so that developers code is tested and deployed automatically. What GitOps needs to really be successful is thorough security considerations through each step of the process. The testing of the code ideally includes security scanning and testing as well.

One of the common problems with GitOps is that occasionally, users mistakenly insert a secret into the git repository. This type of mistake is common across more situations than GitOps designs, but might be even more likely in a GitOps design. Any information we don't want to be public is considered either secret or sensitive in nature. A _secret_ is different from _sensitive data_ in this context. Secrets are encryption keys, passwords, and access credentials, while sensitive data is anything else like _Personally Identifiable Information_ (PII)[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1287-3) that we don't want exposed. We especially want to keep secrets and sensitive data safe from adversaries.

The more information an adversary has, the more they have to work with for researching and attacking. Any amount of information can aid an adversary; however, secrets and sensitive information are the types of data adversaries seek, as they provide benefit to the adversary objectives.

Secrets in the hands of adversaries give them more valid access. Even if the component is protected behind several other components, each secret builds up an adversary's capabilities. Sensitive data such as phone numbers and email addresses contribute to an adversary's ability to conduct social engineering, phishing, identity theft, extortion, or blackmail.

It isn't just cloud systems that struggle to protect secrets due to plaintext storage or transmission issues. All computer systems have these weaknesses, and there are multiple ways to approach handling them. A common architecture pattern is to have centralized authorities for various scopes. Determining that scope is a critical architecture choice. We might have one scope of centralization for systems that are not related to production and are internal only, and another scope of centralization for external customer-related systems.

The scope of centralization may correlate with scope of cryptographic management processes. _Public Key Infrastructure_ (PKI)[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1287-4) is a loose term to describe the people, processes, and technologies that relate to the management of cryptographic components. PKI systems involve the management of cryptographic keys, signing of certificates, distribution of certificates, revocation of certificates, and more. A _Certificate Authority_ (CA)[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1287-5) is a cryptographic component that signs certificate signing requests within a PKI scope. Certificates are in two main categories: public trust and private trust. Public trust certificates are signed by a public certificate authority, while private trust certificates are signed by a private certificate authority. If an application or system is accessed by the public or general internet, then it will use a public CA, while systems that are internal only may use either a public CA or a private CA. While the scope of centralization doesn't have to align completely with the scope of a PKI system, it can be useful in the design.

With centralization and PKI scopes aligned, we have unified identity within that centralization scope. The result of this is that management of data and identity are homogeneous within that scope. A _Hardware Security Module_ (HSM)[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1287-6) might have a certain scope with a PKI internal CA for example, and then a secret management system might have a scope for application secrets, while another system has employee secrets and other end-user secrets.

Even with comprehensive secret management systems, all it takes is one trusted employee to divulge, expose, leak, or sell credentials to cause severe damage. An employee could also enter a long-lived password in a text file on their laptop. The strength of the organization depends on the strength of its weakest links in addition to the primary controls.

1

(GitLab B.V, 2022), [https://about.gitlab.com/topics/gitops/](https://about.gitlab.com/topics/gitops/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1287-1)

2

(Atlassian, 2022), [https://www.atlassian.com/git/tutorials/source-code-management](https://www.atlassian.com/git/tutorials/source-code-management) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1287-2)

3

(NIST, 2022), [https://csrc.nist.gov/glossary/term/personally_identifiable_information](https://csrc.nist.gov/glossary/term/personally_identifiable_information) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1287-3)

4

(Digicert, 2022), [https://www.digicert.com/what-is-pki](https://www.digicert.com/what-is-pki) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1287-4)

5

(Digicert, 2022), [https://www.digicert.com/blog/what-is-a-certificate-authority](https://www.digicert.com/blog/what-is-a-certificate-authority) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1287-5)

6

(NIST, 2022), [https://csrc.nist.gov/glossary/term/Hardware_Security_Module_HSM](https://csrc.nist.gov/glossary/term/Hardware_Security_Module_HSM) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1287-6)

#### Labs

1. In GitOps, which program does a developer use to initiate a deployment?

Answer

2. For GitOps to be successful, we should include ________ tests as well as application tests. Fill in the blank.

Answer

3. What does SCM stand for?

Answer

4. A street address paired with a name is considered what type of data?

Answer

## 1.1.3. Resilience and Reliability

Cloud systems are expected to withstand single-point outages. If a system cannot endure most single-point failures, then it is not likely a cloud system. It is rare that an entire organization operates using 100% cloud architecture, but for this reason, doing so is a valuable posture to have.

A _single point of failure_ is one location of outage. An example of a single point of failure could be a network outage in a datacenter, a broken application deployment, a database, or a disk failure. We want to design the overall system so that any single point of failure has a _failover_, or some mechanism so that business can continue with that aspect down. This is possible even with applications and databases by having entire separate copies and sets of production. These additional redundant systems exist entirely to avoid impact to business operations.

Not every point has a failover, and that is normal. But we should strive for this nevertheless, and consider the value of redundant systems and multiple productions with automatic failure detection on the primary systems. With automatic failure detection, the system can detect when a failure has occurred and take action to route requests away from the failure.

Reliability is the concept of how available a service is. If we leverage proper redundancy, a service might be close to 100% reliable. Without redundancy it is unlikely that we will achieve high reliability, although it is possible.

One example that has been historically difficult to ensure resilience and reliability is the initial access into an application. We can have failovers for certain parts of the application or backups for databases, but a single point is traditionally needed to allow the user to access the application. However, we can use Global Server Load Balancing to distribute this inital access.

_Global Server Load Balancing_ (GSLB)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1292-1) describes how network and application traffic is distributed effectively across the globe. GSLB can be thought of as an enhancement to DNS, updating the DNS record very quickly when an issue is detected, or based on other criteria such as geographic location. Because of the frequent relation of GSLB to DNS, GSLB is sometimes called _intelligent DNS_. GSLB devices are also sometimes referred to as _Global Traffic Manager_ (GTM) devices.

![[OffSec/Cloud/Cloud Essentials/z. images/540310b490db7c2ef73f5dac71c585bb_MD5.jpg]]

Figure 2: Example GSLB diagram

While most GSLB is accomplished by publishing DNS _A records_ based on health check results and/or geo-tagging, GSLB can also be accomplished in other ways. GSLB is often the single-most important aspect making a system a cloud system. Without GSLB, failover at the DNS level may be slow and ineffective, causing longer outages until the DNS _Time-to-Live_ (TTL) expires. If availability is a priority, so is GSLB.

Many cloud providers have built-in GSLB solutions; however, many of them don't enable it by default. This is perhaps the most common mistake in cloud design: customers thinking they are leveraging cloud by using a cloud provider, not realizing they need to configure the cloud provider's GSLB in order to leverage cloud benefits properly. If a customer of a cloud provider installs everything to a single geographic zone, then they are not fully using cloud computing. In order for a system's design to be _cloud_, it must not rely on a single physical system or network.

Setting up GSLB in a private network is more time consuming, and often more expensive. Typically, two separate ISP[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1292-2) connections need to be used, and failover between GSLB nodes across the _Wide Area Network_ (WAN)[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1292-3) needs to be configured as well, so that if the primary GSLB device has an issue, there is automatic failover at that level too.

There can be significant complexity and effort required to configure and validate privately-owned GSLB: the costs of running two datacenters with two ISPs, the costs of WAN appliances and circuits to create an effective _WAN mesh_, likely dealing with _Border Gateway Protocol_ (BGP)[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1292-4) peering, and more. Many public cloud providers take on these aspects for us, as they own and administrate the datacenters and those underlying networks. We can leverage the work the cloud providers do, and create GSLB configurations on top of their WAN engineering, sometimes as easily as marking a single checkbox or config value.

Leveraging the work of cloud providers is the easiest way to improve reliability of a systems architecture. In order to use the load balancing, auto-scaling, and distributed nature of cloud providers, we need to design our system with components that are short-lived and temporary.

For example, a retail organization might want to scale up the amount of servers for a large sale and then destroy those servers after the sale to save on costs. It would be a significant effort for this organization to manually create each server in each geographic location, manually add them to the load balancer, and undo all these changes afterwords. Instead, if each server was designed to be easily and quickly spun up and spun down, the organization would be able to scale much more quickly. These temporary, short-lived, systems are called _ephemeral_[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1292-5) systems.

As more components have become code-based, abstracting further away from hardware, it has become increasingly easier to create ephemeral systems. Ephemeral means capable of existing for short periods of time and then vanishing. Ephemeral systems can be created and destroyed repeatedly, because they rely on other systems or code that can create and destroy them quickly. Not all ephemeral systems are equally ephemeral. Some systems will classify a very specific type of service as ephemeral, even when all of the services are ephemeral to some degree.

The smaller the system, the more ephemeral it can be. For example, containers that run a very specific function can be more easily created and destroyed than a massive virtual machine that serves multiple purposes. _Serverless_[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1292-6) systems are an example of a small single purpose system designed to run a single function on demand and terminate.

Despite it's name, serverless doesn't mean there isn't a server involved, but instead means that the server isn't required to be customer-managed or customer-specific. Serverless is possible by having the server store the required customer function and host a separate API, and that API can create a very ephemeral instance that executes a custom function on demand, then terminates.

Many systems benefit from being designed to be short-lived. The benefit of making it possible for a system to be short-lived is that it can be faster to rebuild it, potentially decreasing maintenance complications. Some types of systems don't make sense to be ephemeral, such as authoritative databases, hardware security systems, and secret storage systems; however, it is possible to design such long-lived systems ephemerally, even if they are used in a more consistent manner.

Non-ephemeral components are often more risky because they may drift in configuration and grow in complexity over time, as well as being likely to house sensitive or critical data. Even if the non-ephemeral component needs to stay non-ephemeral, like a storage system for example, we need to be prepared to rebuild and maintain it effectively. For these reasons, non-ephemeral components are the most likely to rely on outsourced management.

However, there needs to be a balance in the design when dealing with ephemeral systems. Over-engineering a system is a real concern and simplicity needs to be balanced in.

Simplicity is one of the most important concepts in systems design. Every component of a system is a point of failure, so the fewer points of failure there are, the more reliable a system can be. We don't want to just focus on simplicity in such a way that doesn't consider the required complexity. There is a vast amount of complexity in modern systems, and we don't want to try and reduce them in ways that break the resiliency and redundancy. Instead, we consider the requirements and focus on keeping the design as simple as possible within these restraints.

In addition to simplicity, reliability also comes from good application design. Systems should be both well designed and as simple as possible, to ensure the highest rate of reliability. In addition to being simple and well designed, the more testing a system has, the greater the reliability. This is the case because testing reveals mistakes in the design, security vulnerabilities, performance, and stability issues that can then be addressed prior to full release into production.

1

(cloudflare, 2022), [https://www.cloudflare.com/learning/cdn/glossary/global-server-load-balancing-gslb/](https://www.cloudflare.com/learning/cdn/glossary/global-server-load-balancing-gslb/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1292-1)

2

(Merriam-Webster, 2022), [https://www.merriam-webster.com/dictionary/Internet service provider](https://www.merriam-webster.com/dictionary/Internet%20service%20provider) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1292-2)

3

(Merriam-Webster, 2022), [https://www.merriam-webster.com/dictionary/wide area network](https://www.merriam-webster.com/dictionary/wide%20area%20network) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1292-3)

4

(IETF, 2006), [https://www.rfc-editor.org/info/rfc4271](https://www.rfc-editor.org/info/rfc4271) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1292-4)

5

(Merriam-Webster, 2022), [https://www.merriam-webster.com/dictionary/ephemeral](https://www.merriam-webster.com/dictionary/ephemeral) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1292-5)

6

(RedHat, 2022), [https://www.redhat.com/en/topics/cloud-native-apps/what-is-serverless](https://www.redhat.com/en/topics/cloud-native-apps/what-is-serverless) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1292-6)

#### Labs

1. If a database going offline causes the product to crash and not recover on its own, then the database is considered what?

Answer

## 1.1.4. Balancing Controls

Controls are components that an organization leverages to protect itself from vulnerabilities and adversaries. Most threats to an organization come from within, whether malicious or accidental. Controls not only make it difficult for attackers to interfere with the organization, but also make it more difficult for a single individual within the organization.

While controls are an important element of systems architecture, we must also consider the balance of controls and functionality. All the controls we want might make the system intolerable for people to use, so we must consider balancing the controls with ease of use.

In order to determine which areas are tightly-controlled and which are more lax, we can leverage a variety of evaluations that are ideally conducted routinely. From threat modeling, intelligence gathering, load testing, chaos testing, business requirements, user experience designs, and compliance requirements, many factors and processes may be used to determine which controls are placed into a given system.

As an example, Sarah is a cloud architect for a military contractor. The cloud systems Sarah designs are private clouds that run on the battlefield, using proprietary network protocols to link weapon systems to satellites as well as military analysts, both on the battlefield and at the military bases. Sarah receives reports from intelligence analysts and security researchers that she then takes and factors into her cloud systems architecture.

One of the intelligence analysts discovered that an adversary has reverse-engineered the proprietary network protocol used in Sarah's design, and has built systems that are attempting to link to the cloud system. A meeting is called regarding this news, and after teams apply the adversary's technique, they find that despite the secret protocol being reverse engineered, it won't enable the adversaries to connect because the adversary also needs a valid token.

If the adversary is able to obtain the equipment of one of the troops, they could extract the token and then use it to join the cloud system. After this is realized, Sarah determines that the lifespan of these tokens must be shortened from one hour to thirty seconds, and that the system must not allow token re-use at all. Further, equipment must have a remote failsafe so that it can be marked as no longer valid, encrypting all of the configuration files within.

This fictional example lightly describes how threat modelling and intelligence gathering resulted in a change to the cloud systems architecture. Let's explore another fictional example, this time regarding chaos testing.

Mike is a cloud architect for a logistics company that is responsible for delivery of food to distributors. The company has a mature design running on a public cloud that allows satellite systems on each truck, plane, and ship to connect to the public cloud, centralizing detailed metrics and analytics on the shipments.

The company has designed chaos testing that disconnects the primary datacenter and then reconnects it repeatedly to simulate a network condition called _flapping_. The result of the chaos testing showed that when the flapping was occurring, the failover between primary and secondary datacenters was triggered but was slow and never completed: the primary went down, failover initialized, but before failover finished, primary was up, so the system started failing over back to primary.

The result of this was that the trucks, ships, and planes were not able to transmit metrics during the simulation, but also that the systems on the transports crashed if the flapping lasted for more than five minutes.

Mike took this test data and redesigned the failover so that if more than three failovers back and forth between the same datacenters occur within two minutes, a hold would be placed on the secondary system for ten minutes. This change to the cloud architecture avoiding prolonged outages if the primary flaps, and avoided the crash condition on the transporter. Mike also sent the relevant data to the transport applications developers to work on fixing that type of crash in the application.

Both of these fictional examples explain how we can use data to refine security controls and systems designs. How this type of design and controls iteration is done, as well as the types of data included, varies between organizations.

A large factor on how often and how well organizations iterate in this way is the budget of the organization. Controls can be low cost or very costly, from zero to billions of dollars. The security budget of the organization greatly shapes the controls an organization implements. An organization with a large budget might have 24/7 security operations staff, including armed physical security, CCTV cameras, employee access telemetry, AI/ML security systems, well defined security tactics, processes, and procedures, as well as dozens of other software controls. An organization with no security budget might only have some part time employee efforts and some quickly configured free software tools.

Regardless of which controls we have at a given moment, we want to always be reviewing and considering those controls. Controls only need to be strong enough for a given moment in time, no control is perfect and no control is impossible to bypass. Perfection is not attainable in security, only sufficient balance.

## 1.2. CNCF Landscape

The cloud native world is vast, with many thousands of people and organizations involved in creating components we can use to create our clouds. As we have learned, selecting which components to use is an important part of the process. We will look at an example component selection, as well as where to find the larger cloud native component lists.

In this Learning Unit, we will introduce the organizations and big picture structures that make up the cloud native world.

This Learning Unit covers the following Learning Objectives:

1. Learn what the CNCF is
2. Learn what the Linux Foundation is
3. Review the CNCF landscape
4. Learn to stay in tune with relevant CNCF news

## 1.2.1. CNCF Introduction

The _Cloud Native Compute Foundation_ (CNCF) is a an organization that exists to foster and cultivate cloud native software. The CNCF has a process that _incubates_ then eventually hopefully _graduates_ code into being certified cloud native.

We don't have to use the certified cloud native ecosystem to create clouds, although it will likely be much more effective to do so. We could just engineer a few C programs and Bash scripts and create clouds entirely from that, or use whatever languages we want to develop software that is resilient, encrypted, distributed, and provides acceptable performance.

The benefits of the CNCF ecosystem are great, allowing organizations to focus on applications or services rather than engineering all of the networking required for these components to work well together. While Kubernetes may be the center of this CNCF ecosystem, other approaches are viable as well. We don't have to use Kubernetes to leverage CNCF software, although doing so expands our options.

Let's review an example CNCF stack.

|CNCF component|Purpose|
|---|---|
|Envoy|GSLB|
|Calico|networking and security|
|Falco|security|
|Jenkins|CICD|
|k6|CICD performance testing|
|Kubernetes|systems and network platform|
|Prometheus|observation and analysis|
|Redis|caching|
|Cassandra|database|
|MySQL|database|
|Ceph|storage|
|Rook|ceph storage orchestration|
|Etcd|service discovery + storage|
|CoreDNS|service discovery + storage|
|Vault|key management|
|Consul|key management HA storage|

> Table 2 - Example CNCF component stack

In this example stack, everything runs with Kubernetes. We can build out an abstraction of our entire IT infrastructure using Kubernetes configuration files and scripts. Then, _Jenkins_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1296-1) can be configured with Kubernetes to use GitOps and automatically deploy applications and services for the organization when code changes in the git repository.

1

(Jenkins, 2022), [https://www.jenkins.io/](https://www.jenkins.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1296-1)

## 1.2.2. Review the CNCF Landscape

The _CNCF landscape_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1297-1) is a representation of the CNCF cloud native ecosystem. We can use the CNCF landscape to investigate alternative providers and software based on category. While the CNCF landscape may be overwhelming, it provides comprehensive sorted lists of certified, incubating, and graduated software. We should note that the grey boxes in the landscape shown below represent items that are not open source.

![[OffSec/Cloud/Cloud Essentials/z. images/b42ddc28cd7cb502dbc79a84ae86bec1_MD5.jpg]]

Figure 3: CNCF landscape snapshot

We don't need to know everything in the CNCF landscape; however, being vaguely in-tune with it is beneficial when working in cloud computing. The CNCF landscape changes regularly with new providers and change in status of various software, whether it is new items being added or items moving from incubating to graduated. Between the time the image above was captured and the time this Learning Module is read, there may have been changes to the CNCF landscape.

With the frequent changes to the landscape, we might want to check on it periodically for awareness of changes that might provide useful insight for the organization. We can take the arrivals and changes on the CNCF landscape as an indicator to evaluate or review those components to discover whether or not they might be beneficial to the organization.

1

(CNCF, 2022), [https://landscape.cncf.io/](https://landscape.cncf.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1297-1)

## 1.2.3. Linux Foundation

The _Linux Foundation_ (LF) is a standards organization of over 1000 corporate members (including Microsoft). The LF created the CNCF, and accelerates open source initiatives.

More examples of LF projects include _OpenPrinting_,[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1298-1) _Servo_,[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1298-2) _Xen_,[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1298-3) _RISC-V International_,[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1298-4) _Real-Time Linux_,[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1298-5) _Open Mainframe Project_,[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1298-6) and many more. In addition to the many important projects, the LF also provides training and certifications for GNU/Linux and related technologies.

We don't need to interact directly with the LF; however, it is important to know the significance and industry impact of the LF. If an organization needs general Linux-related technology training, the LF is a great resource.

1

(Linux Foundation, 2022), [https://openprinting.github.io/about-us/](https://openprinting.github.io/about-us/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1298-1)

2

(Linux Foundation, 2021), [https://servo.org/](https://servo.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1298-2)

3

(Linux Foundation, 2022), [https://xenproject.org/](https://xenproject.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1298-3)

4

(Linux Foundation, 2021), [https://riscv.org/](https://riscv.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1298-4)

5

(Linux Foundation, 2016), [https://rt.wiki.kernel.org/index.php/Main_Page](https://rt.wiki.kernel.org/index.php/Main_Page) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1298-5)

6

(Linux Foundation, 2022), [https://www.openmainframeproject.org/](https://www.openmainframeproject.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1298-6)

## 1.2.4. Change and Awareness

Systems design is more important than the software running within it. The systems architecture design provides the foundation for security, scaling, and organization success. Even if we have a great design and a successful product launch, that doesn't mean the design should be stuck exactly as it was during launch forever. If the design was to be frozen forever, the organization might fail to adjust as the technology landscape changes. Within a few years, employees might feel like they are working on a piece of history rather than the cutting edge. While that feeling may be okay and justified, often the most talented cloud employees will seek progression of technology and companies that enable them to facilitate improvements.

While it is possible for architectures to change very little or remain unchanged for many years, we no longer need to think so rigidly, especially for cloud computing. Our ability to greenfield and implement new designs and enhancements is greatly improved in cloud architecture compared to purely hardware or VM-based architecture. In a software-defined world, we can let engineers innovate within the cloud software in a complete greenfield replica of the infrastructure.

In order to make architectural changes effectively, we must be aware of changes in the related technologies, and have well established processes for approval, testing, review, and acceptance of changes to systems design. To maintain awareness about new features and changes to technology, it is a good idea to have at least one employee per component that keeps up with related news, vulnerability announcements, and other component research and review processes.

Ensuring that members of an organization are staying in tune with industry news and updates can be important to the success of an organization, cloud or not. There are data feeds, products, and services that can facilitate automated integration of new security information. In addition to building in automated security news integration to CICD and alerts, we might have a formal process for human review of important security news.

Designs and processes that empower software changes are accelerated by cloud computing architectures. While it can become easy to make changes, we also need to put strong security efforts into how changes are made and who can make them. We have made creating production systems easier with cloud architecture, but we have also made destroying production easier.

#### Labs

1. Ephemeral and as-code infrastructures are beneficial for many reasons, but perhaps the greatest benefit is the reduced time required to ______ after a disaster. Fill in the blank.

Answer

2. The systems software platform, not operating system or kernel, at the center of the cloud native ecosystem is what?

Answer

## 1.3. Cloud Designs and Patterns

When designing a cloud architecture, we don't have to start from scratch. Over time, certain design patterns have been created that we can reuse. These design patterns are best practices that have come out of trials and errors from other organizations.

This Learning Unit covers the following Learning Objectives:

1. Explore microservice architecture
2. Examine a Zero Trust architecture

## 1.3.1. Microservice Architecture

Microservices are not a single specification, but a design pattern. Microservice architecture is designed for loosely-coupled, purpose-built, modular services. Microservices work together to accomplish the overall service tasks, rather than having a single monolithic service doing everything. Microservices don't have to be small, but typically are. What is more important than the size of a microservice, is the function of the service in regard to the whole of the system.

One of the main benefits of microservices is that the code for each service can be independently modified or re-used in different ways. Let's take an example small set of microservices and walk through how they connect together. The example is for a fictional online retailer that has a website customers use to order clothes, as well as physical locations with employees filling the orders placed by customers online.

The first microservice we have is a gateway, a service that receives requests from both online shoppers and from employee back office systems. The online shoppers are routed to a website microservice that serves the website front-end, and the back office system requests are routed to a back office API microservice that can query and manage inventory and orders.

A _back office_ is run by employees of the store. Employees might have a GUI software tool installed on computers at the warehouse or office that is a client to the back office microservice, enabling them to check for new orders and update inventory with that tool.

The website microservice and the back office API microservices connect to several more microservices. The first one is a cache microservice that the website microservice uses to quickly store and retrieve data. We also have a product inventory microservice that uses a database to manage product inventory and interacts with the cache microservice to keep the product database and cache in sync.

The next microservice for our example is a customer account microservice that manages customer usernames and passwords, storing them in a separate database. The last microservice in our example is a customer orders microservice, which processes customer orders and stores related data in yet another database. If we had a monolithic service instead of microservices, all of these components might be inside in a single large service.

Going along with our example, we could take any one of those microservices and shut it down, and the other microservices wouldn't crash. They might not have all of the desired functionality available to them, but they can continue to run without other microservices online.

We could take the back office API microservice and replace it with a new back office microservice that uses the same API function names. If we preserved the functionality and function names in our replacement back office microservice, the rest of the microservices could use it without the need to make code changes to each microservice that needs to call it. Additionally, the back office GUI tool that employees use could also continue to work with the replaced back office API microservice.

Microservice architectures accelerate development and make it easier for developers to contribute. If new functionality not directly related to an existing microservice is needed, a new microservice might be made. Having the ability to plug and play with new components makes it easier for changes to be made. If developers were working on a singular monolithic service, even small additions could topple the larger functionality, potentially taking much more time to add.

Most clouds leverage microservice architectures, and cloud native applications are built to function as microservices. There are more microservice benefits than granularity and code re-use, such as improved horizontal scaling, which is the ability to create additional copies of microservices in order to take on more load. Rather than having to copy an entire large application thirty times to have enough scale for the load, we might be able to horizontally scale specific microservices and leave some at lower counts.

There are many security benefits that can come from the modular nature of microservices. We can apply hardening, networking, and isolation to each microservice rather than having to be more permissive with a larger application. Further, if one microservice has a vulnerability, we may be able to more easily patch that microservice compared to a singular large service.

On the downside of microservices, debugging can become more complicated, needing to trace transactions across different services to solve problems. Because of this tracing need, we want to leverage unique identifiers that are created at a per-transaction level that are carried across the microservices. In our example, the online store gateway microservice would generate a unique ID string and pass that along to the next service, and that service can then continue to pass that same ID along to any other service that it calls. Then, in our centralized logging system, we can query for a given transaction ID and retrieve all of the data across the microservices for that transaction.

The logging output of each microservice is critical, but we also don't want to expose everything in excruciating detail. Part of microservice design is what we describe as hiding complexity behind abstraction. The idea with this hiding is that the really complicated stuff can be within the microservice, and that other microservices and people should not need to know about that complexity in order to use or call that microservice. So rather than forcing each other microservice to learn all the inner workings of a new microservice to call it, we want to present a simple and standardized HTTP or _Remote Procedure Call_ (RPC)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1303-1) API, both over TLS, with functions that can be called using standard protocols and specifications. We commonly will use gRPC[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1303-2) for RPC APIs, and REST[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1303-3) for HTTP APIs. The logs can then log the function being called and whether that function was successful or not, along with a short but helpful error message if there was a problem. Then the developers or maintainers of that microservice can use the log data, find the function that had an error, and debug from there.

One of the most common problems with microservices is performance. With singular monolithic services, performance is sometimes easier to attain and maintain, because all of the processing can happen mostly at once in one or few spots. Using microservices means we have to rely on the time for the network, time for the API calls, and the variability of performance across distributed hardware. The good news with this is that the inherent chaos that can come from having many distributed microservices practically forces developers to build applications in very resilient ways. When we expect worse performance, we are more likely to ensure fault tolerance and resilience in design, and consider the entire network between the microservices carefully.

1

(IETF, 2009), [https://datatracker.ietf.org/doc/html/rfc5531](https://datatracker.ietf.org/doc/html/rfc5531) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1303-1)

2

(gRPC, 2022), [https://grpc.io/](https://grpc.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1303-2)

3

(APIs You Won't Hate, 2020), [https://standards.rest/](https://standards.rest/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1303-3)

## 1.3.2. Zero Trust

_Zero Trust_ means that each request for a resource requires validation that the system or person requesting it is appropriate. _Zero Trust Architecture_ (ZTA)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1304-1) is when zero trust is part of the systems design. Both zero trust and ZTA are design responses to architectures that have security on the perimeter but very little security within the network, enabling devices within a network segment to be treated as trusted systems. In zero trust, we eliminate that implicit trust within each network segment.

Instead of allowing systems within a network to access resources in that network, in zero trust we perform authentication, then authorization for each resource request. The result of this is that in order for a person or software to use any resource, the identity of that person or software must first be confirmed with authentication, after which short term access is authorized, commonly via a token.

_Kerberos_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1304-2) is an example of a protocol for authorization and tokens (called _tickets_ in Kerberos) for access within a network. In Kerberos, the access ticket includes the network resources the authorized client is able to access. While Kerberos is well-adopted in Windows, that is not the case for all technologies, and so other mechanisms are needed for granular authorization to many cloud resources. While it is possible to use Kerberos as a layer in ZTA, it often doesn't go far enough on its own without complex configuration work.

A common way to build up ZTA is to use _Open ID Connect_ (OIDC)[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1304-3) to create _federated access_. Federated access means that if the authoritative identity service is active in the end user browser, then they are authorized already for any federated services. This is commonly encountered as a "sign in with Google" button.

Combining OIDC with hardware tokens helps layer additional access controls. However, a common attack pattern on ZTA involves attacks targeting access recovery or password reset functionality. Making it too easy for an individual to get a password reset is a common security vulnerability. An adversary may reset the access, giving them control of the user identity and valid access to the systems.

There are many possible styles of ZTA, but we can start to think of ZTA in terms of all of these layers working well together:

|Component Area|Example|
|---|---|
|Physical hardware|TPM and disk encryption|
|Physical possession|Yubikey or card with reader|
|Proof of knowledge|Security questions|
|Short-lived access|JWT and Kerberos|
|Identity|Cryptographic signatures|
|Files and data|AES256 encryption|
|Audit and monitor|Privacy preserving logging|

> Table 3 - Layers of security and examples

Regardless of which technologies are used to build up ZTA, it is crucial for teams to be able to debug and understand the activity. An organization's ability to protect and audit the tokens used for short term access is essential, otherwise the plain-text tokens might be edited and abused. Partially implementing ZTA may result in a worse security posture than not doing so at all, if tokens are given too liberally without any ability to audit and prevent tampering or bypass.

1

(NIST, 2020), [https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-207.pdf](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-207.pdf) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1304-1)

2

(IETF, 2005), [https://datatracker.ietf.org/doc/html/rfc4120/](https://datatracker.ietf.org/doc/html/rfc4120/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1304-2)

3

(OpenID, 2022), [https://openid.net/developers/specs/](https://openid.net/developers/specs/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1304-3)

#### Labs

1. Each internal service must first _______ then get authorization for each access. Fill in the blank.

Answer

2. Zero Trust considers which type of access to be not worthy of trust?

Answer

3. Zero Trust considers which type of access to be valid access, worthy of trust for a moment?

Answer

4. In ZTA the focus is on protecting what?

Answer

## 1.4. Architecture Examples

Every application and organization designs their infrastructure to fit their needs. The cost, security, ease-of-use, and many other factors will need to be reviewed. However, while the infrastructure needs to be unique, it's important for us to use example architectures to understand what a design might consist of. We also need to understand the problem that might arise from certain types of designs.

This Learning Unit covers the following Learning Objectives:

1. Explore a small scale public cloud architecture example
2. Examine a medium scale public cloud architecture example
3. Review a large scale public cloud architecture example
4. Observe a large scale private cloud architecture example
5. Delve into two Hybrid Cloud architecture examples

## 1.4.1. Small-scale Cloud Example

We will start by examining a very small cloud that uses Docker behind GSLB and is connected to a distributed storage volume mount. The CICD system in this example may or may not be cloud-like, but the entire product itself is a small cloud with single point of failure resilience.

![[OffSec/Cloud/Cloud Essentials/z. images/2df2aa6549291366a7be120252dda383_MD5.jpg]]

Figure 4: Example Small Cloud

We should be careful not to dismiss such small-scale systems as they can still accomplish many needs at much lower costs and risk. A well designed application in a small cloud may still be able to handle several thousand simultaneous application users, although perhaps not at top performance. Small-scale clouds have more capacity than most medium-sized organizations really need.

When building at a small scale, we might not use cloud designs and instead opt for a single virtual machine. While that works when we don't need to prioritize availability, it is often a good idea to start thinking about growth early on. By choosing to create a tiny cloud instead of a single virtual machine, we can better posture for resiliency from outage and prepare for growth.

The smallest modern cloud typically consists of two physical datacenters, each with one or a few Docker containers, and with each datacenter mirroring the other. If a database is required within the design, this could be outsourced to a managed database or we might create a small two-node database cloud.

We can also make a cloud with virtual machines at small scale without using containers. However, using containers will set us up for growth and compatibility with more hosting designs. We can use _HAProxy_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1310-1) with a _floating IP_ to turn a few virtual machines into a little cloud, as long as we use at least two physical datacenters so that any network outages trigger a failover to the other datacenter. A floating IP is an IP address that can move between systems. In order for failover to work when there is an ISP outage, we will also need at least two ISP connections. All of the expensive WAN engineering required to make a cloud is one of the reasons why public cloud providers have become so popular. If we leverage a public cloud to do the WAN engineering for us, we can likely check a box or make a configuration file and leverage the work they have already done for WAN failover between geographic regions.

Regardless of whether we use containers with GSLB or virtual machines with GSLB, public cloud or private cloud, HAProxy is a valuable supplemental tool for load balancing and networking, especially at small scale. HAProxy is valuable because we can further reduce single points of failure. HAProxy can provide TLS where needed. It is valuable because of the quality and performance it provides with the free version, enabling load balancing and firewall features that work whether inside containers, VMs, or on bare metal.

Databases and persistent storage quickly become some of the most expensive and risky aspects of a system's design. The decisions around data storage are important architecture decisions that create design inertia. If we select a relational database and an older architecture design, we might accidentally create cloud anti-patterns (choices that work against the design goals) and hard-coded database connections. It is important to design for storage to be geographically distributed and replicated effectively. Using a cache can help with this.

The rule-of-thumb is to keep a cache next to each service, using the cache whenever possible instead of the database. The performance and speed of the systems can be improved because the data is closer to the service that needs it. Reducing the time required to fetch and store data improves the speed of the service, and can also reduce the scale required for the database.

However we decide to handle the data we'll want some replicated storage that both datacenters can access so that when we fail over to the second datacenter, all of the data comes with it.

Additional considerations for our small scale cloud include how we will accomplish GSLB, or at least DNS. We might opt to outsource the GSLB service to a CDN or cloud service to keep costs low. Given that this example is a small scale design, we might expect to outsource almost every component other than the application itself, assuming we have a small staff budget.

Outsourcing most components also includes our CICD systems. Most likely we will want to use GitHub, GitLab, Travis CI, or a cloud provider CICD service option. The CICD might not be very resilient or comprehensive early on with a small cloud, and may be its greatest security weakness.

Small businesses and startups often need to de-prioritize security, not only for cost reasons, but also to accelerate innovation and rapid prototyping. This often leaves small businesses more vulnerable to identity theft, employee workstation compromise, vendor account compromise, and vendor fraud. The good news for small businesses is that they are often less tempting targets because of the commonly perceived smaller reward for attackers. Where this is not true is primarily when the small business' business model is to be a service provider for larger businesses or organizations. When a small organization positions itself as a specialist for big business, they become a prime target for attacks that are working up to attacking the larger business by first compromising the small business partner.

Architectural measures we can take at the small scale to help our security posture include leveraging CloudFlare or a similar service to provide DNS proxy and _front door_ networking. When we talk about the front door in systems architecture, we are talking about the systems that receive requests initially and are exposed. The front door is typically a network service that protects the ingress into our cloud. We almost always use a vendor for this front door network in cloud design, although some private clouds might not. The strategy of using CloudFlare is affordable and also scales with us easily. CloudFlare's DNS proxy service hides the real server IP from public bots and crawlers, and allows minimalist GSLB configuration, and options for additional services.

Along with DNS proxies, organizations should prepare at least two locations or services for backups. _Tarsnap_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1310-2) is a very affordable backup solution that leverages AWS S3 and AES-256 encryption. Creating a daily tarsnap job to backup important data is one of the lowest-cost backup solutions on the market that still provides effective cloud resiliency and data encryption. To use tarsnap effectively, it's essential to properly protect the tarsnap account access keys.

Most organizations struggle to manage secrets at a small scale. Small organizations often have important security credentials stored in plaintext on individual laptops or other lower security systems. The administrative user laptop or system is a critical weakness at small scale. Building up an effective secret management solution will be an early security goal. The more quickly that can be done, the better the posture will be. If an organization can afford a managed solution with a cloud provider or managed _Hashicorp_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1310-3) platform, that may help, but again the credentials to the secret management service then become a critical weakness. This endless chicken-and-egg problem of access credentials is one of the greatest security challenges clouds of all sizes face. We want to keep secrets and sensitive data encrypted with AES256, and encrypted with a password that has not been leaked to plaintext.

Encryption usage is a critical aspect to most defenses. Adversaries learning the passwords and gaining the private keys then become primary concerns for the defender. Reducing password usage in favor of keys and certificates that are automatically rotated is typically ideal. If we can have math that proves identity, that is better than relying on more passwords. But there still are passwords, and in many cases with small organizations, long-lived static passwords for databases and cloud providers that are not well protected, or if well protected, rely on the memory of an individual staff member. Using a password manager can help, but there is still a password to the password manager.

Recovery methods are a huge attack vector for small organizations. Likely much is outsourced to vendors, and vendors can be social engineered to do password resets, or otherwise attacked to gain access to the organization's assets. These social engineering patterns are a risk to clouds of all scales, especially those that rely on vendors heavily for controls.

1

(HAProxy, 2022), [https://www.haproxy.org/](https://www.haproxy.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1310-1)

2

(Tarsnap, 2022), [https://www.tarsnap.com/](https://www.tarsnap.com/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1310-2)

3

(Hashicorp, 2022), [https://www.hashicorp.com/products/vault](https://www.hashicorp.com/products/vault) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1310-3)

#### Labs

1. What is the smallest number of geographic regions required for a cloud?

Answer

## 1.4.2. Medium-scale Cloud Example

Our next example cloud is a medium scale. Instead of individual containers in two datacenters, we have two 16 GB of RAM and 4 vCPU virtual machines. These virtual machines are worker nodes in a Kubernetes cluster, controlled via Kubernetes Control Plane. This example does not include details about the control plane, analytics, PKI, or identity and access management services, since these are assumed to be vendor-managed clouds. _Identity and Access Management_ (IAM) is any service that provides those functions. Many people use IAM when referring to AWS IAM, but IAM can exist in other systems and providers.

![[OffSec/Cloud/Cloud Essentials/z. images/2a232ee3b90d1745529782bd789e67f3_MD5.jpg]]

Figure 5: Example Medium Scale Cloud

We might run forty containers in a medium scale production, perhaps less or more. In our example, all traffic is routed to one of two worker nodes, each worker node containing all required resources for real time processing. In order to achieve segmentation between the containers, we can use networking rules and other controls to create different tiers of access within the cluster.

A medium-scale cloud may have more than a few services and is starting to adopt better practices with a security focus using microservices. Each component has access rules and logging tied to the IAM solution, so access to any system is centrally logged and rules allow access. Despite these additional measures, there may be technical debt that brings insecure or anti-pattern settings along with it as the cloud grows. Eliminating those debts of hard-coded configurations and plaintext credentials becomes an important area of focus to many medium sized cloud systems.

Many medium sized clouds also frequently have legacy systems connected to them, potentially including some virtual machines that do not have GSLB or automatic failover. Eliminating single points of failure is a continuous effort for clouds of all sizes. We commonly want to ensure that IP addresses are not coded anywhere outside of the GSLB/DNS systems, using DNS names instead for all code configurations. The consequence of DNS use is attacks on DNS.

Of course, the business can choose to not leverage cloud techniques for a given component. In order to make the decision to have a non-cloud asset, we must evaluate what the impact of outage is for that component. If that component being down causes the customer-facing service to fail, then we may want that component to be cloud designed.

As an organization prepares to implement more microservices, the chances of the organization selecting Kubernetes becomes much greater. Kubernetes might be used at a small scale too; however, the benefits of Kubernetes don't really show until larger scale. For this reason, we might see organizations adopt immature and insecure Kubernetes configurations at medium scale. A medium sized organization might be drawn to managed Kubernetes services like Amazon's _Elastic Kubernetes Service_ (EKS)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1312-1) in an attempt to reduce burdens on staff. Even a managed Kubernetes service still requires careful configuration and use by an organization's employees.

Managed Kubernetes can ease the engineering of Kubernetes. Often times, clusters at this stage will be fairly predictable and cost more than the benefits. Medium sized organizations will often follow design patterns for large clouds even when they don't have a need for that scale yet, resulting in spending more money than is required for the real situation. Kubernetes is useful for scaling; however, it increases opportunities for attackers and generates costs that would otherwise not be required. Even if the organization isn't gaining immediate benefits from Kubernetes because the tasks are relatively small scale, adopting Kubernetes does help set up the organization for growth.

Yet again, plaintext credentials on user workstations becomes a huge issue. Additionally at medium scale, we might start to encounter bastion hosts, another area that might have access to valuable credentials or networks. Bastion hosts are systems that are used to connect to other systems, often with strong security and audit measures, and potentially special network access. Bastion hosts are commonly incorporated in designs to regulate and monitor access to restricted internal resources. Administrators might SSH from their laptop to a bastion host, and then from there, interact with a cloud provider API.

If the medium scale organization is more mature, with strong security measures in place and security-conscious staff, this may be the most secure of all cloud sizes. The medium cloud with a security-oriented organization is often stronger than the large scale cloud because of reduced attack surface, tighter staff relationships, and more homogeneous systems. The more unified the security approach and designs, the faster the detection and response is, and also the less likely that an adversary will find an initial vector.

Identity management and auditing start to become more challenging at medium scale. Even if the organization doesn't have too many components at this stage, their ability to do detection and response to credential compromise might be poor. The smaller scale cloud is so small that one person can likely read through all of the related tracing data and handle incident response. At medium scale, we might be generating enough data and have enough complexity that one person is no longer sufficient for efficient incident response.

At the medium scale, we need to start investing in managing centralized logging data, and refine systems to ensure the desired information is being logged. With the addition of Kubernetes, organizations might not be logging all of the Kubernetes activity yet, resulting in longer response times to Kubernetes credential compromise.

Hopefully, the medium sized cloud includes a well-understood PKI and integration with identity management and secret management. Without those components in place, the organization may struggle to revoke a compromised identity, resulting in a longer or riskier path to recovery.

Alternative to centralized PKI, identity, and secret management systems, we might see a chaotic sprawl of multiple systems with various scopes. This chaos might actually have better segmentation; however, the amount of time and effort often required to properly implement the central systems might result in configuration sloppiness or other weaknesses in these systems. This can happen when employees are not allocated enough time to refine system security, or feel pressure from deadlines and cut corners, or from ignorance about some configuration nuance. In addition to the central security systems, the Kubernetes clusters may also have missing or sloppy configurations, out of ignorance or lack of dedicated Kubernetes-focused staff.

The medium cloud is often over-sized for the real compute volume needed, spending more money and creating more complexity than required for the use. Businesses commonly do this in anticipation of growth, yet fail to invest in the appropriate scaling of related security staff, systems, and practices. Instead of being over-provisioned, another problem we may find at medium scale is the reverse: organizations create non-performant software that is not efficient with system resources and worker nodes are pushed over capacity, resulting in performance and availability impacts.

To avoid these issues, load and performance testing and evaluation can be an important set of processes and practices to build into a medium sized cloud design. Understanding the capacity of a cloud and which scaling technologies to leverage not only improves the cost utilization, but also the customer experience.

A single container might be able to handle millions of service requests and process them completely, or a single container may only be able to take a single interaction per twenty minutes. Also, it only takes one resource-intensive container to disrupt performance for other containers on the same host, unless resource limitations are put in place per workload and hosts are well balanced for the tasks they take on. Our example medium scale cloud only has two larger sized worker nodes; however, we might have dozens more worker nodes in the same cluster, or outside of the cluster as individual virtual machines or cloud services at medium scale.

1

(Amazon Web Services, 2022), [https://aws.amazon.com/eks/](https://aws.amazon.com/eks/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1312-1)

#### Labs

1. Is the default Kubernetes attack surface small, medium, or large?

Answer

## 1.4.3. Large-scale Cloud Example

Fortune 500 companies, top 10 financial institutions, data collectors, internet providers, and government agencies might operate at a large scale. Our large scale cloud example has ten Kubernetes clusters distributed across twelve datacenters, and many more supporting non-cloud or optionally-cloud services.

![[OffSec/Cloud/Cloud Essentials/z. images/8bde34f1694e2e1cfa4210c52153bd1f_MD5.jpg]]

Figure 6: Example Large Scale Cloud

Large scale cloud organizations might be slower moving in terms of application changes than medium and small clouds, with changes sometimes taking a year to plan for. This doesn't mean that large clouds don't have quick reactions. Often, large scale clouds have high performing staff and detailed designs. Large clouds have the potential to be very resilient; however, many times their own bureaucracy impacts their ability to correct course. This results in anti-patterns in large clouds, and policy that might mistakenly decrease security.

A large cloud likely has staff examining security data at all times, and more than one person doing so. Such organizations might also deploy additional vendors and AI/ML systems reading through the data in nearly real-time to hunt for threats. One of the problems that impacts large scale organizations is complexity. Cloud systems this large might contain more data than humans can understand at once.

Large clouds are likely to pool data in _data lakes_, and spend enormous amounts of money on data storage. Access to these data lakes might provide access to customer information, as well as internal security details or metrics. Data lakes are typically lower security than secret storage systems, and often have multiple different systems and persons reading from them.

Large clouds often rely too heavily on security tooling while lacking a solid handle on more important basic aspects of security. A common example of this is inventory. Many large organizations struggle to keep up with what is in their clouds and which endpoints have access to which cloud components.

Behavior analysis of large organizations with diverse assets may result in more difficult behavior profiles for AI/ML alignment, resulting in more lax posturing. When a large organization is more predictable, with roles and scope clearly defined for each aspect, and every dependency is understood, a large cloud can be highly effective. However, most large organizations do not have an easy time making that the case.

What helps an organization handle complexity successfully is a systems architecture that enables the aspects of the organization to thrive and be unified. The confusion between departments is an area where division and social engineering can take hold and manipulate forces. Social engineering and betrayal becomes more risky as the organization becomes large. Large organizations often require many employees, so the risk of one employee being malicious is greater.

Advanced and well-funded adversaries might get hired at organizations with large clouds, in order to gain valid access to that cloud. For these reasons, large scale clouds must especially be ready for attackers with valid employee credentials and access.

In order to respond to attacks from within, especially with a large cloud, we may want to implement the systems design pattern called _observability_. Observability isn't just monitoring for metrics and errors or alerts. It is a systems-engineering principle of being able to infer the internal state of a system by measuring the output of the system. We must design systems to be observable, as not all systems have observable outputs.

Large clouds likely have hundreds or thousands of microservices, and dozens of Kubernetes clusters, each cluster containing many worker nodes, potentially thousands. There might be hundreds or thousands of staff that work on the cloud or clouds in a large organization. Even with all of this scale, Kubernetes is able to keep up with it, mostly.

Kubernetes clusters that become very large with more than 2000 worker nodes, especially when wide area networking is involved, can start to show serious performance issues. Careful planning around cluster design is required to ensure success at large scale. One of the common patterns is to not use clusters that large, but instead use more clusters. There are some designs that unify multiple clusters in a federation, or otherwise leverage groups of clusters under unified management automation.

We might find that each development team has its own cluster, and then each major product or service has its own cluster, and then further that there are security clusters, AI clusters, and internal service clusters. Alternatively, we might find larger clusters that run development and production, security and product. With large clusters with multiple team scopes, the organization is likely depending on role-based access controls to segment between teams using the same cluster.

#### Labs

1. What do we call it when we link management of multiple clusters together?

Answer

## 1.4.4. Large-scale Private Cloud Example

Our example large scale private cloud includes two physical datacenters, with one rack in each datacenter. Each rack has identical hardware, the data is replicated to both racks, and GSLB allows failover between them.

![[OffSec/Cloud/Cloud Essentials/z. images/99e239c2d2db7c6fb47f3e3a7fc3baa6_MD5.jpg]]

Figure 7: Example Private Cloud

When an organization has their own physical datacenters, and enough financial resources, they may want to build private clouds. Even if an organization leverages public clouds, they might use their physical datacenters as a private cloud. Organizations might use their datacenters in unique ways while also leveraging clouds, like having non-cloud designed datacenters with supercomputers or custom hardware.

Private clouds require doing the work that public cloud providers do for you. Alternatively, we might outsource private cloud builds to vendors that specialize in building private clouds. In this way with outsourcing, it is much like the public cloud; however, the infrastructure is not shared with others and perhaps the infrastructure access is not internet based.

As we design private clouds, we must consider not only the platform and software designs, but also the hardware designs and physical controls. Private clouds require physical security to protect the datacenter from tampering, destruction, or exfiltration. Data can be exfiltrated from physical locations by multiple technical and social means. Adversaries may pretend to be employees of the telecommunications company, or a construction worker, and use the proximity to the datacenter to acquire information or implant sensors or malware.

Private clouds have more staff with administrative access, including physical access. Compromises to security and identity systems might result in physical consequences such as fires, floods, and destruction, or persistent control gained by an adversary that had physical access, controls below the operating systems of the servers in the firmware, hardware, or infrastructure components.

Most large scale clouds rely on centralized logging systems, often more data lakes with technologies such as _Elasticsearch_.[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1316-1) These logs are often parsed by security systems that analyze the data and alert staff to anomalies and take automated action when threats are detected. One common misconception about data lakes is that they are dumping grounds, and some organizations treat them that way. A good data lake is well organized and curated.

More commonly-used than private clouds are hybrid clouds, which leverage both private and public clouds. Hybrid clouds are any clouds that use multiple clouds from different providers. Hybrid clouds are not inherently better and are often worse in terms of expense and risk.

1

(Elastic, 2022), [https://www.elastic.co/what-is/elasticsearch](https://www.elastic.co/what-is/elasticsearch) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1316-1)

#### Labs

1. Even though hardware can be less expensive long term in private clouds, what is the cost that is much higher for private clouds?

Answer

## 1.4.5. Hybrid Cloud Example 1

Hybrid clouds are the reality of many large scale organizations. Having multiple separate clouds increases complexity and costs. Some medium and large organizations let their cloud instances and designs go into chaos, with different people creating new clouds without business awareness. When an organizations systems become fractured between multiple clouds, the time it can take to respond to an issue or incident is often increased.

The example design with two public clouds, likely multiple enterprise offices, and remote employees, makes an enormous attack surface. This type of design allows an organization to juggle costs and outages between two clouds, but might double the configuration costs in some cases.

![[OffSec/Cloud/Cloud Essentials/z. images/04d5cf17803c3117229527ec304942b5_MD5.jpg]]

Figure 8: Example Hybrid Cloud 1

While some cloud providers are making improvements to linking multiple public cloud providers together, and managing them separately, it doesn't reduce the complexity much. We may be able to make one of the two clouds authoritative in design, pushing or managing the policies and configurations of the other provider. When such a linkage is in place, we can reduce some configuration management costs.

In our example, the larger dotted line box on the left is the primary corporate cloud that has not only all of the office infrastructure linked but also is authoritative over the second public cloud to the right. The second public cloud does not have all of the office infrastructure linked to it, but has a complete copy of the Kubernetes clusters, distributed storage, networking, data, and serverless functions.

The serverless function infrastructure not being 100% the same between the two clouds adds some variability, even though the functions themselves can be identical. The control planes and worker nodes between the two clouds are also not 100% the same; however, the workloads are identical. These subtle differences need to be kept in mind during testing and observation.

The designated primary cloud contains the identity management and authoritative security controls. If the primary cloud is completely down, then some aspects of business operations are halted, while production customer services can continue to run entirely in the second cloud without downtime. Not all hybrid clouds have the example limitations, some are completely resilient; however, many have some dependency on a primary provider.

## 1.4.6. Hybrid Cloud Example 2

This example hybrid cloud has four on-prem datacenter/offices, two private clouds comprised of pairs of those datacenter/offices, and one public cloud that links them all together, which provides public facing assets, storage, and analytics.

![[OffSec/Cloud/Cloud Essentials/z. images/145add2eabb5c699686d3bdc95f2cd09_MD5.jpg]]

Figure 9: Example Hybrid Cloud 2

Not all hybrid clouds are messy; some are well-ordered. When machine learning is a focus for the organization, vertical scaling is often desired. Vertical scaling is when we want to increase the resources available to a given application or microservice, such as increasing the RAM, disk, or CPU available to a single container, VM, or bare metal system. Vertical scaling causes much higher public cloud provider bills, so one common pattern is to offload ultra heavy vertical workloads to private clouds where the cost is lower. Sometimes these private clouds have lower security, or are missing physical security with systems sitting out in an office area for interacting with them administratively.

Physical security of systems might also drive the private cloud use. For example, if you must have tighter security than AWS, an organization might create a well-guarded physical datacenter and large proximity protections. Having a SOC team directly within the physical protection of this datacenter has some advantages. Close guarding from physical tampering and eliminating internet administrative controls can add value to the security posture for some threat models.

With ML workloads, we are commonly running training for the software, running through the data so the software can update its understanding of how to behave. In our example architecture, there are three separate clouds, with a focus on vertical scaling and maximizing disk storage with lowest costs. There are two private clouds in four physically separate office buildings. Each office building contains a LAN that has a server rack and two GPU custom systems within it.

In this example, each office connects to the same public cloud. The public cloud is used for internet facing services and centralization between all three clouds. The security services are mostly in the public cloud; however, each building also has several physical devices uniquely dedicated to security in that LAN, communicating data to the public cloud security systems.

Each office private cloud is a Kubernetes cluster on QEMU KVM hypervisors. The private office clouds do not connect to each other directly, but through the public cloud services. Each private cloud has _TrueNAS_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1319-1) storage arrays with backups of all data and ML models. TrueNAS is a _Network Attached Storage_ (NAS) system, enabling many types of storage configurations and integrations, including backup systems. There is TrueNAS deployed into the public cloud that syncs data between the two private cloud TrueNAS physical systems. Users within each office have zero network hops between them and the office cluster, and security rules must be separately written and enforced in each cloud, greatly increasing the cost of security configuration, while reducing the cost of running extra large sized instances.

The extra large instances in each private cloud might have 500GB of RAM, 24 or more physical CPUs, multiple GPUs, or much more resources to pack in as much training for the ML as quickly as possible. Mainframes, supercomputers, or any other custom large system could be used in this case. Military and science applications may require secret operations and custom hardware used in a private cloud.

As time goes on, more custom hardware options are available in public cloud services. As of this writing, AI/ML services, quantum computing resources, and custom hardware security such as isolated RAM enclaves and ARM hardware are all available in public cloud services.

We could realistically merge the private clouds, but this design specifically avoids all network traffic between the two, and with anything other than the public cloud instances. By restricting network traffic to only the public cloud endpoints from each private cloud, we can create strong logical and physical segmentation. Heavy and sensitive workloads can run in a private cloud, while the public cloud may delegate workloads to one of the private cloud segments. In this way, all three clouds really act like one large cloud.

One of the best qualities of having the three clouds like this is that each office is capable of operating without the internet for development and testing work. It doesn't always take an entire physical datacenter to do development and testing in isolated networks. Instead of having datacenters, another approach to this is to create meshes of employee devices that provide a separate employee private cloud design. In our example, we split such an employee private cloud into two at the physical offices, then merge them together in the public cloud services. The public cloud is authoritative in this case, and each private cloud office represents a geographic location of hardware physically managed by the organization itself.

1

(iXsystems Inc., 2022 ), [https://www.truenas.com/](https://www.truenas.com/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1319-1)

## 1.5. Common Security Controls

This Learning Unit introduces and explores common security concepts we may want to integrate into our cloud architecture. Without carefully considering the security of a given cloud, it may be difficult to keep the systems secure, understandable, and respond to incidents that might happen.

These concepts apply to more than cloud design, but are important elements to IT architecture as well. When we consider cloud design, we must also consider the IT architecture as a whole, and how the cloud or clouds fit together with our IT infrastructure.

This Learning Unit covers the following Learning Objectives:

1. Explore software-defined network controls
2. Examine access management controls
3. Review identity management controls
4. Discuss state-enforcing controls
5. Learn about monitoring and observability controls
6. Delve into immutability and verification controls
7. Investigate application security controls

## 1.5.1. Network Controls

In more traditional architectures, one or more physical firewall devices in a datacenter provide most or all of the networking controls, perhaps augmented slightly with some _iptables_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1321-1) or host-firewall rules if the organization was more mature in security posture. We still leverage these components in more modern cloud network architectures; however, we are less likely to physically or administratively manage perimeter network appliances as they are commonly located in cloud provider datacenters.

With cloud networking, we abstract the network into software defined networking, also known as SDN and SD WAN. For SD networking, different interfaces such as APIs or GUIs set the networking rules. We also deploy components into shared infrastructure, so implementing SDN rules for each service component becomes more important.

In Kubernetes, we can use _NetworkPolicy_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1321-2) to create detailed network requirements down to individual containers, which is an abstraction for the Kubernetes cluster, applying to each server in the cluster. This same NetworkPolicy mechanism can also create access categories between physical hosts within the cluster.

In addition to Kubernetes and other GNU/Linux kernel networking, cloud providers often have custom SD networking that can be used. Most likely we want to use both the cloud provider SD networking and the in-kernel networking. All of these SD WAN, SD LAN, and in-kernel networking are fundamentally providing multiple layers of packet filtering firewalls at each hop within our cloud.

Beyond firewall packet-filtering, we also want to ensure the communication is properly encrypted. There are two primary network encryption approaches, IPSEC VPN[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1321-3) and _Transport Layer Security_ (TLS). TLS is the successor to SSL, still commonly referred to as SSL or SSL/TLS. SSL is not used on any modern systems, which use TLSv1.2 or TLSv1.3. One way to think about the difference between IPSEC and TLS is that IPSEC encrypts between two networked devices while TLS encrypts between a service and a client of that service; the client could be a system or person. VPNs can use forms of TLS instead of, or in addition to, IPSEC.

TLS is how we add the encryption and identity to HTTP for HTTPS, but that isn't the only use of TLS. It's also used for system-to-system communications security as well as authentication. In ZTA, TLS is a critical component, not just at the public facing HTTPS but between every system and microservice, and not just minimal TLS, but client authenticated TLS, aka _mTLS_.

We can leverage mTLS between each person and service for one or more clouds to not only encrypt all network communication, but also having the client side of the connection authenticated in addition to the server side. In this scenario, only the holder of the private key and corresponding certificates will be able to access services. In this way, each service can be segmented between other services and persons at a protocol level before giving access to the application. This means that even if the firewalls are bypassed because the threat is in the network, they also must have valid identifying certificates for each service they want to reach. Each employee also needs the appropriate keys and certificates to directly access deeper-tier microservices and components.

Public-facing internet components are very unlikely to use mTLS, and use TLS for server-only authentication based on public CA trust, which ships automatically with devices and browsers.

All of these certificates and public key authentication components require systems and processes for management, known as public key infrastructure (PKI).

Many small organizations do not have good PKI, but store private keys in various places, sometimes in plaintext where they could be leaked. A good PKI system might include many components to protect private keys on PKI servers, and centralize certificate management automation. Examples of free PKI automation tools include nginx certbot. Certbot and web servers require easily accessible private keys for server identity. Thankfully, those keys are the least useful for an adversary; however, they still could be used in monster-in-the-middle (MITM, sometimes known as man-in-the-middle) or identity deception attacks. The keys an adversary will benefit greatly from are the mTLS private keys that allow access to the PKI systems themselves.

In addition to TLS and mTLS configuration, many organizations leverage virtual private networks (VPN). Using modern Linux, we can automatically create a VPN using Linux kernel _wireguard_[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1321-4) features. Calico CNI in Kubernetes can leverage a patch to automatically encrypt all network traffic at the kernel level, creating a cluster-wide automatic VPN mesh.

There are multiple ways to automatically implement mTLS between all containers in our clusters to facilitate encryption and identity for each network interaction. There are also ways to use automatic in-kernel wireguard VPN between all cloud nodes. Both approaches accomplish network encryption; however, mTLS also provides two-way authentication and management.

1

(die.net, 2022), [https://linux.die.net/man/8/iptables](https://linux.die.net/man/8/iptables) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1321-1)

2

(Kubernetes, 2022), [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1321-2)

3

(IETF, 2011), [https://datatracker.ietf.org/doc/html/rfc6071](https://datatracker.ietf.org/doc/html/rfc6071) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1321-3)

4

(Edge Security, 2022), [https://www.wireguard.com/](https://www.wireguard.com/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1321-4)

#### Labs

1. The most important aspect to network security is which protocol?

Answer

## 1.5.2. Access Management Controls

Access to systems and services is a critical component of security for an organization or individual. Most people are now familiar with passwords, and even the idea of certificates is gaining ground. However, there is still a lot of ignorance around access controls and cryptography in general. Whether we are performing offensive or defensive functions, scrutiny of access management is critical. Incorrect access management configurations or vulnerabilities might allow privilege escalation, potentially leading to complete compromise of the entire infrastructure, cloud, or enterprise.

Many modern access management systems use tokens for access. Tokens are typically granted after an authorization, frequently tied to an identity service. The tokens generally provide access to services for short periods of time. One of the common issues with these access tokens is that tokens may be manipulated to extend access periods, potentially indefinitely. _JSON web tokens_ (JWT)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1323-1) are commonly used for access to cloud and application resources. A JWT contains a header that includes the algorithm, a payload that includes an expiration time, and a signature. The most common manipulation in this regard may be editing the expiration value in a JWT. In order to protect against token tampering, organizations might wrap tokens in other layers such as asymmetric signing. We might also leverage caching systems to force token expiration and provide other tamper-proofing controls.

Along with tokens, cryptographic private keys are commonly paired with a certificate to provide access control. When leveraging mTLS, a key and certificate signed by a special internal CA creates the authentication components on the employee system. Using mTLS and/or JWT authentication is common for Docker and Kubernetes administration, among other things. One of the largest problems of JWT and mTLS integrations is that the certificate and key pair or JWT are commonly stored in plaintext on the admin laptop. Compromise or access to the admin laptop might provide full administrative access to the entire cloud.

Aside from API authentication components, symmetric encryption is also a common form of access control. While not everyone uses tokens and keys, almost everyone uses symmetric encryption access controls. Passwords are examples of symmetric encryption access controls. Passwords may not only be for identity, but also may be used to access files and disks that are encrypted. Passwords can also be used for authentication via hash check, typical for operating system and website user accounts. Rather than using the password as the symmetric key, the password is hashed and compared to the stored hash, ensuring a match before authorization.

Another type of access control is _Mandatory Access Control_ (MAC).[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1323-2) MAC adds additional restrictions to a system, originally developed with BSD.[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1323-3) MAC was brought into GNU/Linux with the _Security-Enhanced Linux_ (SElinux)[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1323-4) abstraction of MAC, and again with an alternative MAC abstraction called _AppArmor_.[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1323-5) These types of controls apply to operating systems, restricting access to components in granular ways. MAC controls are often a _secondary_ control, meaning they're intended to slow or stop an adversary that has already gained some level of system access. For example, if an adversary was able to exploit a web application that would result in a system shell, MAC might prevent the reverse shell or reduce the access that the gained shell has.

As an organization selects access controls, it is important to reference access control model designs that are well thought-out and understood within the organization, as well as implemented and maintained consistently. An access model is the architecture for access to each component of a system. As we design clouds, we should build up access models, test and validate them rigorously, implement them consistently, and then continually audit them.

Access controls are sometimes the only defense an organization has, and are commonly the first line of defense. Access controls exist at the front door of a cloud system with the access to the cloud administrative accounts as well as the cloud networking. Access controls exist within the system on a per-service level, and often increase in strictness as we move further into the network.

1

(IETF, 2015), [https://www.rfc-editor.org/rfc/rfc7519](https://www.rfc-editor.org/rfc/rfc7519) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1323-1)

2

(The FreeBSD Project, 2022), [https://docs.freebsd.org/en/books/handbook/mac/](https://docs.freebsd.org/en/books/handbook/mac/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1323-2)

3

(bsd.org, 2022), [https://www.bsd.org/](https://www.bsd.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1323-3)

4

(RedHat, 2022), [https://www.redhat.com/en/topics/linux/what-is-selinux](https://www.redhat.com/en/topics/linux/what-is-selinux) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1323-4)

5

(AppArmor, 2022), [https://www.apparmor.net/](https://www.apparmor.net/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1323-5)

#### Labs

1. What is the largest problem generally seen with access controls?

Answer

## 1.5.3. Identity Management Controls

When we have many systems and people connecting and communicating with each other and external vendors, it is critical to know which systems and employees exist, and what each can access. Identity management quickly becomes key. Many businesses use Active Directory[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1325-1) or LDAP[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1325-2) for identity management, while some small scale organizations don't have robust identity management systems.

There are public cloud service offerings for _Identity and Access Management_ (IAM).

Often times, AWS IAM is the default thought when discussing IAM. IAM is not unique to AWS; however, AWS IAM includes unique features.

We might use Google as our identity provider for cloud design. In this scenario, we could use Google authentication to validate our internal employees by linking Google authorization to our web browser interface authorizations. One of the potential issues with federated identity is that if the Google identity of an employee is compromised, then the attacker potentially has access to our internal systems.

A _source of truth_ in systems design is an authoritative system for a given scope, a centralized system that is the authority of a given component. In terms of identity management, a source of truth is where that identity is truly set. We might have other systems that then pull from the source of truth, or the source of truth may push into other systems. In this way, we can have a single place that is ultimately where the identity is maintained, and then other systems check against that centralized identity. Without a centralized source of truth with identity, individual systems may have different passwords and usernames, using different values to authorize access.

Rather than Google being the source of truth identity provider, it is more common that Google OIDC federation is used as a form of two-factor authentication. Modern identity solutions that we might use as the source of truth for internal employee identity is most commonly Active Directory, and Active Directory is a huge attack vector for this reason. A more secure, and often lower cost, identity solution is to leverage a cloud _identity provider_ (IdP) as the source of truth for components in cloud designs. In Azure Cloud, the IdP is Azure Active Directory, which is a major attack vector for Azure clouds.

Not all organizations have a single source of truth for identity, and that can result in disasters or benefits, entirely dependent on the situation. We might benefit by having employee workstations using AD because they're leveraging Microsoft, while the cloud might benefit from separate cloud IdP solutions for employee identity, or a GNU/Linux or BSD environment might use LDAP. The major consequence of having multiple sources of truth is having to maintain accounts and controls in multiple places, and de-conflict or sync them if the domains have any overlap. Segmentation between domains is a desirable design quality, so moving identity into a few manageable large groups based on primary technology can make sense; however, we might also choose one system to be authoritative and push changes into the others, or to have the others pull from the chosen source of truth. Issues with these types of systems can be complex, often related to synchronization between different downstream identity systems.

Identity management solutions ultimately seek to centralize the identity component of access authorization, but they might also track employee activity or gather security metrics. Our ability to audit identity management systems, quickly rebuild them if disrupted, and restore them from backups becomes an important exercise. If the authoritative identity source of truth is disabled, users might experience failed password changes or denial of service.

1

(Microsoft, 2022), [https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/get-started/virtual-dc/active-directory-domain-services-overview](https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/get-started/virtual-dc/active-directory-domain-services-overview) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1325-1)

2

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol](https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1325-2)

#### Labs

1. The most commonly adopted and most commonly attacked identity management system is ______ _________. Fill in the blank.

Answer

## 1.5.4. State-enforcing Controls

State enforcement is the concept of being able to ensure a given state of systems is maintained and potentially stop malicious activity during execution. While a very useful security control, it has not been commonly adopted because organizations may not fund security enough to implement it fully or properly. This is because there are many variables in a system state, and ensuring those variables are as-expected is not a trivial effort when the organization uses customized or multiple divergent systems.

An example of state enforcement is offered by _Puppet Agent_. The example Puppet Agent software has configured instructions regarding the desired state of the system. An example of a desired state is that a file in the path of **/usr/bin/myapp** is present and matches the hash "3dc17fa6899d65a02480fefd02152480a018c2e4a58d3a4ce08a46bb1ff8", which represents the exact byte configuration of myapp. If any change is made to **/usr/bin/myapp**, the Puppet Agent observes the changing hash, then reverts the binary to the intended file.

A newer style at the time of this writing for state enforcement is to use _Tetragon_.[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1327-1) Tetragon provides eBPF-based kernel-level state enforcement for Kubernetes that is granular down to the individual container level. Tetragon can run in a Kubernetes cluster and enforce states by manipulating and tracing system calls. We can use Tetragon to enforce states as well as detection and alerting for network, process, and filesystem access.

State-enforcing controls used to be too tedious for many organizations, as maintaining so many configurations didn't scale well. Using state enforcement is much easier with the assistance of Kubernetes. Kubernetes Controllers enforce various states across any number of computers based on the same configuration used to deploy the software. This way, no additional Puppet Agents are needed at the systems level; instead, state enforcement is an inherent property of the system.

While systems state enforcement is mostly taken care of in cloud situations by the nature of the infrastructure, binary security and security inside of individual containers is not fully addressed by Kubernetes out of the box, and running agent software inside of the container can be challenging or computationally expensive.

Enforcing the state of a system within a running container is not well-adopted at the time of this writing, although it's growing in adoption rapidly. Container states are also validated before production use in the CICD pipeline. We can scan the contents of the container images, measuring the hashes and checking for potential vulnerabilities. We can inventory the software within the container and create a SBOM automatically in the CICD. We can combine the SBOM with _Cosign_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1327-2) to measure the binary security of each software dependency within the container image.

For running containers, we can configure software within the container to perform binary security checks, but that might be too heavy for some designs. Another approach is to measure container execution and alert staff when containers have executions inside them that are unexpected or potentially malicious. While alerting is not state enforcement, it is the next best thing: state alerting and observability.

1

(Cilium, 2022), [https://github.com/cilium/tetragon](https://github.com/cilium/tetragon) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1327-1)

2

(sigstore, 2021), [https://www.sigstore.dev/](https://www.sigstore.dev/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1327-2)

#### Labs

1. Binary security is primarily accomplished via which type of cryptographic function?

Answer

## 1.5.5. Monitoring and Observability Controls

Control systems theory includes a concept known as _observability_, which essentially means that we can infer the state of a system from its outputs. New designs influenced by this theory are popular in cloud design. Not all cloud systems are observable, and not each system needs to be observable. However, designing a system to be observable provides benefits for both support and incident response capabilities.

By mapping outputs to meanings, AI/ML (as well as human analysts) can curate maps of outputs belonging to known and unknown states. Rather than just doing ping tests and cURLs for HTTP 200 to determine if a particular microservice is malfunctioning, observability would mean that we can examine the output of those commands to understand whether there is a network error. We can send the cURL request into a specific context used for observation, such as **/status/health/all** and receive a compact, but comprehensive data blob in response describing the internal state of that service.

Monitoring is also important, so we want to use status codes and network measurements to help our support staff and incident responders understand components, but observable systems reduce the time-to-resolution of issues by pointing us to changes and issues automatically, rather than requiring an in-depth investigation. Instead of having to research to learn the cause of the issue, the issue is raised by the observability system as it observed the change. From there, staff can review the change, and determine from the output that a specific microservice is in a divergent state. This divergence might have been caused by a deployment mistake, a bug, or a malicious actor.

The divergence or fault can be mapped and linked to corresponding playbooks for resolution. Then the next time this specific issue comes up, the observability system can provide insight to the responders about matching outputs, or possibly run the playbook on its own. Without each component of a system being designed very carefully for observable outputs, there might be many scenarios that map to a single output. This is common for system level errors like disk space, network issues, and other hardware failures. Observing and understanding chaotic and complex systems and their disasters is no simple task, but with the principles of observability, we can add that data along with monitoring data and system log data to create complex correlation and event-based systems that help us understand what is happening.

One of the challenges to achieving observability is accounting for in-kernel activity. We can trace kernel data, but it becomes a lot of data very quickly, and mostly consists of data that we are not interested in. _Extended Berkeley Packet Filter_ (eBPF)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1329-1) is a feature of the Linux kernel that allows special programs referred to as _maps_ to be loaded into the kernel. These maps can change system behavior, modify networking, and trace or modify system calls. Some of the maps that do tracing are types of kernel probes. eBPF is not only the main reason we have this problem of observability, but also the solution to it. With eBPF probes, we can trace kernel activity and integrate it with monitoring and correlation systems.

eBPF doesn't need to be written by hand by employees, and most likely that is not what an organization wants. Instead, we can leverage production-ready eBPF tooling such as Calico and Cilium, both of which have eBPF programs available for use. We can take these vendor-provided eBPF programs to empower in-kernel observability and monitoring, as well as network optimization, security, and more.

Sometimes, observability and state-enforcement are accomplished by the same tools. Other times, the service itself is designed to include observability and state-enforcement within. Both observability and state-enforcement are hot topics in cloud security, because they are what has been missing from many cloud architectures. Clouds are often easy to create and destroy, but keeping clouds in a specific known state, and being able to observe what is happening within them, can be a challenge.

1

(Linux Foundation, 2021), [https://ebpf.io/](https://ebpf.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1329-1)

#### Labs

1. Which area is commonly the least monitored area of the stack?

Answer

## 1.5.6. Immutability and Verification Controls

If you have ever used GNU/Linux, you might have realized that it allows users with shells to do many things, even destroy the system, unless more security controls are put in place. _Immutability_ is a property of an object that cannot be modified. There is a bit in Linux file permissions called the _immutable_ bit that can be added and removed with _chmod_ system calls. Most systems' immutability is not achieved via _chattr_ and chmod, but instead by using read-only runtimes. If we mount a system as a read-only filesystem, then _read_ syscalls are allowed, but no writes can be made.

Many systems are not designed in a way that can use a read-only filesystem, because they need to make disk writes for one reason or another. We can still add immutability to systems that are less immutable using other techniques, such as making system usage ephemeral. This means creating the system to execute the needed task and then destroying it. In an extremely ephemeral system, the disk might be writable from within a running container, but we only have that entire containerized filesystem overlay for a few seconds before it is destroyed.

In addition to immutability, we may also want to use verification controls. Verification controls come in many forms, from code parsers, linters, and binary security. Parsers and linters check source code for issues while binary security checks files on the filesystems, including container images and everything inside them.

Because of the increased complexity of binary security when we have many containers rather than more consistent virtual machines, and immutability can be difficult to achieve inside these containers using read-only file systems, a design pattern to create the most isolation and immutability possible is common among security focused organizations. The peak of this isolation involves an immutable _unikernel_ virtual machine. A unikernel VM has only the needed components for it to run; within the VM, we have removed concepts like shells and user commands in the operating system so that only a specific program is able to run. AWS Lambda uses ephemeral, on-demand unikernel VMs, with containers inside to execute the function in a sandbox.

Even though we can create extreme application isolation, that doesn't mean AWS Lambda and other strict systems remain completely ephemeral. For example, if a function runs for a long time, and the application code includes a vulnerability allowing remote code execution for application internals, an adversary might manipulate these internals while they're running. Another, more common way strict functions might lose immutability is from adversaries that have CICD and git repo access. If an adversary is able to change the code that runs inside the function by making an authorized release of malicious code, then they can use the system release processes to change the otherwise immutable system based on their malicious intent.

Another approach to immutability comes from within the kernel space. We can leverage security profiles and tooling that can be granularly applied to containers. These profiles are empowered by eBPF and other kernel features, restricting the system calls possible to a container. By using _seccomp_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1331-1) profiles, we can reduce the syscalls possible from within the container. Maintaining, applying, and testing security profiles is an additional cost, and is typically a last layer of defense used when a shell has been obtained by an adversary. If we have a kernel level security profile that prevents disk writes within the container, the adversary's activities might be thwarted or slowed. Applying extremely strict security profiles is standard practice for top security systems.

1

(Linux Kernel Project, 2022), [https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html](https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1331-1)

#### Labs

1. Which syscall commonly conflicts with immutability?

Answer

## 1.5.7. Application Security Controls

In addition to making the underlying operating system immutable, to mitigate more threats we also need to make the application internals secure. Application security is more important than ever, as with many companies' architectures the applications not only provide customers with services, but run in untrusted environments that the organization might not control. A public cloud provider can't fix the internals of a cloud provider customer's application, although they often have tools to help customers do that.

Some programming languages are inherently better set up for security than others, but each major language can be used in more secure ways. In order to improve application security, we might examine the protocols and data structures, referencing _requests for comments_ (RFCs) created by the _Internet Engineering Task Force_ (IETF)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1337-1) to ensure we are implementing to internet engineering standards. We also should trace the flow of each piece of data, including the social and legal aspects that go along with data. As we follow data through the data lifecycle, we should consider the value of that data and how well we are protecting it.

Sometimes, architecture decisions end up leaving gaps in security. An example of this is when TLS or VPN encryption is not enabled, exposing plaintext data across the networks. We can make up for this a little but not fully, by adding encryption to the data structures transmitted and processed by the applications. We can transform inputs into encrypted text that then can only be decrypted by the correct service to digest the data. If full application encryption is in use, we likely want to strongly consider how effective the logging and debugging capabilities are of the services.

In addition to applications using encryption on data, we might also have applications use encryption on configuration files. Configuration encryption is not the most popular control because it can make debugging or development more time-consuming or confusing to operate. More commonly, application configurations are stored in plaintext or inserted into memory, such as with environment variables. If secrets are stored on disk, then we should consider who and what has access to that disk. If secrets are stored in environment variables, we should consider who and what has access to that operating system or container. Better than environment variables or on disk, is inserting secrets into protected application-use-only memory. This type of integration can take more development effort to implement. To push the isolation further, we might additionally choose to leverage special hardware for _confidential computing_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1337-2) to further isolate the hardware used by the software.

While it may be easy enough to insert secrets, we have to consider how they are inserted in the first place. If the mechanism to insert them exposes them in a script or config, that could result in a worse posture. Centralized secret management systems help align secret usage for application configurations. By requiring all secrets to be stored centrally in secret management, we know the authoritative value of that secret and can update it in a single location. We also can create standard processes for inserting the values into application memory or onto disks. With a secret management system in place, we might require each secret to be pulled into memory from the secret management system after multi-factor authentication. Multi-factor authentication and authorization of software systems might be done by confirming the source network to be specific, maybe incorporating a reverse DNS check or a system of record check, possibly using mTLS, and only allowing a pull for a short window of time after authorization.

Even if we don't use application configuration encryption, we can work to avoid putting secrets in files that are exposed to untrusted sources. Exposed plaintext secrets is one of the most common weaknesses in clouds. Another way we can protect resources is by taking special hardware measures. _Hardware Security Modules_ (HSM)[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1337-3) and _Trusted Platform Modules_ (TPM)[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1337-4) can help when we want to strongly link decryption and signing to hardware. But yet again, HSMs have administrative passwords and the leakage of those credentials exposes even the hardware security components locally on that hardware, or remotely if it is a cloud HSM. We can use secret management systems integration where applications pull secrets into protected memory, but that ability to pull from a secret management system may be a weakness as well.

Application security not only involves protecting sensitive data, but also mitigating _injections_. There are many types of injections in information security, but an injection is essentially when someone or something is able to insert code, configurations, or commands that were not intended. In addition to designing applications to avoid injections, we must also consider memory and thread safety. Memory safety is when the application correctly manages its RAM use, and thread safety is when the application correctly manages its process threads. Memory and thread safety relate to mitigating many types of vulnerabilities, from undesired crashes, memory leaks, buffer overflows, un-intended behavior, access restriction bypass, and more. For this reason, most cloud native applications are developed in languages that work to ensure both memory and thread safety.

We can dictate the languages used in the applications and microservices as part of the architecture design. By choosing memory-safe and thread-safe languages like _Go_[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1337-5) and _Rust_,[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fn-local_id_1337-6) we can eliminate entire classes of bugs and vulnerabilities in our applications. The architecture might dictate two languages for everything, one compiled and one interpreted. For example, we might select Go and Bash as the two languages available for our cloud architecture, and then all applications and systems automation is to be written in either or both of those two languages. This helps keep the skills required for an organization's developers more consistent, supportable, and unified.

Both the application and systems designs should work together to use controls effectively. If we don't control the systems or hardware, only the application, then we likely want to put strong efforts into controls and safety within the application itself. If the system provides multiple layers of security, then the application security might be able to relax a little and still have good controls. Even if we can relax a little because of strong controls outside of the product software, we still need to ensure that the controls mitigate vulnerabilities by continuous security testing. We must try to exploit our applications and systems so that any flaws and vulnerabilities may be identified by the organization before being leveraged by an adversary.

The overall architecture of the cloud typically shapes the security controls that are used. Many organizations at the time of this writing have fairly predictable configurations in public cloud providers; the infrastructure looks chaotically standardized based on greenfielding from large providers or well-funded organizations. By knowing how these cloud systems are controlled, we learn about what a potential adversary can do with a given identity authorization, and begin to explore what it takes to stop an active adversary in a cloud.

1

(IETF, 2022), [https://www.ietf.org/](https://www.ietf.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1337-1)

2

(Intel Corporation, 2022), [https://www.intel.com/content/www/us/en/security/confidential-computing.html](https://www.intel.com/content/www/us/en/security/confidential-computing.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1337-2)

3

(NIST, 2022), [https://csrc.nist.gov/glossary/term/Hardware_Security_Module_HSM](https://csrc.nist.gov/glossary/term/Hardware_Security_Module_HSM) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1337-3)

4

(Trusted Comping Group, 2022), [https://trustedcomputinggroup.org/resource/tpm-library-specification/](https://trustedcomputinggroup.org/resource/tpm-library-specification/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1337-4)

5

(Google, 2022), [https://go.dev/](https://go.dev/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1337-5)

6

(Rust, 2022), [https://www.rust-lang.org/](https://www.rust-lang.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/cloud-architecture-overview-40762/common-security-controls-40809/application-security-controls-40783#fnref-local_id_1337-6)

#### Labs

1. Applications can be integrated with what type of management system to improve application security of passwords used by applications?

Answer