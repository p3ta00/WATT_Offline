In this topic we will cover the following Learning Units:

- Learn Kubernetes terms
- Understand why Kubernetes can be useful
- Start using Kubernetes

_Kubernetes_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1539-1) is an important set of components we can leverage to build cloud systems without having to reinvent the wheel. We can extend Kubernetes' functionality by leveraging components built by businesses and communities. This modularity, along with the community-driven standards, makes Kubernetes a strong choice for designing clouds. While Kubernetes (also known as _k8s_) is a good choice for building blocks, it is important to understand that Kubernetes is not secure by default.

In this Topic, we will introduce Kubernetes from a security perspective, acknowledging the potential benefits and consequences of various Kubernetes features and implementations. This Topic assumes that we have already studied GNU/Linux, networking, and docker.

1

(The Linux Foundation, 2022),[https://Kubernetes.io/](https://Kubernetes.io/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1539-1)

## 4.1. Learning Kubernetes Terms

This Learning Unit covers the following Learning Objectives:

- Learn about Kubernetes concepts
- Understand Kubernetes essential terminology

## 4.1.1. Introducing Kubernetes

Rather than a single application, Kubernetes is a collection of modular components designed to work together to achieve distributed systems. In this case, modular means that the software exists in separate programs designed to work together, but allows individual pieces to be replaced or added. The core components come from the developers of Kubernetes, while further components are maintained by other projects, companies, and groups. We won't cover all of the components in this Topic, but let's get started by using Kubernetes.

Rather than manipulating virtual machines or hardware directly, Kubernetes manipulates containers and container networking. Kubernetes can also be installed inside of virtual machines or on bare metal systems before use.

The components of Kubernetes _clusters_ are _abstractions_. Abstractions allow people and systems to interact with clusters via standardized Kubernetes API[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1566-1) YAML, rather than having to manage the components separately. Instead of having several independent services that must be installed, configured, and maintained, components of Kubernetes are developed to be declared via the Kubernetes API. These abstractions save users time by making it easier to develop consistent and repeatable operations across many potential servers and use cases. Abstractions allow components to work better together, and to be more easily integrated.

Not only does Kubernetes have many potential components, it also has a number of implementations or distributions of Kubernetes. These implementations add or remove features, customizing Kubernetes to fit a specific use case or goal.

An implementation of Kubernetes is comprised of select components and configurations that are used as a starting point, then further configured. The Kubernetes implementation we'll use first is minikube,[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1566-2) which is primarily used for demos and developing local tests.

1

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1566-1)

2

(The Linux Foundation, 2022), [https://minikube.sigs.k8s.io/docs/](https://minikube.sigs.k8s.io/docs/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1566-2)

## 4.1.2. Kubernetes Essential Terminology

Before we start the minikube lab machine, let's cover some Kubernetes terminology[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-1). A _Pod_ is a desired state of one or more OCI[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-2) containers. Pods can contain images created with docker,[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-3) as docker also creates and manages OCI containers.

While there are some similarities between Docker and Kubernetes, Kubernetes is more modular, has more features, and fits different needs. Not only can Pods have multiple containers within them, but they can also be managed in more complex ways with Kubernetes compared to containers managed via Docker.

Pods can be created and destroyed in many ways, but the primary way is with _Deployments_. Deployments are a declared set of one or more Pods with additional configuration parameters. These potential parameters are numerous; however, we can simplify these additional Deployment parameters for now into _namespaces_, _labels_, and _replicas_. Namespaces[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-4) leverage Linux kernel namespace capabilities to create logically-grouped segments. Labels build the mappings between Pods and other aspects of Kubernetes, such as Services and namespaces. Replicas represent the desired number of copies of Pods to be deployed in a _cluster_. A Kubernetes cluster is one or more Linux systems that are united via Kubernetes.

Other operating systems, such as Windows, may be able to run Kubernetes; however, Kubernetes was designed for Linux-based systems.

After systems are joined together to form a cluster, operations to distribute applications across those systems can be abstracted via the Kubernetes API. This means that rather than copying an application to many individual servers, the application can be deployed to a Kubernetes cluster by declaring a Deployment in that cluster.

Let's review an example Deployment YAML for an nginx[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-5) Pod.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: demo
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: demo
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
```

> Listing 1 - example Deployment

Kubernetes provides a lot of flexibility, which can be daunting to a new learner. Its potential abstractions empowers people to quickly and consistently design and configure vast distributed systems. While these abstractions may take some time to fully understand, we can begin operating Kubernetes without a full understanding of each component. The consistency of the Kubernetes API YAML is the foundation for this expansive flexibility and modularity. Each Kubernetes API request can be represented as YAML, with the first section (_apiVersion_) defining which Kubernetes API the request is to be sent to.

Kubernetes APIs are _declarative_, meaning that the API requests are declaring a desired state for the cluster. There isn't a hard limit to what the scope of a cluster is, because APIs and features can be added to Kubernetes to expand the scope. However, the default scope of the cluster consists of the containers and container networking.

Kubernetes may be thought of as a container orchestration system. However, it technically removes the need for orchestration, as the functionality is declared rather than requiring a set of steps to be executed. Kubernetes does not provide maintenance, deployment, or CICD; those systems must be built or configured in or around Kubernetes. Kubernetes will continuously attempt to ensure the declared state is achieved, but Kubernetes does not fix all problems for us. In fact, Kubernetes creates many new potential problems that we must understand in order to successfully operate Kubernetes in production.

Before we use Kubernetes in a production environment, there are many topics to study and components to test and review. In this Topic and the following Kubernetes-related Topic, we will explore and test Kubernetes components in lab VMs, building up the knowledge and skills we need to use and secure Kubernetes.

Kubernetes does not dictate which software is used within the cluster. Any software that can run in an OCI container can be used within Kubernetes. Rather than having to directly orchestrate OCI containers, in Kubernetes we can work with Pods, informing Kubernetes of our declared requirements.

Each Pod has its own IP address. Containers within a Pod share an IP and can communicate to each other within the Pod by default. While most Pods have a single container, a Pod may have many containers. One use case for this is having a caching container in the Pod with another application container, so the cache is right next to the application container. For example, we could have a front-end application in a Pod, along with supporting services, and then only expose the front-end port as a service to the outside. Alternatively, those supporting services could be moved into their own Pods for their own Deployments and Services.

Deployments allow us to group Pods together and configure Kubernetes options for them. A Deployment may represent the difference between development and production, different software, different customers, or other potential ways we might want to group containers. In a Deployment, we group containers we want to declare together in the same namespace. For example, if we have a database Pod and a back-end Pod in the same Deployment, that would typically indicate that those containers are associated and are to be deployed together in the same namespace.

Services expose Pod ports, providing automatic service discovery based on Pod labels. _ClusterIP_[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-6) Services only expose Pods within Kubernetes, while the other types of Services expose Pods to the host or other external connections outside of Kubernetes. A _NodePort_[7](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-7) Service assigns a port in the NodePort range 30000 to 32767 to a Pod label that can be accessed from outside of the cluster on each node of the cluster. NodePort is perhaps the most simple way to expose a Pod to the outside; however, is limited by the port range. A _LoadBalancer_[8](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-8) Service also assigns a NodePort port on each node of the cluster that maps the Pod label, in addition to potential provisioning of load balancing between the nodes outside of Kubernetes. If the NodePort value is not defined within a NodePort or LoadBalancer Service, one will be automatically assigned. An _Ingress_[9](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-9) isn't a Service, but a different kind of Kubernetes object. Ingress is used to expose Services outside of the cluster, along with other potential features inside of the cluster.

A NodePort Service can still be used with an external load balancer. The difference is that a LoadBalancer Service can automatically request an external load balancer that has been provisioned. LoadBalancer is much like NodePort, but has additional features. If we are unsure which type of Service to use, LoadBalancer is often a good choice to start.

Kubernetes Ingress is technically different from a Service. Ingress may likely have its own Pods and added software configurations. We will cover Ingress resources in more detail in a later Topic, but for now we should understand Kubernetes Ingress as a way to add in additional routing and potential SSL/TLS termination. We should be careful not to confuse this with the networking terminology "ingress", which refers to inbound network traffic as a whole.

Kubernetes itself has Pods that run the Kubernetes core services in the _kube-system_[10](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1542-10) namespace, as well as system daemons that run outside of Pods. Alongside these core Kubernetes components, additional modular components to Kubernetes also run in the kube-system namespace. These core and modular kube-system components often run services that execute Kubernetes API requests, store the declared state, take action to attempt to make the declared state available, and route requests to the correct Pods. We will cover these kube-system components in a later Topic.

1

(The Linux Foundation, 2021), [https://kubernetes.io/docs/reference/glossary/?fundamental=true](https://kubernetes.io/docs/reference/glossary/?fundamental=true) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-1)

2

(The Linux Foundation, 2020), [https://opencontainers.org/](https://opencontainers.org/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-2)

3

(Docker, 2022), [https://www.docker.com/](https://www.docker.com/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-3)

4

(The Linux Foundation, 2022), [https://Kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/](https://Kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-4)

5

(nginx, 2022), [https://nginx.org/en/](https://nginx.org/en/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-5)

6

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-6)

7

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport](https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-7)

8

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-8)

9

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-9)

10

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#working-with-namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#working-with-namespaces) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1542-10)

## 4.2. Understand Why Kubernetes May Be Useful

This Learning Unit covers the following Learning Objectives:

- Understand why Kubernetes is a good choice for systems design
- Learn about why distributed Kubernetes clusters have benefits
- Discover how Kubernetes provides compute efficiency at scale

## 4.2.1. Why Kubernetes

Although Kubernetes is complex, its goal is to make IT operations easier and more consistent. Kubernetes truly shines as a business or organization grows and scales, empowering teams to work quickly in repeatable, industry-standard ways, across thousands of servers around the world.

Many senior systems engineers can replace Kubernetes or operate at scale without it, but if that senior engineer moves on to another position, it may not be easy for others to understand what was done. Kubernetes aims to solve this problem by ensuring systems are built with community standards. By standardizing, new engineers can train independently, begin a position at an organization that uses Kubernetes, and readily contribute, benefiting the company with their existing skills. If the systems engineering is done entirely in-house, it may take much longer to train new systems engineers on how to use and maintain it.

Maintaining and installing individual Linux-based systems may be trivial to an experienced systems engineer. If an organization has just one system, the need for Kubernetes is much lower than if the organization has many systems. This is because Kubernetes empowers us with tools for running containerized applications across small or large Kubernetes groups of servers with relative ease and efficiency. Rather than having to build or configure automation that manages many separate servers, servers can be grouped into clusters, allowing much of the management to be abstracted to the clusters rather than the individual servers. Clusters can have nodes added or removed, and can move from one set of servers to another with relative ease.

In a distributed multi-node Kubernetes cluster, if a single physical server goes down, the cluster can continue to operate without it by automatically routing traffic away from the broken server. This enables automatic fault tolerance. In addition to the fault tolerance of a distributed cluster, Kubernetes can use our servers and cloud instances more efficiently at scale. This efficiency largely comes from the ability to run many instances of an application across one or more servers, allowing us to maximize utilization of system resources like CPU, RAM, disk, and network, by stacking containers next to each other on the same host.

In a virtual machine, an application might use a fixed directory path and file handle, and without customizing each application instance to not overlap, may not be able to run more than one instance of an application. With containerized applications, we can run many of the same applications with the same application configuration because they are isolated in containers.

Before we build large globally-distributed clusters, we will build a single node cluster in a lab VM, and start to use Kubernetes at a very small scale. Small-scale Kubernetes can still be useful, as it sets us up for growth, helps us test Kubernetes, and helps keep the deployments repeatable. Even if an organization only uses one server, running Kubernetes may make it easier for the organization to move the applications to new servers and clouds in the future.

## 4.3. Start Using Kubernetes

This Learning Unit covers the following Learning Objectives:

- Learn about minikube
- Understand how to deploy Kubernetes in a three node cluster
- Learn about k3s
- Understand how to use microk8s
- Use Kubernetes Deployments and Services
- Develop our knowledge of the differences in Kubernetes implementations

## 4.3.1. Accessing the minikube Lab

We will use a lab VM to learn about minikube in this module.

We can edit the hosts file in our Kali instance to map names to the lab IPs. The IP addresses to use in the hosts file may vary in the third octet, so use the IP address of the actual lab VM.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.51.91  minikubelab
```

> Listing 2 - /etc/hosts entries

We can start the lab machine then set up the hosts file on our Kali instance if we want to use names instead of IP addresses to access the lab machines from Kali.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

minikubelab

## 4.3.2. Getting Started

We will **ssh** to our _minikube_ [1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1547-1) lab VM from our kali machine and start using an implementation of Kubernetes known as minikube. Minikube isn't meant to be used at a larger scale in a distributed design and isn't focused on security, so it's typically used for demos and developers.

```
kali@kali$ ssh offsec@minikubelab
offsec@minikubelab's password:
...cut...
offsec@minikubelab:/home/offsec$
```

> Listing 3 - ssh to the minikube lab

Within the minikube VM, we'll notice some docker images placed in a running docker daemon cache. These were placed ahead of time, so we don't have to pull the images during the lab exercise. We will use docker to facilitate our minikube lab. As the _offsec_ user in the minikube lab machine, we can start minikube.

In the example command to start minikube, we are executing the **minikube** binary, passing the **start** argument and the **--vm-driver=docker** to run on top of docker.

```
offsec@minikubelab:/home/offsec$ minikube start --vm-driver=docker
ðŸ˜„  minikube v1.25.2 on Debian 11.2
âœ¨  Using the docker driver based on existing profile

ðŸ§¯  The requested memory allocation of 2200MiB does not leave room for system overhead (total system memory: 2987MiB). You may face stability issues.
ðŸ’¡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'

ðŸ‘  Starting control plane node minikube in cluster minikube
ðŸšœ  Pulling base image ...
ðŸ”„  Restarting existing docker container for "minikube" ...
ðŸ³  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    â–ª kubelet.housekeeping-interval=5m
ðŸ”Ž  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ðŸŒŸ  Enabled addons: default-storageclass, storage-provisioner
ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

> Listing 4 - Initialize minikube

After starting minikube as a single node cluster, we can begin using it. Let's start by running a default nginx Pod. We will use the tool called _kubectl_ to interact with Kubernetes. Kubectl[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1547-2) is a program written in Go[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1547-3) and compiled as a binary tool that can be used to send API requests to the Kubernetes API. The first argument we will pass to **kubectl** is **run**, which is not something we often do unless debugging or attacking. The next argument is **test-nginx**, which is a name we will give to our Pod. The third argument is **--image=nginx** to reference the OCI container image. The last argument is **--port=80** to set the Pod port to 80. Let's also specify **--image-pull-policy='Never'** to prevent minikube from attempting to pull from the internet.

```
offsec@minikubelab:/home/offsec$ kubectl run test-nginx --image=nginx --port=80 --image-pull-policy='Never'
pod/test-nginx created
```

> Listing 5 - Run an nginx Pod

We can use **kubectl** to query the Kubernetes API for information. In this case, we will **get** the **pods** in the default namespace. Unless a namespace is declared, the API actions will happen in the default namespace.

```
offsec@minikubelab:/home/offsec$ kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
test-nginx   1/1     Running   0          2m12s
```

> Listing 6 - Get Pods with kubectl

Next, let's expose the Pod to an external listener on the host outside of the namespace and outside of Kubernetes. If we don't expose the Pod, it is still potentially available to other Pods. We will say "potentially" in many discussions with Kubernetes, because the assumption depends on implementations and configurations allowing the behavior.

In the next command, we'll use **kubectl** again, but we will use **expose**, exposing the type of resource **pod** as the second argument, with the name **test-nginx** set in the third argument. The fourth argument **--type="NodePort"** is the type of Service we want to use.

```
offsec@minikubelab:/home/offsec$ kubectl expose pod test-nginx --type="NodePort" --port 80
service/test-nginx exposed
```

> Listing 7 - Expose an nginx Pod

The default Service type is ClusterIP, which is used to expose Pods internal to Kubernetes nodes. If we want to expose a Service to the outside, we can use NodePort or LoadBalancer. Or instead we can potentially use an Ingress to route external traffic. With all of these approaches, we can use Pod labels to perform automatic service discovery.

Now that the Pod has a Service and is externally exposed, we can interact with the Pod from outside of Kubernetes. We will interact with our nginx Pod via _cURL_. Minikube has a feature we will use to automatically print out a usable URL for the Service, including the assigned NodePort. Let's use this feature inside of a subshell, curling the output of the Minikube Service feature, which we can conveniently use to provide a valid target.

```
offsec@minikubelab:/home/offsec$ curl -v $(minikube service test-nginx --url)
*   Trying 192.168.49.2:30072...
* Connected to 192.168.49.2 (192.168.49.2) port 30072 (#0)
> GET / HTTP/1.1
> Host: 192.168.49.2:30072
> User-Agent: curl/7.74.0
> Accept: */*
> 
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.21.6
< Date: Sat, 12 Feb 2022 04:39:53 GMT
< Content-Type: text/html
< Content-Length: 615
< Last-Modified: Tue, 25 Jan 2022 15:03:52 GMT
< Connection: keep-alive
< ETag: "61f01158-267"
< Accept-Ranges: bytes
< 
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
* Connection #0 to host 192.168.49.2 left intact
```

> Listing 8 - cURL an nginx Pod

To output the logs, we can use the **kubectl logs** command. Kubernetes has built-in logging aggregation we can use to examine log data.

```
offsec@minikubelab:/home/offsec$ kubectl logs test-nginx
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/02/12 04:39:04 [notice] 1#1: using the "epoll" event method
2022/02/12 04:39:04 [notice] 1#1: nginx/1.21.6
2022/02/12 04:39:04 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2022/02/12 04:39:04 [notice] 1#1: OS: Linux 5.10.0-11-amd64
2022/02/12 04:39:04 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/02/12 04:39:04 [notice] 1#1: start worker processes
2022/02/12 04:39:04 [notice] 1#1: start worker process 32
2022/02/12 04:39:04 [notice] 1#1: start worker process 33
172.17.0.1 - - [12/Feb/2022:04:39:53 +0000] "GET / HTTP/1.1" 200 615 "-" "curl/7.74.0" "-"
```

> Listing 9 - Get test-nginx logs

Next, let's delete the Pod we created. We will pass the **delete** option to **kubectl**, followed by the type of resource we want to delete, in this case a **pod**, and then the name of that resource, in this case **test-nginx**.

```
offsec@minikubelab:/home/offsec$ kubectl delete pod test-nginx
pod "test-nginx" deleted
```

> Listing 10 - Delete test-nginx Pod

We will rarely deploy containers as an individual Pod in Kubernetes. Typically, instead of deploying a single Pod, our Deployment will include one or more Pods via **kubectl apply** and a Deployment YAML.

1

(The Linux Foundation, 2022), [https://minikube.sigs.k8s.io/docs/](https://minikube.sigs.k8s.io/docs/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1547-1)

2

(The Linux Foundation, 2022), [https://Kubernetes.io/docs/reference/kubectl/kubectl/](https://Kubernetes.io/docs/reference/kubectl/kubectl/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1547-2)

3

(Google, 2022), [https://go.dev/](https://go.dev/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1547-3)

## 4.3.3. Declarative YAML Manifests

We will typically deploy from a directory of YAML files, or individual YAML files that we can manage in version control. This way, the desired state of Kubernetes is represented in the YAML. When applying YAML, we will use **kubectl apply** to apply the YAML and **kubectl delete** to delete what was created from that YAML.

We will begin to use **kubectl apply** to declare the desired state of Kubernetes. In this next example, we will declare two namespaces. The **-f** is followed by a YAML file or a directory of YAML files to apply. In this case, our two YAML files create a namespace in the directory.

```
offsec@minikubelab:/home/offsec$ cat /var/lib/production_initial/*
---
kind: Namespace
apiVersion: v1
metadata:
  name: blue
  labels:
    name: blue
...
---
kind: Namespace
apiVersion: v1
metadata:
  name: green
  labels:
    name: green
...

offsec@minikubelab:/home/offsec$ kubectl apply -f /var/lib/production_initial/
namespace/blue created
namespace/green created
```

> Listing 11 - Apply initial namespaces

Let's deploy alternate configurations of the same applications, one version to the green namespace and the other to the blue namespace. The way that we organize Kubernetes is important; the example below demonstrates an organizational scheme of three directories.

```
offsec@minikubelab:/home/offsec$ ls -larth /var/lib/production_*
var/lib/production_initial:
total 16K
drwxr-xr-x  2 root root 4.0K Feb 11 14:23 .
drwxr-xr-x 45 offsec offsec 4.0K Feb 11 15:04 ..
-rw-r--r--  1 offsec offsec   87 Mar  3 14:08 blue_namespace.yml
-rw-r--r--  1 offsec offsec   89 Mar  3 14:08 green_namespace.yml

/var/lib/production_blue:
total 16K
drwxr-xr-x 45 root root 4.0K Feb 11 15:04 ..
-rw-r--r--  1 offsec offsec  313 Feb 12 18:57 blue-service.yml
-rw-r--r--  1 offsec offsec  509 Mar  3 09:17 blue-deployment.yml
drwxr-xr-x  2 offsec offsec 4.0K Mar  3 09:17 .

/var/lib/production_green:
total 24K
drwxr-xr-x 45 root root 4.0K Feb 11 15:04 ..
-rw-r--r--  1 offsec offsec  411 Feb 28 11:55 green-deployment.yml
-rw-r--r--  1 offsec offsec  168 Feb 28 16:36 green-service.yml
drwxr-xr-x  2 offsec offsec 4.0K Mar  4 10:31 .
```

> Listing 12 - Example organization

The first directory is comprised of configurations that apply across the cluster, while the second two are namespace-specific directories that we can apply and delete repeatedly while working on Deployment configurations for the corresponding namespaces. Next, we can apply the YAML files for Deployment and Service in the green namespace. There are two YAML files in this directory: one file for Deployment, and another for the corresponding Service.

```
offsec@minikubelab:/home/offsec$ cat /var/lib/production_green/*
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-prod-falcon
  namespace: green
spec:
  replicas: 1
  selector:
    matchLabels:
      app: green-prod-falcon
  template:
    metadata:
      labels:
        app: green-prod-falcon
    spec:
      containers:
      - name: prod-falcon
        image: "prod-falcon:green"
        imagePullPolicy: Never
        ports:
        - name: prod-falcon
          containerPort: 8000
...
---
apiVersion: v1
kind: Service
metadata:
  name: green-harpoon
  namespace: green
spec:
  selector:
    app: green-harpoon
  ports:
    - port: 8000
  type: NodePort
...

offsec@minikubelab:/home/offsec$ kubectl apply -f /var/lib/production_green/
deployment.apps/green-prod-falcon created
service/green-harpoon created
```

> Listing 13 - Create the green Deployment

We'll notice some broken aspects in our green Deployment and Service that we will use to demonstrate how service discovery works in Kubernetes. Let's examine the YAML files in the **/var/lib/production_green/** directory. The first is **green-deployment.yml**. This file has a Kubernetes API request of _kind_ Deployment. The declaration of _kind_ is the category of API request, while _apiVersion_ points the API request to the correct API of _apps/v1_ within Kubernetes.

The _metadata_ section is where we declare the Deployment name and namespace that the Deployment goes within. After _metadata_, we encounter the _spec_ block, which contains the Deployment specifications, including how many replicas to deploy, the labels to use, and any other configuration options for this Deployment.

```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-prod-falcon
  namespace: green
spec:
  replicas: 1
  selector:
    matchLabels:
      app: green-prod-falcon
  template:
    metadata:
      labels:
        app: green-prod-falcon
    spec:
      containers:
      containers:
      - name: prod-falcon
        image: "prod-falcon:green"
        imagePullPolicy: Never
        ports:
        - name: prod-falcon
          containerPort: 8000
...
```

> Listing 14 - Review the green Deployment

The _containers_ blocks define the containers for the Deployment, including the _image_ name. In the example green Deployment, we have the image _prod-falcon:green_, where the image name is "prod-falcon" and the tag is "green". The Deployment has a name, but the container in the Deployment also has a name of "prod-falcon", along with a _containerPort_ declared as 8000. This declared container port does not set the listening port for the service (that must be done within the container image itself), but instead matches the Deployment to the listening port in the container. If the container is listening on port 3000 but the Deployment has that container port set to 5000, the listener will not be available outside of the container.

Let's note the two _containers_ sections, starting with the first section. An empty block such as this is not typical, but it _is_ a valid API request. Let's remove that line from the file and re-apply the Deployment.

```
offsec@minikubelab:/home/offsec$ kubectl apply -f /var/lib/production_green/
deployment.apps/green-prod-falcon unchanged
service/green-harpoon unchanged
```

> Listing 15 - Re-apply the same green deployment

Although we changed the YAML file, the change didn't alter the Deployment. This is because the change didn't declare anything new

- the extra _containers_ line didn't declare anything in the first place, so removing it had no functional change.

We _do_ need to make a functional change to fix the green Service. Let's examine the green Service and identify the configuration issue. First, we can **curl** the green Service to demonstrate the problem. We will use minikube to get the URL of the service defined in the green Service YAML.

```
offsec@minikubelab:/home/offsec$ curl $(minikube service green-harpoon -n green --url)
curl: (7) Failed to connect to localhost port 30333: Connection refused
```

> Listing 16 - cURL the broken green Service

The Service failed due to a mismatched configuration between the Service and the Deployment. Let's identify this issue by opening up the Service YAML and comparing it to the Deployment YAML. In **/var/lib/production_green/green-service.yml**, we are declaring the Service for the green Deployment.

```
---
apiVersion: v1
kind: Service
metadata:
  name: green-harpoon
  namespace: green
spec:
  selector:
    app: green-harpoon
  ports:
    - port: 8000
  type: NodePort
...
```

> Listing 17 - Review the green Service

Much like the Deployment YAML, we need to declare which _apiVersion_ to use. Next, we encounter the _kind_ of "Service", the metadata naming the Service and setting the Service namespace, followed by the _spec_ specification block. Kubernetes Services perform service discovery automatically based on selector labels. The _app_ metadata value is used to match the Service port to the appropriate Pods.

The label being declared by this Service YAML is "app: green-harpoon", which is not a label we defined in the Deployment. Since we don't have any apps in our Deployment with the label of "green-harpoon", this results into nothing for the Service routes' requests. We can correct this by changing the app selector to match our Deployment "green-prod-falcon", then re-apply.

```
offsec@minikubelab:/home/offsec$ kubectl apply -f /var/lib/production_green/
deployment.apps/green-prod-falcon unchanged
service/green-harpoon configured
```

> Listing 18 - Re-apply the corrected green service

With the app label aligned between the Service and the Deployment, we can **curl** the Service again and validate success.

```
offsec@minikubelab:/home/offsec$ curl $(minikube service -n green green-harpoon --url)
{"title": "404 Not Found"} 
```

> Listing 19 - cURL the corrected green Service

We have reached the green falcon Pod via the Service port. It successfully returned a JSON payload with an HTTP 404 response from the falcon application we deployed in the container in the Pod. We can use the same mechanism we just used for service discovery with the app label to develop networking and security controls within Kubernetes, restricting access to resources based on labels.

Next, let's declare another Deployment and Service. We will call it the blue Deployment. In this case, the blue Deployment deploys an identical version of the same container image, but into a new namespace with a new Service port. The blue Deployment includes an nginx container in the same Pod as the falcon container and has two _replicas_ of the Pod. The blue Service exposes an additional port for nginx.

```
offsec@minikubelab:/home/offsec$ cat /var/lib/production_blue/*
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue-prod-falcon
  namespace: blue
spec:
  replicas: 2
  selector:
    matchLabels:
      app: blue-prod-falcon
  template:
    metadata:
      labels:
        app: blue-prod-falcon
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: Never
        ports:
        - name: http
          containerPort: 80
      - name: prod-falcon
        image: prod-falcon:blue
        imagePullPolicy: Never
        ports:
        - name: prod-falcon
          containerPort: 8000
...
---
apiVersion: v1
kind: Service
metadata:
  name: blue-prod-falcon
  namespace: blue
spec:
  selector:
    app: blue-prod-falcon
  ports:
    - port: 8000
      targetPort: 8000
      nodePort: 30332
      name: falcon
    - port: 80
      targetPort: 80
      nodePort: 30331
      name: nginx
  type: LoadBalancer
...

offsec@minikubelab:/home/offsec$ kubectl apply -f /var/lib/production_blue/
deployment.apps/blue-prod-falcon created
service/blue-prod-falcon created
```

> Listing 20 - Create the blue Deployment

We will query Kubernetes for items in the blue namespace. The command for this next example uses **get** as the first argument to **kubectl**, followed by **-n** to specify a namespace, and finally **blue** to specify the namespace we deployed into.

```
offsec@minikubelab:/home/offsec$ kubectl get all -n blue
NAME                                    READY   STATUS    RESTARTS   AGE
pod/blue-prod-falcon-6d5447db55-7rszn   2/2     Running   0          11s
pod/blue-prod-falcon-6d5447db55-pshxz   2/2/     Running   0          10s

NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                       AGE
service/blue-prod-falcon   LoadBalancer   10.110.179.4   <pending>     8000:30332/TCP,80:30331/TCP   11s

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/blue-prod-falcon   2/2     2            2           11s

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/blue-prod-falcon-6d5447db55   2         2         2       11s
```

> Listing 21 - Get all from the blue namespace

For the green namespace, we will do the same. We'll run **kubectl get all** again, this time using **-n green** to query from the green namespace.

```
offsec@minikubelab:/home/offsec$ kubectl get all -n green
NAME                                     READY   STATUS    RESTARTS   AGE
pod/green-prod-falcon-5f75fd9b6b-xqprg   1/1     Running   0          9s

NAME                    TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
service/green-harpoon   NodePort   10.102.131.68   <none>        8000:32515/TCP   3m55s

NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/green-prod-falcon   1/1    1            1           9s

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/green-prod-falcon-5f75fd9b6b   1         1         1       9s
```

> Listing 22 - Use kubectl to get all from the green namespace

We'll observe "1/1" with the green Pods, as opposed to "2/2" for the blue Pods, since the blue Deployment has two containers in each Pod. We'll also notice two blue Pods and a single green Pod, which is because the blue Deployment has _replicas_ set to "2", while green has _replicas_ set to "1". The blue Service also exposes two ports, one for each of its two Pod containers.

Assuming we are done with our green Deployment for now, let's delete it. The **delete** declaration works the same as the **apply** declaration.

```
offsec@minikubelab:/home/offsec$ kubectl delete -f /var/lib/production_green/
deployment.apps/green-prod-falcon deleted
service/green-harpoon deleted
```

> Listing 23 - Use kubectl to delete the green deployment

With the green Deployment and Service deleted, we shouldn't receive any results by sending **get** to the green namespace with **kubectl**.

```
offsec@minikubelab:/home/offsec$ kubectl get all -n green
No resources found in green namespace.
```

> Listing 24 - Use kubectl to review no Pods in green namespace

The apply and delete actions can be done repeatedly, locally or remotely, either manually or via automation to configure all Kubernetes configurations. A single apply declaration pointed to a directory of files can trigger the configuration of a Kubernetes cluster to the desired state. This means that, using a single declaration, a cluster with 100 nodes could gracefully have applications deployed to each.

While our minikube lab VM is only a single node cluster, let's move on to create a three-node cluster. This will allow us to practice further deployments as we learn more about Kubernetes.

#### Labs

1. What would be the kubectl command to get all Pods in the _development_ namespace in the default output format?

Answer

## 4.3.4. Accessing the Three-node Kubernetes Lab

We will use three VMs in a lab to learn about Kubernetes in this module.

We can edit the hosts file in our Kali instance to map names to the lab IPs. The IP addresses to use in the hosts file may vary in the third octet, so use the IP address of the actual lab VM.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.51.92  k8s
192.168.51.93  k8sWrk1
192.168.51.94  k8sWrk2
```

> Listing 25 - /etc/hosts entries

We can start the lab machines then set up the hosts file on our Kali instance if we want to use names instead of IP addresses to access the lab machines from Kali.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Intro to Kubernetes - k8s three node cluster

## 4.3.5. Starting to use Multi-node Kubernetes Clusters

The core Kubernetes components are developed by the Kubernetes team and the open source community, comprised of over 3,000 people. It takes a large global effort to develop, maintain, and grow the community-based, standards-driven Kubernetes.

The Kubernetes _upstream_ is _the Kubernetes project_ with many different groups and activities, setting standards and initiatives, and releasing the core components and defaults. This top-of-stream release of Kubernetes is simply called _Kubernetes_ or _k8s_. Other implementations of Kubernetes use this Kubernetes code as a starting point.

We will use the plain "vanilla" Kubernetes to create a three-node Kubernetes cluster in a lab. "Vanilla" in this case means the default upstream Kubernetes with no specific implementation, providing us with the core and default modular building components.

We'll create the cluster with one control plane[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1551-1) node and two worker nodes. The control plane represents the cluster nodes that run the Kubernetes API and related Kubernetes components.

On the control plane node, we will initialize the cluster. We will use the program called _kubeadm_ to bootstrap the cluster. Kubeadm is another Golang binary, used primarily for the creation of new Kubernetes clusters. Kubeadm accomplishes this by generating config files and creating the Pods needed to initialize Kubernetes. We will use **kubeadm** manually, however, these actions are commonly automated.

Let's redirect the STDOUT of the initialization to **/root/cluster_init.out** and STDERR to **/root/cluster_error.out**. This will allow us to reference the output in those files without it getting lost in the terminal window. Let's also make sure swap is off with **swapoff -a**, as Kubernetes will refuse to bootstrap with swap enabled by default.

```
kali@kali$ ssh offsec@k8s
offsec@k8s's password:
...cut...

offsec@k8s:/home/offsec$ sudo -i
Password:
...cut...

root@k8s# swapoff -a

root@k8s# kubeadm init > /root/cluster_init.out 2> /root/cluster_error.out
```

> Listing 26 - Use kubeadm to initialize Kubernetes

If initialization is successful, we'll observe a _join_ command within the output. The join command allows us to join worker nodes to the cluster.

```
root@k8s# cat /root/cluster_init.out
[init] Using Kubernetes version: v1.23.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"

...cut...

Your kubernetes control plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.51.92:6443 --token n52bn6.p4yvcqo5y8jfpkjz \
        --discovery-token-ca-cert-hash sha256:8cb2d0d9fb9bbb0463ffb44a73649d12235886906717758175c4715d96718d38 
```

> Listing 27 - Review the output of cluster initialization

Don't use the example join command provided; each cluster initialization has unique parameters.

Let's copy the join command from the lab VM and execute it on the worker nodes to join them to the cluster. We can **ssh** to the worker nodes and execute the **join** from the output as superuser.

```
kali@kali:~$ ssh offsec@k8sWrk1
offsec@k8sWrk1's password:
...cut...

offsec@k8sWrk1:~$ sudo -i
Password: 
...cut...

root@k8sWrk1:~# swapoff -a

root@k8sWrk1:~$ kubeadm join 192.168.51.92:6443 --token n52bn6.p4yvcqo5y8jfpkjz \
        --discovery-token-ca-cert-hash sha256:8cb2d0d9fb9bbb0463ffb44a73649d12235886906717758175c4715d96718d38
...cut...

root@k8sWrk1:~# exit
...cut...

offsec@k8sWrk1:~$ exit
...cut...

kali@kali:~$ ssh offsec@k8sWrk2
offsec@k8sWrk2's password:
...cut...

offsec@k8sWrk2:~$ sudo -i
Password: 
...cut...

root@k8sWrk2:~# swapoff -a

root@k8sWrk2:~# kubeadm join 192.168.51.92:6443 --token n52bn6.p4yvcqo5y8jfpkjz \
        --discovery-token-ca-cert-hash sha256:8cb2d0d9fb9bbb0463ffb44a73649d12235886906717758175c4715d96718d38
...cut...

root@k8sWrk2:~# exit
...cut...

offsec@k8sWrk2:~$ exit
```

> Listing 28 - Manually joining cluster nodes

The lab with the name "k8s" is the control plane node that runs the Kubernetes API and supporting system components. The other two nodes, k8sWrk1 and k8sWrk2, are worker nodes that will run our application Pods. Let's focus on the control plane node to set up the configuration files for kubectl.

After Kubernetes is initialized, the systems with copies of the **/etc/kubernetes/admin.conf** file have full administrative control over the Kubernetes API by default. If **admin.conf** is copied over to another remote system, such as a user workstation or Kali machine, that system could potentially control Kubernetes remotely via the Kubernetes API. In our lab, we will use kubectl to control this cluster on the k8s lab VM.

Let's create a directory named **~/.kube** and store the **/etc/kubernetes/admin.conf** file in **~/.kube/config**. This will place the cluster administrative config file in the default config path for kubectl. The Kubernetes API authentication is accomplished via client authenticated TLS (mTLS), with kubectl as the client and the Kubernetes API as the server. We could copy **admin.conf** to our Kali system and control Kubernetes remotely, however, for this lab, we'll use **kubectl** on the k8s lab VM.

Some Kubernetes implementations will use JWT[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1551-2), or potentially other tokens, to authenticate instead of mTLS. When a kubectl config has a token instead of a certificate and key pair, that shows that token authentication is being used instead of mTLS.

```
root@k8s:~# mkdir ~/.kube 

root@k8s:~# cp /etc/kubernetes/admin.conf ~/.kube/config
```

> Listing 29 - Setup kubectl config file

Once the nodes are joined into the cluster, we will need to install a _Container Network Interface_ plugin manifest. CNI refers to a specification as well as a set of libraries for configuring networks for Linux containers. A CNI plugin is a modular component for Container Network Interface. Vanilla Kubernetes has only a minimal CNI plugin, allowing us to select a CNI plugin that we want to use. We'll install a _Calico_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1551-3) CNI plugin _manifest_. A manifest is Kubernetes API YAML, often containing many YAML files concatenated together. This Calico manifest will install a default Calico configuration that can be further configured later on.

This manifest will apply Calico networking to our cluster. In this lab, the container images are already pulled; if not, this manifest would pull the Calico containers from **docker.io**. We won't review the manifest in this Topic, as there are other subjects to focus on for now.

```
root@k8s:~# kubectl apply -f /var/lib/network/calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-kube-controllers created
```

> Listing 30 - Apply a Calico CNI plugin manifest

Once our selected CNI plugin is in place and our nodes joined, we've achieved the starting point for our example cluster. We can examine the nodes of the cluster with **kubectl** and the **get nodes** arguments.

```
root@k8s:~# kubectl get nodes
NAME      STATUS   ROLES                  AGE     VERSION
k8sWrk1   Ready    <none>                 1h54m   v1.23.3
k8sWrk2   Ready    <none>                 1h54m   v1.23.3
k8s   Ready    control plane,master   1h58m   v1.23.3
```

> Listing 31 - Use kubectl get cluster nodes

Now that the cluster is initialized, nodes are joined, and a CNI plugin is in place, we can move on to creating Deployments and other configurations in this cluster. Commonly, clusters are created with automation that includes joining the worker nodes. For the purpose of learning more about how clusters are formed, we did this process manually.

Unlike minikube, our vanilla Kubernetes has everything running as root. We can give any other user administrative access to kubectl by providing **/etc/kubernetes/admin.conf** to them. We will use root on the k8s lab VM for now, understanding that any user and any system with a kubectl binary and a copy of **/etc/kubernetes/admin.conf** has the potential to completely control our Kubernetes cluster by default.

```
root@k8s:~# kubectl apply -f /var/lib/production_blue/
deployment.apps/blue-prod-falcon created
service/blue-prod-falcon created
```

> Listing 32 - Deploy the blue Deployment

Another difference between our single-node cluster and this three-node cluster is that the three-node cluster has two types of nodes: worker nodes and control plane nodes. Worker nodes host Pods for our Deployments while control plane nodes run the Kubernetes API, along with the other core and modular kube-system components. We will explore these kube-system components in a later Topic.

After a cluster is initialized and worker nodes added, further worker nodes can be added with relative ease. Control plane nodes can be added as well; however, adding additional control plane nodes typically requires significantly more steps. The kubectl config on a control plane node by default has full administrative access to Kubernetes. This results in control plane nodes being sensitive servers in general, as control of the control plane results in control of the entire cluster. We will use the control plane node in our lab named k8s to run kubectl.

1

(The Linux Foundation, 2022), [https://Kubernetes.io/docs/concepts/overview/components/](https://Kubernetes.io/docs/concepts/overview/components/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1551-1)

2

(IETF, 2015), [https://datatracker.ietf.org/doc/html/rfc7519](https://datatracker.ietf.org/doc/html/rfc7519) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1551-2)

3

(Calico, 2022), [https://projectcalico.docs.tigera.io/about/about-calico](https://projectcalico.docs.tigera.io/about/about-calico) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1551-3)

## 4.3.6. Kubectl for YAML and JSON Output

In the next example, let's use kubectl to write JSON and YAML files containing Kubernetes state information on our Pods. The **-o** option for **kubectl** sets the output format. We might use this feature to capture data that is loaded into a monitoring system.

```
root@k8s:~# kubectl get pods -n blue -o JSON > blue_state.json

root@k8s:~# kubectl get pods -n blue -o YAML > blue_state.yml

root@k8s:~# less blue_state.json
```

> Listing 33 - Kubectl output formats

The output files are not used to declare Deployments, but rather to reference or capture some specifics of the running state. If we want to create a new Deployment YAML from scratch, kubectl provides Deployment templates to help us accomplish this. We can generate such a template with **kubectl create deployment**, followed by the name of the Deployment we want, using **--image=nginx** to specify our nginx container image, then **--dry-run=client**, directing kubectl to check the action rather than execute it, and finally **-o yaml** to output the YAML template to STDOUT. We will redirect STDOUT to a file called **demo.yml**.

```
root@k8s:~# kubectl create deployment demo --image=nginx --dry-run=client -o yaml > demo.yml

root@k8s:~# cat demo.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: demo
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: demo
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

root@k8s:~# kubectl apply -f demo.yml
deployment.apps/demo created

root@k8s:~# kubectl delete -f demo.yml
deployment.apps "demo" deleted
```

> Listing 34 - Kubectl output Deployment template

## 4.3.7. Imperative vs Declarative kubectl Usage

In addition to the files declared, we can use the Kubernetes API in an imperative way rather than a declarative way, by "patching" and doing kubectl one-line commands instead of declaring from YAML, as we did with the first nginx Pod demo in the minikube lab. We will use a kubectl command on our lab cluster to scale the blue Deployment up to eight replicas. Managing the state of Kubernetes imperatively like this is not ideal in many situations, as having the YAML in source control provides better auditing, testing, and control.

In the next example, we can imperatively scale the blue Deployment replicas to eight. Kubectl converts the imperative command into a declarative request for the Kubernetes API. Afterwards, we can use kubectl to describe Pods in the blue namespace, and pipe the command output to a **grep** for "Node" to print out which servers the Pods are running on in the blue namespace. The IP addresses and allocations can be different between labs.

```
root@k8s:~# kubectl scale deployment blue-prod-falcon --replicas=8 -n blue
deployment.apps/blue-prod-falcon scaled

root@k8s:~# kubectl describe pods -n blue | grep "Node:" 
Node:         k8sWrk1/192.168.51.93
Node:         k8sWrk2/192.168.51.94
Node:         k8sWrk2/192.168.51.94
Node:         k8sWrk2/192.168.51.94
Node:         k8sWrk2/192.168.51.94
Node:         k8sWrk1/192.168.51.93
Node:         k8sWrk1/192.168.51.93
Node:         k8sWrk1/192.168.51.93
```

> Listing 35 - Kubectl imperative scaling example

Now we will scale the Deployment declaratively with our YAML file by changing the _replica_ value in the YAML and running a **kubectl apply**. However, we can edit the YAML to our liking. In this example we will use **sed**. Let's edit this file and change the file lines with "replicas: 2" to "replicas: 3".

```
root@k8s:~# sed -i 's/replicas: 2/replicas: 3/g'  /var/lib/production_blue/blue-deployment.yml

root@k8s:~# kubectl apply -f /var/lib/production_blue/
deployment.apps/blue-prod-falcon configured
service/blue-prod-falcon unchanged

root@k8s:~# kubectl get pods -n blue
NAME                                READY   STATUS        RESTARTS   AGE
blue-prod-falcon-6d5447db55-9c4jf   2/2     Terminating   0          12h
blue-prod-falcon-6d5447db55-dc5qb   2/2     Running       0          17h
blue-prod-falcon-6d5447db55-fs9hj   2/2     Running       0          12h
blue-prod-falcon-6d5447db55-g9dj2   2/2     Terminating   0          12h
blue-prod-falcon-6d5447db55-lqn2w   2/2     Terminating   0          12h
blue-prod-falcon-6d5447db55-m8knq   2/2     Terminating   0          17h
blue-prod-falcon-6d5447db55-n9fxz   2/2     Running       0          12h
blue-prod-falcon-6d5447db55-x452r   2/2     Terminating   0          12h
```

> Listing 36 - Update a Deployment manifest and apply it with kubectl

In many situations, only a single replica is needed. Additional replicas are usually only needed for capacity-related optimization. If a Deployment can use one replica, we should use only one replica.

As the Pods are created and destroyed in our Deployments, let's note how each Pod name is unique. The format for the individual Pod names in a Deployment has the name of the Pod as defined in the YAML, with a unique id value appended to it. If we want to view the logs of a Pod in a Deployment with kubectl, we can copy the name of the Pod and use it as an argument to a **kubectl logs** command.

Rather than repeatedly having to query and copy the Pod name, we might choose to set a shell variable to the Pod name we are using, so we don't have to remember it for each command. In the next example, let's use **kubectl** to print all of the Pods in the blue namespace, pipe the output to **tail** to grab the last two lines, use **head** to grab the first of those two, then **awk** to print the first column, which is the Pod name. The name of the Pod in the lab will change as the unique id string is randomly set, so that each Pod has a unique name. In the following listing, the shell variable _cid_ is set to "blue-prod-falcon-6d5447db55-nnptp".

```
root@k8s:~# cid=$(kubectl get pods -n blue | tail -n2 | head -n1 | awk '{print $1}')

root@k8s:~# echo $cid
blue-prod-falcon-6d5447db55-nnptp
```

> Listing 37 - Assign a shell variable to a Pod name

To break down this long command, we can remove piped parts to review what we are accomplishing by examining the output of each command.

```
root@k8s:~# kubectl get pods -n blue | tail -n2 | head -n1
blue-prod-falcon-6d5447db55-nnptp   2/2     Running   0          6d20h

root@k8s:~# kubectl get pods -n blue | tail -n2
blue-prod-falcon-6d5447db55-nnptp   2/2     Running   0          6d20h
blue-prod-falcon-6d5447db55-n9fxz   2/2     Running   0          6d14h

root@k8s:~# kubectl get pods -n blue
NAME                                READY   STATUS    RESTARTS   AGE
blue-prod-falcon-6d5447db55-nnptp   2/2     Running   0          6d20h
blue-prod-falcon-6d5447db55-n9fxz   2/2     Running   0          6d14h
```

> Listing 38 - Reviewing command outputs

Next, we will use **kubectl** to query for logs using the _$cid_ shell variable we created. Because the blue Pod has two containers in it, when we use kubectl to query for logs with the Pod name, we also need to specify which container in the Pod to read the logs from. After we close the terminal shell session, the _cid_ variable disappears along with that shell session. In some situations, using shell variables with kubectl can be helpful to save time or automate some tasks.

```
root@k8s:~# kubectl logs $cid -n blue
error: a container name must be specified for pod blue-prod-falcon-6d5447db55-nnptp, choose one of: [nginx prod-falcon]

root@k8s:~# kubectl logs $cid nginx -n blue
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/02/12 04:12:32 [notice] 1#1: using the "epoll" event method
2022/02/12 04:12:32 [notice] 1#1: nginx/1.21.6
2022/02/12 04:12:32 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2022/02/12 04:12:32 [notice] 1#1: OS: Linux 5.10.0-11-amd64
2022/02/12 04:12:32 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/02/12 04:12:32 [notice] 1#1: start worker processes
2022/02/12 04:12:32 [notice] 1#1: start worker process 33
2022/02/12 04:12:32 [notice] 1#1: start worker process 34
```

> Listing 39 - Use kubectl to read Pod logs

Alternatively to hunting for specific Pod logs, we can also automatically select some logs for a given Deployment. We can accomplish this by specifying **deployment/** followed by the name of the Deployment after the slash with no space.

```
root@k8s:~# kubectl logs deployment/blue-prod-falcon -n blue nginx
Found 2 pods, using pod/blue-prod-falcon-6cbcc775fc-q2mx5
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/06/22 19:16:58 [notice] 1#1: using the "epoll" event method
2022/06/22 19:16:58 [notice] 1#1: nginx/1.21.6
2022/06/22 19:16:58 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2022/06/22 19:16:58 [notice] 1#1: OS: Linux 5.10.0-11-amd64
2022/06/22 19:16:58 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/06/22 19:16:58 [notice] 1#1: start worker processes
2022/06/22 19:16:58 [notice] 1#1: start worker process 31

```

> Listing 40 - Use kubectl to read Deployment logs

This approach automatically selects a Pod to print the logs from. The same syntax can be used to get logs from other objects in Kubernetes. Instead of people using kubectl to read logs directly like this, we might opt to have another service or system reading them, perhaps sending them to a separate centralized location outside of the cluster.

We typically want to automate as much as possible with Kubernetes to reduce human error and save time. However, doing tasks manually is a good way to learn what is happening. Kubernetes configurations can become complex, but automation can reduce that complexity. Some, but not all, automation might rely on shell scripts. We will learn more about automating Kubernetes actions in later Topics.

Before we move on to learning more about Kubernetes components, let's take some time to review other common Kubernetes implementations. Although there are more implementations than we can cover in this Topic, we will introduce some of the more popular ones.

How to use the cloud_grader for the exercises:

Many of the cloud challenges that involve making or configuring something related to the Topic will require the construction of a _challenge state_. The Offensive Security cloud_grader is a way to measure a challenge state in a Topic to determine whether the required configuration has been accomplished. A challenge question may have a prompt describing the requirements for the challenge state required to complete that challenge.

After creating the challenge state in the lab, the _root_ user on the control plane node can run **cloud_grader** to measure the state of the lab. If the completed challenge state is measured, a message like "OS{80f6eb78c3bdf5d36eddaafcc46edcf2}" will be printed. We can copy the string, including the "OS" and curly braces, and paste it as the answer to the challenge.

In order to create a completed challenge state, the objects in the lab Kubernetes cluster must match the described aspects in the challenge prompt. If the cloud_grader encounters extra _ReplicaSets_, extra _Roles_, or if Pods are in the process of terminating or restarting, the challenge state and the string will not match (meaning our solution is "incorrect").

After completing a challenge, please be sure to run **kubectl delete -f manifest.yml** to clean up after a challenge before moving on to the next one. This will ensure that you have a clean challenge state as you enter the next challenge.

#### Labs

1. What would the kubectl command be like for applying the manifest **/home/bob/k8s/qa.yml**?

Answer

2. Which generated file provides full administrative control over a cluster by default? (enter the absolute path)

Answer

3. In the three node Kubernetes lab, construct the following: create a _maroon_ namespace, deploy the _falcon_ application to it with two replicas in a Deployment named _maroon-prod-falcon_, and a _LoadBalancer_ Service that exposes the maroon-prod-falcon Deployment on port 30348. On the control plane node, run the **/usr/sbin/cloud_grader** application afterwards to check your work and get a flag to submit below.

Answer

## 4.3.8. Accessing the k3s Lab

In the next section we will use a lab VM to run k3s.

We can edit the hosts file in our Kali instance to map names to the lab IPs. The IP addresses to use in the hosts file may vary in the third octet, so use the IP address of the actual lab VM.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.51.95  edge1
```

> Listing 41 - /etc/hosts entries

We can start the lab machine then set up the hosts file on our Kali instance if we want to use names instead of IP addresses to access the lab machines from Kali.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

edge1

## 4.3.9. Rancher and k3s

Rancher[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1558-1), part of the OpenSUSE[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1558-2) family, has an enterprise Kubernetes implementation called _Rancher Kubernetes Engine_ (RKE). Rancher is a complete software stack for organizations using or adopting containers. Rancher makes it easier to run Kubernetes and support enterprise needs. With RKE and the supporting tooling, Rancher provides an expanded web interface and integrations to provide a Kubernetes platform at installation.

_k3s_ is part of the Rancher ecosystem, and is just one tool out of hundreds. The Rancher ecosystem contains many applications and tooling, which are all open source. k3s has reasonable security defaults for lightweight environments.

k3s can be used without the rest of Rancher, which is how we will use it in our lab. k3s is much more minimal than upstream Kubernetes, as well as more minimal than RKE, the full-scale Kubernetes implementation by Rancher. In k3s, we can use less RAM and CPU to enable a more lightweight implementation of Kubernetes, with less than 100 MB binary and half the memory usage of upstream Kubernetes.

Instead of _etcd_,[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1558-3) the default Kubernetes API storage system, k3s uses _sqlite_[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1558-4) for the API storage. The sqlite storage can be swapped out for etcd, mysql, or postgres. k3s also hides much of the complexity of TLS options, and includes the components it needs to run pre-installed. This is useful for small-scale clouds, as well as edge, IoT, embedded systems, and CICD. k3s has been used by military organizations for "tactical edge" computing, connecting combat and field equipment to distributed k3s systems.

Let's access a k3s lab VM, which has already been initialized as a single node. We will query for the nodes with **k3s kubectl** and another **get** request.

```
[root@edge1 ~]# k3s kubectl get nodes
NAME    STATUS   ROLES                  AGE     VERSION
edge1   Ready    control-plane,master   5d22h   v1.29.5+k3s1

[root@edge1 ~]# k3s kubectl get nodes -o wide
...cut...

[root@edge1 ~]# k3s kubectl describe nodes
...cut...
```

> Listing 42 - Use k3s kubectl to get and describe cluster node information

In the k3s Kubernetes implementation, the **/etc/kubernetes** directory is not used. Instead, we have **/etc/rancher/k3s/**, which contains **k3s.yaml** rather than **admin.conf**, as well as **/etc/rancher/node/** containing a file called **password**. This password file is used to add nodes to the cluster in k3s. In k3s, we call worker nodes _agents_. Different implementations of Kubernetes may use variations on terminology.

```
[root@edge1 ~]# ls /etc/kubernetes
ls: cannot access '/etc/kubernetes': No such file or directory

[root@edge1 ~]# ls /etc/rancher/k3s/
k3s.yaml

[root@edge1 ~]# cat /etc/rancher/node/password
0f54516ef54192a9f0d8d8d544ff52ad
```

> Listing 43 - k3s admin config file

Kubectl and Kubernetes API access can do a lot, especially when we have _cluster-admin_ level privileges. Cluster-admin is the "root" account for a Kubernetes cluster. For each of these implementations, the cluster-admin is our initial default account. We can check if we have cluster-admin access in several ways; one approach is using the **can-i** argument to **kubectl**. This example queries Kubernetes to check if our current user can take any Kubernetes API action. K3s has an embedded kubectl that can be called via **k3s kubectl**.

```
[root@edge1 ~]# k3s kubectl auth can-i '*' '*'
yes
```

> Listing 44 - Use k3s kubectl get check account access

To list all accounts in the cluster, let's perform a **get** on cluster roles from **clusterroles.rbac.authorization.k8s.io** with **-A** to retrieve from all namespaces.

```
[root@edge1 ~]# k3s kubectl get clusterroles.rbac.authorization.k8s.io -A
NAME                                                                   CREATED AT
cluster-admin                                                          2024-06-19T20:08:14Z
system:discovery                                                       2024-06-19T20:08:14Z
system:monitoring                                                      2024-06-19T20:08:14Z
system:basic-user                                                      2024-06-19T20:08:14Z
system:public-info-viewer                                              2024-06-19T20:08:14Z
system:aggregate-to-admin                                              2024-06-19T20:08:14Z
system:aggregate-to-edit                                               2024-06-19T20:08:14Z
system:aggregate-to-view                                               2024-06-19T20:08:14Z
system:heapster                                                        2024-06-19T20:08:14Z
system:node                                                            2024-06-19T20:08:14Z
system:node-problem-detector                                           2024-06-19T20:08:14Z
system:kubelet-api-admin                                               2024-06-19T20:08:14Z
system:node-bootstrapper                                               2024-06-19T20:08:14Z
...cut...
```

> Listing 45 - Use kubectl list all accounts

Most accounts are created automatically by manifests for different components. We can also create accounts manually for systems or people, giving them a specific scope of access, such as within a namespace. We will explore Kubernetes accounts further in another Topic.

In order to use local container images in k3s, we have to import them. We can use **k3s ctr images** to manage the container images available locally.

```
[root@edge1 ~]# k3s ctr image import /root/nginx.tar
unpacking docker.io/library/nginx:latest (sha256:edd8cc85d0cc216f72a41f41259f72c7fc660158230dbdba87d3c6040a386ac3)...done
```

> Listing 46 - K3s image import

When we use a local image in k3s YAML, we also need to specify that the image should not be pulled with "imagePullPolicy: Never", otherwise k3s will attempt to pull from the internet.

1

(Rancher, 2022), [https://rancher.com/docs/](https://rancher.com/docs/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1558-1)

2

(OpenSUSE, 2022), [https://www.opensuse.org/](https://www.opensuse.org/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1558-2)

3

(etcd, 2022), [https://etcd.io/](https://etcd.io/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1558-3)

4

(sqlite, 2022), [https://sqlite.org/index.html](https://sqlite.org/index.html) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1558-4)

#### Labs

1. In the k3s lab, construct the following: update the blue Deployment to remove the _prod-falcon:blue_ container from the Pod, scale up the replicas to six, and update the nginx Service port to 30444. Run **/usr/sbin/cloud_grader** afterwards to check your work and get a flag to submit below.

Answer

2. In the k3s lab, delete any existing Deployments then construct the following: in the blue namespace, deploy five nginx Pods named _abo_, _nabut_, _smur_, _cru_, and _cran_. Create a _NodePort_ Service that exposes the _abo_ Pod on port 30322. Create a _ClusterIP_ Service that exposes nabut with port 11 and targetPort 80. Run **/usr/sbin/cloud_grader** afterwards to check your work and get a flag to submit below.

Answer

## 4.3.10. Accessing the microk8s Lab

In the next section we will use a lab VM to run microk8s.

We can edit the hosts file in our Kali instance to map names to the lab IPs. The IP addresses to use in the hosts file may vary in the third octet, so use the IP address of the actual lab VM.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.51.96  microk8slab

```

> Listing 47 - /etc/hosts entries

We can start the lab machine then set up the hosts file on our Kali intance if we want to use names instead of IP addresses to access the lab machines from Kali.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

microk8slab

## 4.3.11. Microk8s

Microk8s is a Kubernetes implementation made by Canonical, the creators of Ubuntu. Microk8s works well as a single node cluster, is very easy to add additional control plane or worker nodes, and is the easiest Kubernetes implementation to install on just about any Linux system via _Canonical Snap_.[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1562-1) The microk8s Snap is like a packaging container that allows microk8s dependencies to be self-contained.

Microk8s is useful for quick installations and smaller implementations. For these reasons, it might be used by developers for dev tests, or used for edge or IoT cases. Like k3s, microk8s will not use locally-cached docker images and will always attempt to pull images from the internet by default, resulting in our image pulls failing in our microk8s lab VM. We can fix this in the lab by importing docker images with **microk8s ctr**.

```
root@microk8slab:~# docker save nginx > nginx.tar

root@microk8slab:~# microk8s ctr image import nginx.tar
unpacking docker.io/library/nginx:latest (sha256:544da96b39fda3b54e8d0f3d4b60c2a4e75fec7d6c2638015f6ccad544771a11)...done
```

> Listing 48 - Microk8s local images

Let's perform a **describe** on **all** resource types with the **-A** option for all namespaces. We'll then pipe that output to **less** to read it. This command describes the state of the entire cluster.

One thing unique to microk8s is that, by default, microk8s uses **microk8s.kubectl** instead of kubectl because it bundles kubectl inside of the Snap.

```
root@microk8slab:~# microk8s.kubectl describe all -A | less
Name:                 calico-node-msd4b
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 microk8slab/192.168.1.179
Start Time:           Thu, 03 Mar 2022 18:30:49 +0000
Labels:               controller-revision-hash=775fc58c48
                      k8s-app=calico-node
                      pod-template-generation=2
Annotations:          kubectl.Kubernetes.io/restartedAt: 2022-03-03T18:30:49Z
                      scheduler.alpha.Kubernetes.io/critical-pod: 
Status:               Running
IP:                   192.168.1.179
IPs:
  IP:           192.168.1.179
...cut...
```

> Listing 49 - Use kubectl to describe all cluster Kubernetes objects

1

(Canonical, 2022), [https://snapcraft.io/canonical-livepatch](https://snapcraft.io/canonical-livepatch) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1562-1)

## 4.3.12. Other Kubernetes Implementations

_Minikube_ is another implementation of Kubernetes created by the Kubernetes team. Minikubes are created for demos and small-scale local use. The Kubernetes team also offers other implementations of Kubernetes used for prototyping and development that we will not be covering.

_OpenShift_ is RedHat's[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1567-1) Kubernetes implementation. OpenShift takes upstream Kubernetes and engineers it for enterprise use cases, on-prem or in public cloud. OpenShift is an implementation of Kubernetes that has more secure defaults, helping to manage some of the decision making when it comes to Kubernetes component configurations. Despite many decisions being made for us in this Kubernetes implementation, it still allows the Kubernetes platform to be customized to the organization's needs to some degree. OpenShift has some pre-built integration options for non-Kubernetes infrastructure, as well as support and added enterprise features.

OpenShift is a popular Kubernetes implementation for organizations that are already using the RedHat ecosystem. Although OpenShift is much like Rancher, the OpenShift install process is more complicated than Rancher, as many RedHat products are, with licensing activations and guided menus. Both OpenShift and Rancher take the building blocks of Kubernetes and provide structure and added software to further support adoption, monitoring, integration, and more security-focused defaults, including additional security tooling.

_Managed Kubernetes_ is a service solution in which a third-party provider hosts Kubernetes on-demand as a service offering. This can be a strong choice if an organization does not have Linux experts on the team, allowing the team to be platform specialists and focus less on the underlying systems and hardware running Kubernetes. There is still a strong need for Linux knowledge with professional Kubernetes use; however, organizations can ease that demand to some degree by using a managed Kubernetes cluster. Some examples of managed Kubernetes service offerings include: _Elastic Kubernetes Service_ (EKS),[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1567-2) _Azure Kubernetes Service_ (AKS),[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1567-3) and _Google Kubernetes Engine_ (GKE).[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fn-local_id_1567-4) There are managed Kubernetes configurations and options for both Rancher and OpenShift in EKS, AKS, and GKE as well.

Using a managed Kubernetes service does not secure our cluster for us, nor does it ensure that Kubernetes is used correctly. The cloud provider shared responsibility model divides the responsibilities between the service provider and the customer platform team.

There are more implementations of Kubernetes that we did not cover in this Topic. Kubernetes provides the following building blocks: we can make our own implementation of it, modify an existing implementation, or use a specific implementation that makes some choices for us regarding the components and configuration.

1

(RedHat, 2022), [https://www.redhat.com/en](https://www.redhat.com/en) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1567-1)

2

(Amazon Web Services, 2022), [https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1567-2)

3

(Microsoft, 2022), [https://docs.microsoft.com/en-us/azure/aks/](https://docs.microsoft.com/en-us/azure/aks/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1567-3)

4

(Google, 2022), [https://cloud.google.com/kubernetes-engine/](https://cloud.google.com/kubernetes-engine/) [â†©ï¸Ž](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-i-39800/start-using-kubernetes-39832/other-kubernetes-implementations-39815#fnref-local_id_1567-4)

#### Labs

1. Which Kubernetes implementation is developed by Canonical?

Answer

2. Instead of imperative configurations, we typically want ____ configurations.

Answer