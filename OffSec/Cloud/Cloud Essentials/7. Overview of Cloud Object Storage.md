This Module covers the following Learning Units:

- Understanding Object Storage
- Adoption of Object Storage in the Cloud

In this Module, we'll learn the basics of the _cloud object storage service_ that we often find in cloud deployments.

First, we will explore _object storage_ and compare it with other storage technologies. Next, we'll examine why this approach has gained popularity in cloud computing. Finally, we'll spend most of this module engaging with Amazon Web Services (AWS) Cloud Object Storage, concentrating on key features for proper deployment while highlighting common misconfigurations to avoid.

If we're not experienced with AWS, it would be beneficial to review the [_Introduction to AWS_](https://portal.offsec.com/library/learning-modules/introduction-to-aws-51813/introduction-to-aws/introduction-to-aws) Module to learn the basics of this public cloud service provider.

It's also recommended to read the _IAM Essentials_ Module to understand the basics of [_Identification and Access Management_](https://portal.offsec.com/library/learning-modules/iam-essentials-166980/iam-essentials/iam-essentials) (IAM) in AWS. We'll need to apply many of those concepts when learning about authorization in our lab's object storage.

## 7.1. About the Public Cloud Labs

Before we jump in, let's run through a standard disclaimer.

This Module uses OffSec's Public Cloud Labs for challenges and walkthroughs. **OffSec's Public Cloud Labs** is a type of lab environment that will complement the learning experience with hands-on practice. In contrast to our more common VM labs found elsewhere in OffSec Learning materials (in which learners will connect to the lab through a VPN), learners using the Public Cloud Labs will interact directly with the cloud environment through the Internet.

OffSec believes strongly in the advantages of learning and practicing in a hands-on environment, and we believe that the OffSec Public Cloud Labs represent an excellent opportunity for both new learners and practitioners who want to stay sharp.

Please note the following:

1. The lab environment should not be used for activities not described or requested in the learning materials you encounter. It is not designed to serve as a playground to test additional items that are out of the scope of the learning module.
    
2. The lab environment should not be used to take action against any asset external to the lab. This is specifically noteworthy because some modules may describe or even demonstrate attacks against vulnerable cloud deployments for the purpose of describing how those deployments can be secured.
    
3. Existing rules and requirements against sharing OffSec training materials still apply. Credentials and other details of the lab are not meant to be shared. OffSec monitors activity in the Public Cloud Labs (including resource usage) and monitors for abnormal events that are not related to activities described in Learning Modules.
    

Activities that are flagged as suspicious will result in an investigation. If the investigation determines that a student acted outside of the guidelines described above, or otherwise intentionally abused the OffSec Public Cloud Labs, OffSec may choose to rescind that learner's access to the OffSec Public Cloud Labs and/or terminate the learner's account.

Progress between sessions is not saved. Note that a Public Cloud Lab that is restarted will return to its original state. After an hour has elapsed, the Public Cloud Lab will prompt to determine if the session is still active. If there is no response, the lab session will end. Learners can continue to manually extend a session for up to ten hours. The learning material is designed to accommodate the limitations of the environment. No learner is expected or required to complete all of the activities in a module within a single lab session. Even so, learners may choose to break up their learning into multiple sessions with the labs. We recommend making a note of the series of commands and actions that were completed previously to facilitate the restoration of the lab environment to the state it was in when the learner left. This is especially important when working through complex labs that require multiple actions.

## 7.2. Understanding Object Storage

This Learning Unit covers the following Learning Objectives:

- Compare File, Block, and Object Storage
- Identify Why We Should Implement Object Storage

The concept of object storage in computing has its roots in the late 1990s. It was developed to address the need for a more cost-efficient and scalable storage solution to face the rapidly increasing consumption of data.

Let's explore more about this technology to better understand why it has become widely adopted, especially in the cloud computing realm.

## 7.2.1. File, Block and Object Storage Comparison

When learning about storage technologies, we need to think beyond only the physical hardware where data is stored, such as hard disks, solid state drives, tapes, etc. We also need to understand other instances, such as how this data is stored and retrieved in storage devices and also how it's transmitted over the network when the storage is not directly attached to the computer that needs it.

In the 90s. the two principal approaches were _File-based_ and _Block-based_ storage. Even today, in our day-to-day, we use both of these technologies in our computers.

File-based storage organizes data into a hierarchical structure of folders and files. The most common usage of this approach is in our operating systems where we use this approach to organize our files.

Block-based storage divides data into fixed-size blocks of bytes to store and transfer. Even though we don't interact directly with block-based storage, we use it every day in our computers to store data.

In general, file systems can be thought of as an _abstraction layer_ on top of block devices. When we create a file in our computers, this file is converted into bytes, those bytes are grouped in blocks of a fixed size, and finally stored in block devices (usually our local disk). Additional metadata is stored within the file so that the file system can retrieve the blocks and present it as a file we can interact with.

Both approaches work together in most cases where the storage device is attached directly to the computer. However, significant differences start to show when computing and storage are in separate locations.

Larger storage deployments implement dedicated servers with the unique purpose of storing data. These storage servers can transmit data over the network as blocks or files. Depending on different scenarios, one option will be more convenient than the other.

When data is transferred as files, we often refer to this storage as a _Network Attached Storage_ (NAS). Technologies that we normally use in NAS are network protocols such as _Server Message Block_ (SMB), _File Transfer Protocol_ (FTP), and _Network File System_ (NFS).

Alternatively, there are _Storage Area Networks_ (SAN) where data is transferred as blocks. The two principal technologies used in SANs are _Fibre Channel_ (FC) and _Internet Small Computer Systems Interface_ (iSCSI). Two major downsides of SAN technologies were that they lost performance over major distances and they were more expensive than NAS.

Implementing SAN requires specific hardware and a dedicated network architecture that makes it more expensive than NAS. However, it improves performance and reliability significantly when transferring large amounts of data.

At the time that NAS and SAN were the robust options for storage, object-based storage was born as an alternative that improves scalability and cost-efficiency. In this approach, data is stored as objects in a flat-based storage space, with no hierarchy or folder structure, making data retrieval straightforward and fast.

The device that stores objects is known as _Object-based Storage Device_ (OSD). To transmit objects over the network we normally use the _Hyper Text Transfer Protocol_ (HTTP) and an _Application Programming Interface_ (API), so there is no need for specific network protocols like in the file-based approach.

When working with object storage, we use the word "objects" instead of "files" to refer to the stored data. An object contains not only the file data but also includes the object's metadata and a unique identifier used by the OSD to identify and locate the objects.

We can't claim that one of these storage technologies is better than the other. Different scenarios call for different approaches, making it crucial to discern which to use based on our specific needs.

## 7.2.2. Why Object Storage?

Although there are solutions to implement object storage on-premises, its adoption is more notable in cloud computing.

One of its primary advantages is scalability. Unlike traditional file or block storage, object storage can handle massive amounts of unstructured data, scaling out as needed without compromising performance. Furthermore, object storage is cost-effective, especially for storing large data that doesn't require frequent modification.

These features permit cloud providers to offer a relatively low-cost storage service that is charged on-demand. Given that implementing a highly-available and fault-tolerant storage solution requires major investment and maintenance costs, companies find such cloud services very attractive.

Additionally, due to the flat storage space, it becomes easier to store and retrieve files. Normally, this only requires some interaction with an API, which is very normal in application development.

In summary, object storage's scalability, reduced complexity, and cost-effectiveness make it an ideal solution for managing large amounts of data, particularly in cloud-native applications, static websites and content, multimedia delivery, big data storage such as Data Lakes, and backup and archiving.

## 7.3. Adoption of Object Storage in AWS

This Learning Unit covers the following Learning Objectives:

- Review the Basics of Object Storage in AWS
- Implement Authorization for Objects
- Configure Publicly Available Objects
- Configure Encryption in Object Storage
- Protect Data With Versioning
- Integrate With Other Cloud Services

Due to its ease of scale, object storage became a promising technology for the cloud computing model. One notable early example is the _Amazon Simple Storage Service_ (Amazon S3), the first AWS service launched in 2006, followed by other public cloud provider services such as _Microsoft Azure Blob Storage_ and _Google Cloud Storage_.

In the following sections, we'll learn the most common features we'll find in the object storage services while implementing some of them in our AWS labs.

## 7.3.1. Accessing the Getting Started Lab

We will use an AWS account and the _AWS CLI tool_ to follow along with the commands and activities in the following sections.

If AWS CLI is not already installed on our Kali machine, we can do so by running **sudo apt update** and then the **sudo apt -y install awscli** command. For instructions on installing the tool in other distributions or operating systems, we can check the official [_Install AWS CLI_](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) documentation page.

```
kali@kali:~$ sudo apt update
[sudo] password for kali:
Hit:1 http://kali.download/kali kali-rolling InRelease
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
882 packages can be upgraded. Run 'apt list --upgradable' to see them.

kali@kali:~$ sudo apt -y install awscli
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
...
```

> Listing 1 - Installing AWS CLI

After starting the lab, we can retrieve the credentials to configure the AWS CLI. We'll run the **aws configure** command. It will prompt for four things. We'll submit the _Access Key ID_ and the _Secret Access Key_ retrieved after starting the lab. We'll set the _Default region name_ to **us-east-1** and the _Default output format_ to **json**.

To validate that the credentials are working correctly, we can use **aws sts get-caller-identity**, which will only return information about the credentials if they are valid.

```
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 2 - Configuring AWS CLI and validating the user

The credentials provided include a Username, Password, and the URL for accessing the Management Console. This web interface provides a user-friendly way to manage the AWS account, although we encourage the usage of AWS CLI. Mastering programmatic interaction with the API is a valuable skill for cloud professionals. Checking the configurations in the Management Console will be an extra mile activity in most of the sections.

For now, we'll simply start the lab to retrieve credentials and validate that we can interact with AWS API via CLI.

## 7.3.2. Getting Started with Buckets

To start using object storage in AWS, we need to interact with the S3 service and create a [_Bucket_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html).

A bucket is the flat storage space where we'll put the objects. We can't store any data if we don't create at least one bucket. At the bucket level, we can enable and manage some features such as _encryption_, _access management_, _versioning_, and _authorization policies_.

Buckets are global resources in AWS, which means that each bucket name must be unique across all AWS accounts in all the AWS Regions. After we create a bucket, we can't rename it unless we delete it, so it's important to plan a naming convention from the beginning and check the [bucket naming guidelines](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html).

In AWS CLI we have two command sets to interact with the S3 service:

- The _s3_ set of high-level commands provides a simpler interface for common S3 operations including creating a bucket, listing buckets, and putting/getting objects. If we have experience working in Linux, we'll notice that these commands are similar to those used for file manipulation in the operating system.
    
- The _s3api_ command set provides a low-level interface offering more granular control and advanced operations. It closely resembles interacting with the actual S3 API.
    

To check all available commands in each set, we can run **aws s3 help** and **aws s3api help**, respectively.

Let's dive into a quick review of interacting with the cloud object storage service in AWS. Our current user simulates an admin user with enough permissions to interact with the S3 service.

To start, we'll create a bucket by running **aws s3 mb s3://data-bucket-$RANDOM-$RANDOM**. In this command, we are using the _s3_ high-level command set, we are running the **mb** ("make bucket") command, and we are submitting a single argument: the bucket name in the _"s3://bucket_name" format,_ also known as [S3Uri](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/index.html). We need a unique name, so we are appending random numbers using the shell environment variable **$RANDOM** to avoid conflicts with a pre-existing bucket from another lab.

Bucket names are global. It's important to add a random value to avoid getting an error if we choose a bucket name that already exists in another account.

```
kali@kali:~$ aws s3 mb s3://data-bucket-$RANDOM-$RANDOM
make_bucket: data-bucket-20260-11366
```

> Listing 3 - Creating a S3 bucket via AWS CLI

We received confirmation that the bucket was successfully created and the bucket's name. Had there been any errors, we would have received a "make_bucket failed" message. We can also confirm from the bucket's name that our shell converted the $RANDOM variable to random integers.

Next, let's list the buckets in the account with the **aws s3 ls** command.

The names of the buckets in this lab include a randomly-generated suffix (-qaajpqsg) to prevent name collisions with other labs. This suffix will change every time you start the lab.

```
kali@kali:~$ aws s3 ls
2024-01-21 15:10:36 offseclab-copy-me-qaajpqsg
2024-01-21 15:10:36 offseclab-delete-me-qaajpqsg
2024-01-21 15:31:22 data-bucket-27453-6730
```

> Listing 4 - Listing all buckets in the account

In the output, we can find our newly-created bucket along with two other buckets that were created for this lab. We'll interact more with these buckets later.

By default, we can create up to 100 buckets in the AWS account with the possibility of requesting a quota increase to a maximum of 1,000 buckets. In the next section, we'll learn how to interact with objects in the buckets.

#### Labs

1. What is the equivalent command to list all the buckets using the s3api set?

```
Write only the name of the command. For example, in aws s3api somecommand the expected answer is somecommand.
```

Answer

## 7.3.3. Interacting with Objects

Let's interact with these buckets further. To list the objects in a bucket, we can also use the **ls** command, specifying the bucket name (or s3URI) as an argument. For example, let's list the content of the two new buckets we found. We'll use both ways to specify the bucket.

You will need to modify the bucket name in the following command to match the bucket name in your lab.

```
kali@kali:~$ aws s3 ls offseclab-copy-me-qaajpqsg
2024-01-21 15:10:38         33 copyme.txt

kali@kali:~$ aws s3 ls s3://offseclab-delete-me-qaajpqsg
                           PRE data/
```

> Listing 5 - Listing the objects in the buckets

Although the output is different, we can check that there are objects in the buckets. The **copy-me** bucket has one object (**copyme.txt**). The **delete-me** bucket lists a [Prefix](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html) identified by the string _"PRE"_. Unlike a traditional file system, Amazon S3 doesn't use hierarchy to organize its objects, however, the concept of a folder can be emulated using prefixes.

To better understand prefixes, let's try an alternative way of listing the objects by using the **s3api list-objects-v2** command. We'll specify the "delete-me" bucket with **--bucket**.

```
kali@kali:~$ aws s3api list-objects-v2 --bucket offseclab-delete-me-qaajpqsg
{
    "Contents": [
        {
            "Key": "data/data000",
            "ETag": "\"1eea14f9de9411296a34eb7c70195f60\"",
            "Size": 33,
            "StorageClass": "STANDARD"
        },
        {
            "Key": "data/data001",
            "ETag": "\"1eea14f9de9411296a34eb7c70195f60\"",
            "Size": 33,
            "StorageClass": "STANDARD"
        },
        {
            "Key": "data/data002",
            "ETag": "\"1eea14f9de9411296a34eb7c70195f60\"",
            "Size": 33,
            "StorageClass": "STANDARD"
        }
    ],
    "RequestCharged": null
}
```

> Listing 6 - Listing objects in a bucket using the s3api list-objects-v2 command

This command returns more information about the objects, including the _Key_, which is the unique name of the object within the bucket. Let's take the first object as an example. We might think that the name of the object is **data000** and it's inside a directory called **data**. In fact, the whole object name is "data/data000". Prefixes help us organize the data in a filesystem-like structure.

There is no max bucket size or limit to the number of objects that we can store in a bucket. We can store all of the objects in a single bucket, or we can organize them across several buckets. There is no difference in performance whether we use many buckets or just a few.

We'll use the **cp** and **mv** commands to copy and move objects respectively. The syntax for both commands is similar: **aws s3 cp/mv _source_ _destination_** where:

- _Source_ can be a local file or an S3 object in a bucket (specified with the s3Uri)
    
- _Destination_ can be a local path or an S3 bucket
    

Because S3 buckets are global resources, we can specify buckets from any account. The operation will succeed as long as we have the proper authorization to read/write in those buckets.

The following listing shows the command to download the object in the **copy-me** bucket to the current directory of our local machine by copying it. We then use **cat** to dump the content of the text file.

```
kali@kali:~$ aws s3 cp s3://offseclab-copy-me-qaajpqsg/copyme.txt .
download: s3://offseclab-copy-me-qaajpqsg/copyme.txt to copyme.txt

kali@kali:~$ cat copyme.txt
Copy this file to the data bucket
```

> Listing 7 - Copying an object from a bucket to the local machine and reading the content

The output and absence of error messages indicate that the download operation executed successfully. The file contains the instructions to copy it to the "data" bucket (the bucket we created previously). Let's run this by using the **cp** command again. This time, both the origin and destination are S3 buckets.

```
kali@kali:~$ aws s3 cp s3://offseclab-copy-me-qaajpqsg/copyme.txt s3://data-bucket-27453-6730
copy: s3://offseclab-copy-me-qaajpqsg/copyme.txt to s3://data-bucket-27453-6730/copyme.txt
```

> Listing 8 - Copying and object from one bucket to another

This time the response indicates the "copy" operation from one bucket to another.

We can now deduce that to upload an object to the bucket, we can use the same **cp** command. The origin should be a local file, and the destination should be the bucket to upload the file.

We can upload an object up to 5GB in size with the **cp** command. For larger objects (up to 5TB) we need to use a [multipart upload operation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html). We can check the documentation for more details about [uploading objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/upload-objects.html).

Finally, we can delete a bucket with the command **rb** (Remove Bucket) and we can delete an object with **rm**. We need to submit the bucket or the object name in the s3Uri format. For example, let's delete the "delete-me" bucket.

To delete a bucket, it needs to be empty first. That means we have to remove all the objects inside it. We can empty a bucket by using **rm --recursive** and specifying the bucket name. Then, we can remove the bucket.

```
kali@kali:~$ aws s3 rm s3://offseclab-delete-me-qaajpqsg --recursive
delete: s3://offseclab-delete-me-qaajpqsg/data/data001
delete: s3://offseclab-delete-me-qaajpqsg/data/data000
delete: s3://offseclab-delete-me-qaajpqsg/data/data002

kali@kali:~$ aws s3 rb s3://offseclab-delete-me-qaajpqsg 
remove_bucket: offseclab-delete-me-qaajpqsg
```

> Listing 9 - Emptying and deleting the delete-me bucket

With the first command, we receive confirmation of all the deleted objects in the bucket. Then we receive a response confirming the "remove_bucket" operation and, by the absence of an error message, we can conclude that it succeeded.

We need to keep in mind that the _s3_ commands are implemented by AWS CLI to provide a user-friendly set of commands to interact with buckets and objects. Behind the curtains, the tool communicates with the corresponding S3 API endpoint.

The following table showcases examples of _s3_ commands alongside their corresponding _s3api_ commands and S3 API endpoints.

|S3 command|s3api command|S3 API endpoint|
|---|---|---|
|mb|create-bucket|CreateBucket|
|ls|list-buckets|ListBuckets|
|rm|delete-object|DeleteObject|
|rb|delete-bucket|DeleteBucket|
|cp||GetObject, PutObject, CopyObject|

> 1 - Examples of S3 commands with the corresponding API endpoint

The table illustrates that the _s3api_ command sets align more directly with the actual API. Some _s3_ commands, such as **cp**, don't have an equivalent _s3api_ command because they interact with different endpoints, depending on the action. For instance, using **cp** to copy a file from an S3 bucket to our local filesystem invokes the _GetObject_ endpoint, while the reverse operation utilizes _PutObject_. When copying an object between buckets, it employs the _CopyObject_ endpoint.

This section introduced many of the common operations we'll perform with buckets. However, except for some specific management tasks, it is uncommon to interact directly with buckets like this. Typically, applications run these operations programmatically. We'll cover an example of this by the end of the Module. In the next sections, we'll focus on some particular features and security practices for implementing object storage.

#### Labs

1. What option we can send to the **aws s3api get-object** command to download an object only if it has been modified since a given date (e.g. 2024-04-20)?

```
Write only the name of the option. Example: --option
```

Answer

## 7.3.4. Authorization in Object Storage

To proceed with the follow along, please start the lab found within **Resources** at the end of this section. Continue after initiating the lab.

To follow along with the lab of this section, we need to retrieve the access key pair and use them to configure AWS CLI. After that, we'll list the buckets in the account to make sure the credentials are working fine.

In this lab, bucket names start with a unique prefix to prevent collisions with buckets from other learners. While we'll skip the prefix in our discussions, remember to use the complete bucket name whenever you're running a command that involves the bucket.

```
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws s3 ls
2024-02-10 12:02:57 civvjqbh-offseclab-acl
2024-02-10 12:02:57 civvjqbh-offseclab-restrict-me
2024-02-10 12:02:57 civvjqbh-offseclab-restricted
```

> Listing 10 - Configuring AWS CLI and listing the buckets in the account to validate credentials

We find three buckets in the account. We'll work with them throughout this section while learning some authorization concepts.

In AWS, there are two methods for authorizing access to objects and buckets: _Bucket Policies_ and _Bucket Access Control Lists_ (ACLs).

ACLs are a legacy configuration and are currently disabled by default.

A bucket policy is a type of resource-based policy, meaning it is directly linked to a specific resource, such as a bucket in this case. Buckets inherently have an implicit "deny" statement. To permit access to users outside the AWS account, we must explicitly allow it through a bucket policy.

To start working with resource-based policies, we should be familiar with IAM and Identity-based policies. We can refer to the _IAM Essentials_ Module to reinforce these concepts if needed.

IAM users in the same account where the bucket was created are not affected by the implicit "deny" because buckets are owned by the account, not individual IAM users.

Let's check the following example of a policy document that allows the user _Bob_ from external AWS account _111111111111_ to list a bucket called **somebucket** and retrieve all objects in it.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowBucketAccessToBob",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::111111111111:user/Bob"
            },
            "Action": [
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::somebucket",
                "arn:aws:s3:::somebucket/*"
            ]
        }
    ]
}
```

> Listing 11 - Policy document allowing a user from an external account to list the bucket and get objects

Let's analyze the policy document from top to bottom.

The _Version_ element at the beginning specifies the version of the policy language syntax rules. Current best practices recommend using the latest version, i.e., "2012-10-17".

_Statement_ is an array that must have at least one statement object. Each statement includes information about a single permission and consists of the following elements: _Sid_, _Effect_, _Action_, and _Resource_.

_Sid_ is an optional element that allows us to provide a descriptive identifier for the policy statement. This is useful if we have several statements and we need a way to quickly identify what each of them does.

_Effect_ can be either "Allow" or "Deny" and determines whether the statement allows or denies access.

_Principal_ specifies the user, account, service, or other entity that is allowed or denied access to a resource.

_Action_ is an array that describes the specific actions that will be allowed or denied. To describe the action, we start writing the service name prefix (e.g. S3, Lambda), followed by the name of the action. In our sample policy, we are specifying two _S3_ actions: _GetObject_ and _ListBucket_.

_Resource_ is an array that specifies the resources to which the actions apply. We are specifying two resources: the bucket and all the objects in it. We are using the "*" wildcard to specify all objects within the bucket.

We don't have an external user in this lab, so we can't apply this policy. Instead, we'll make some modifications. We'll write a policy statement to _explicitly_ **Deny** the same _ListBucket_ and _GetObject_ actions to our current IAM user, **cloudadmin**. We'll deny these actions on the **offseclab-restrict-me** bucket. The modified policy should appear like the example below. Let's save this policy in our current directory with the name **bucket-policy.json**.

We can get the account ID from the output of the **sts get-caller-identity** command.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Principal": {
                "AWS": "arn:aws:iam::accountID:user/cloudadmin"
            },
            "Action": [
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::civvjqbh-offseclab-restrict-me",
                "arn:aws:s3:::civvjqbh-offseclab-restrict-me/*"
            ]
        }
    ]
}
```

> Listing 12 - Modified Policy document denying the cloudadmin user to list the bucket and get its objects

In the policy above, we modified the _Effect_, _Principal_, and _Resource_ elements. We also removed the _Sid_ element to show that it's optional. We'll save the modified policy document in our current directory with the name _bucket-policy.json_.

Before applying the policy to the **offseclab-restrict-me** bucket, let's confirm that we can currently list the bucket's content.

```
kali@kali:~$ aws s3 ls civvjqbh-offseclab-restrict-me
2024-02-10 12:02:59          0 some-object.txt
```

> Listing 13 - Listing the content of the offseclab-restrict-me bucket

We find that the bucket has one object, indicating that currently we have permission to list the bucket.

Now, let's associate our modified policy with the "restrict-me" bucket by running the **aws s3api put-bucket-policy** command. We need to submit two arguments: the **--bucket** name, and the **--policy** document using the prefix **file://** to instruct AWS CLI to read the policy from the given file.

```
kali@kali:~$ aws s3api put-bucket-policy --bucket civvjqbh-offseclab-restrict-me --policy file://bucket-policy.json
```

> Listing 14 - Attaching a bucket policy to a bucket

The command doesn't return a response, but the absence of an error message indicates that the policy was attached successfully. Otherwise, we'll need to check the error and fix the policy.

Alternatively, we can use the _Policy Editor_ in the Management Console to edit and attach the policy.

![[OffSec/Cloud/Cloud Essentials/z. images/bbe4379e22ab47bd3561f90c06d45f2a_MD5.gif]]

Figure 1: Accessing the Policy Editor in the Management Console

After applying the policy, we would expect that the _cloudadmin_ user is unable to list the **restrict-me** bucket. Let's test this by listing the bucket.

```
kali@kali:~$ aws s3 ls civvjqbh-offseclab-restrict-me

An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied
```

> Listing 15 - Listing the bucket and getting an "Access Denied" error

The error message indicates that access is now denied, as expected after applying the policy.

If we don't receive this error, we can check the policy associated with the **offseclab-restricted** bucket. This bucket was already configured with the desired policy.

To get the policy associated with a bucket, we can run **aws s3api get-bucket-policy**, specifying the **--bucket** name. Since the default output format in AWS CLI is JSON, and the policy itself is also a JSON object, we can enhance readability by adding the **--output text** argument to modify the output format. Then, we can pipe this output (the policy) to the **jq** tool, which is a JSON parser for the command line. This will add indentation and break lines to the JSON object, making it more reader-friendly.

```
kali@kali:~$ aws s3api get-bucket-policy --bucket civvjqbh-offseclab-restricted --output text | jq
{
  "Version": "2012-10-17",
  "Statement": [
    {
      ...cut...
      "Resource": [
        "arn:aws:s3:::civvjqbh-offseclab-restricted",
        "arn:aws:s3:::civvjqbh-offseclab-restricted/*"
      ]
    }
  ]
}

```

> Listing 16 - Getting the bucket policy and improving the readability in the terminal using the jq JSON parser tool

This command shows the bucket policy in an easier-to-read JSON format. If the **jq** tool is not available in our system, we can omit it and we'll still be able to list the policy. Alternatively, we can use the Management Console to read this policy.

Bucket ACLs are another way to grant access to buckets and objects. However, this deprecated method is no longer required for modern use cases.

To enable or disable ACLs, we need to configure the [Bucket Ownership](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-ownership-retrieving.html) setting at the bucket level. To check this configuration, we'll run **aws s3api get-bucket-ownership-controls**, specifying the **--bucket** name.

Let's create a bucket, and then check the default ownership setting.

```
kali@kali:~$ aws s3 mb s3://test-bucket-$RANDOM-$RANDOM
make_bucket: test-bucket-24012-42424

kali@kali:~$ aws s3api get-bucket-ownership-controls --bucket test-bucket-24012-42424
{
    "OwnershipControls": {
        "Rules": [
            {
                "ObjectOwnership": "BucketOwnerEnforced"
            }
        ]
    }
}
```

> Listing 17 - Getting the bucket ownership setting of a new bucket to check if ACLs are enabled by default

We received a JSON response. We need to check the value of the _"ObjectOwnership"_ setting. _"BucketOwnerEnforced"_ means that ACLs are disabled. Any other possible value (_ObjectWriter_ and _BucketOwnerPreferred_) means that ACLs are enabled.

Now, let's check if ACLs are disabled in the "offseclab-acl" bucket.

```
kali@kali:~$ aws s3api get-bucket-ownership-controls --bucket civvjqbh-offseclab-acl
{
    "OwnershipControls": {
        "Rules": [
            {
                "ObjectOwnership": "BucketOwnerPreferred"
            }
        ]
    }
}
```

> Listing 18 - Getting the bucket ownership setting of the "offseclab-acl" bucket

We can check that the value of the _ObjectOwnership_ setting is "BucketOwnerPreferred", which means that ACLs are enabled. To disable ACLs, we need to change this setting to "BucketOwnerEnforced".

We can run **aws s3api put-bucket-ownership-controls** to configure this setting. We'll need to submit the **--bucket** name and the **--ownership-controls** rules. The rules are specified using the syntax **"Rules=[{ObjectOwnership=_Value_}]"**, where the value can be _BucketOwnerEnforced_, _BucketOwnerPreferred_, or _ObjectWriter_.

This command doesn't return any output after modifying the bucket, but we can validate the change by running the **get-bucket-ownership-controls** command.

```
kali@kali:~$ aws s3api put-bucket-ownership-controls --bucket civvjqbh-offseclab-acl --ownership-controls "Rules=[{ObjectOwnership=BucketOwnerEnforced}]"

kali@kali:~$  aws s3api get-bucket-ownership-controls --bucket civvjqbh-offseclab-acl
{
    "OwnershipControls": {
        "Rules": [
            {
                "ObjectOwnership": "BucketOwnerEnforced"
            }
        ]
    }
}
```

> Listing 19 - Disabling ACLs in the "offseclab-acl" bucket and checking the configuration

We confirmed that the setting was applied successfully and ACLs are now disabled in the bucket.

Before disabling ACLs, it's important to ensure that a bucket policy is in place to substitute the authorization previously granted by ACLs. We can find more details on the [Prerequisites for disabling ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-ownership-migrating-acls-prerequisites.html) AWS documentation page.

In summary, the key points for authorizing access to buckets and objects in AWS are the use of bucket policies instead of ACLs, and ensuring that these policies grant only the specific access required.

In this section, we've configured authorization for IAM identities within the same or an external AWS account. However, with cloud object storage, we can also grant access to entities not affiliated with AWS. In the next section, we'll explore how to configure publicly accessible buckets and objects, and discuss important security considerations to keep in mind during this process.

#### Labs

1. Add another statement to the policy document **bucket-policy.json** that allows the _lab_ IAM user to list the "offseclab-restrict-me" bucket, but denies access to retrieve any object from the bucket.

Answer

## 7.3.5. Publicly Available Objects

To proceed with the follow-along, please start the lab found within **Resources** at the end of this section. Continue after initiating the lab.

The S3 service was designed for easy integration with web applications and simplified data retrieval. Besides using an AWS account to interact with buckets and objects, we can do this through the HTTP protocol via any web browser. Let's try this out.

After configuring the credentials in AWS CLI, let's list the buckets in the account.

In this lab, bucket names start with a unique prefix to prevent collisions with buckets from other learners. While we'll skip the prefix in our discussions, remember to use the complete bucket name whenever you're running a command that involves the bucket.

```
kali@kali:~$ aws s3 ls
2024-02-15 17:22:06 zstdlgcf-offseclab-private
2024-02-15 17:22:06 zstdlgcf-offseclab-public
```

> Listing 20 - Listing the buckets in the account

We can construct the URL to list a bucket or get an object by using the _bucket-name_, the _region-code_ (e.g. _us-east-1_), and the _key-name_ of the object. The following listing shows the crafted URL. This format is called [virtual-hosted-style](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html#virtual-hosted-style-access) request.

If we omit the _key-name_ of an object, accessing the bucket's URL will list its content, assuming that the _s3:ListBucket_ permission is granted to any user.

```
https://bucket-name.s3.region-code.amazonaws.com/key-name
```

> Listing 21 - Request S3 Object Using URL Format in Virtual Hosted Style

Using this format, we can construct the URL to list both buckets:

- [https://s3.us-east-1.amazonaws.com/zstdlgcf-offseclab-private](https://s3.us-east-1.amazonaws.com/zstdlgcf-offseclab-private)
- [https://s3.us-east-1.amazonaws.com/zstdlgcf-offseclab-public](https://s3.us-east-1.amazonaws.com/zstdlgcf-offseclab-public)

Let's try listing both buckets with these URLs via a web browser. As an example, we'll use **curl**, a command line web client.

We'll receive an XML response. The command line doesn't format this output, but if we access the links via a web browser, we'll find the responses easier to interpret.

```
kali@kali:~$ curl http://s3.us-east-1.amazonaws.com/zstdlgcf-offseclab-public
<?xml version="1.0" encoding="UTF-8"?>
<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>FQ085B595HFP1KGY</RequestId><HostId>ma2VBmZ9QHK9gFaRsvQUp/YQ3B6w1lWD7YVYM8BmUVRjThg70ARnRCI4+YMvEPlVA5FIlNDGQF0=</HostId></Error>

kali@kali:~$ curl http://s3.us-east-1.amazonaws.com/zstdlgcf-offseclab-private
<?xml version="1.0" encoding="UTF-8"?>
<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Name>zstdlgcf-offseclab-private</Name><Prefix></Prefix><Marker></Marker><MaxKeys>1000</MaxKeys><IsTruncated>false</IsTruncated><Contents><Key>some-object.txt</Key><LastModified>2024-02-10T22:22:08.000Z</LastModified><ETag>&quot;b53227da4280f0e18270f21dd77c91d0&quot;</ETag><Size>12</Size><Owner><ID>0560a617a6fa9f34ae6f331c9a614a0b98d6500657ec846eb2d96905b32cf42d</ID><DisplayName>cloudadmin</DisplayName></Owner><StorageClass>STANDARD</StorageClass></Contents></ListBucketResult>
```

> Listing 22 - Listing the buckets via HTTP

When attempting to access the _offseclab-public_ bucket, we received an _Access Denied_ error, whereas accessing the _offseclab-private_ bucket successfully returned an XML list of objects. This outcome is contrary to the intended access settings based on the buckets' names. To correct this, we need to adjust their configurations, ensuring the public bucket is listable by the public, while the private bucket remains inaccessible to unauthorized users.

To allow public access to buckets and objects, we can use either bucket policies or ACLs. Since we previously learned that ACLs are a deprecated feature, we'll focus solely on using bucket policies.

In the previous section, we analyzed a bucket policy that allows access to an entity in an external AWS Account. In this section, the new policy **public-access.json** contains a small change in the _Principal_ element to allow public access.

Make sure to replace the bucket name in the policy with the correct name in your lab.

The policy presented here serves as an example to illustrate poor security practices. It should not be used as a reference for future implementation.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowPublicAccess",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::zstdlgcf-offseclab-public",
                "arn:aws:s3:::zstdlgcf-offseclab-public/*"
            ]
        }
    ]
}
```

> Listing 23 - Analyzing a bucket policy that allows access to the internet

We are using a wildcard **"*"** as the _Principal_ element to specify _All_ possible entities, including anonymous users. We'll save this policy in our local Kali machine with the name **public-access.json**.

Next, let's associate the policy with the **offseclab-public** bucket.

```
kali@kali:~$ aws s3api put-bucket-policy --bucket zstdlgcf-offseclab-public --policy file://public-access.json

An error occurred (AccessDenied) when calling the PutBucketPolicy operation: Access Denied
```

> Listing 24 - Putting the public-access policy to the offseclab-public bucket

We received an _"Access Denied"_ error. Normally, when it's an authorization issue, the error points out the action that is not allowed to execute, but in this case, we didn't receive much information.

The reason for the error is that, to protect customers from unintentionally sharing buckets publicly, AWS implemented the [_Block Public Access_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html) feature as additional protection which, by default, prevents granting public access using policies (or ACLs).

These settings are configured at the bucket level. Let's check the default configuration in the public bucket by running **aws s3api get-public-access-block** and submitting the **--bucket** name.

```
kali@kali:~$ aws s3api get-public-access-block --bucket zstdlgcf-offseclab-public
{
    "PublicAccessBlockConfiguration": {
        "BlockPublicAcls": true,
        "IgnorePublicAcls": true,
        "BlockPublicPolicy": true,
        "RestrictPublicBuckets": true
    }
}
```

> Listing 25 - Getting the Public Access Settings of the offseclab-public bucket

The output shows that this configuration provides four settings. Let's focus solely on the settings related to policies. More details about the four options can be found in the [_Block public access settings_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html#access-control-block-public-access-options) documentation page.

Activating the _BlockPublicPolicy_ option (setting it to TRUE) makes the S3 service deny requests to attach any policy that permits public access to the bucket and its objects. This is why we encountered an "Access Denied" message earlier. If the bucket already had a policy allowing public access, enabling this option to TRUE won't revoke that access.

On the other hand, activating the _RestrictPublicBuckets_ option blocks all forms of public access to the bucket and its objects, regardless of whether a bucket policy exists that grants such access.

Notice that both settings are related to public access granted by policies and not by ACLs. For the latter, we need to configure the other two options, which are out of scope for this Module.

In summary, to enable the policy and permit public access, we need to set both the _BlockPublicPolicy_ and _RestrictPublicBuckets_ options to FALSE.

To configure the Public Access settings in the offseclab-public bucket, we'll run **aws s3api put-public-access-block**, submitting two arguments: the **--bucket** name and the **--public-access-block-configuration**. We'll need to specify the following four settings in the syntax:

_BlockPublicAcls=boolean,IgnorePublicAcls=boolean,BlockPublicPolicy=boolean,RestrictPublicBuckets=boolean_

The boolean value can be either _true_ (enable) or _false_ (disable).

Let's configure the offseclab-public bucket, setting the _BlockPublicPolicy_ and _RestrictPublicBuckets_ to false. Then we'll try to reenable the public-access.json policy.

```
kali@kali:~$ aws s3api put-public-access-block --bucket zstdlgcf-offseclab-public --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=false,RestrictPublicBuckets=false

kali@kali:~$ aws s3api put-bucket-policy --bucket zstdlgcf-offseclab-public --policy file://public-access.json
```

> Listing 26 - Configuring the offseclab-public bucket to allow public access via policies

The command we used to configure the public access settings didn't produce any output, but the lack of an error message suggests it was successful. This time, after applying the policy, we didn't encounter the "Access Denied" error, indicating that this command was also executed successfully.

Now, let's test by trying to access the bucket again in the browser or via the command line using curl.

```
kali@kali:~$ curl http://s3.us-east-1.amazonaws.com/zstdlgcf-offseclab-public
<?xml version="1.0" encoding="UTF-8"?>
<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Name>zstdlgcf-offseclab-public</Name><Prefix></Prefix><Marker></Marker><MaxKeys>1000</MaxKeys><IsTruncated>false</IsTruncated><Contents><Key>some-object.txt</Key><LastModified>2024-02-10T22:22:08.000Z</LastModified><ETag>&quot;b53227da4280f0e18270f21dd77c91d0&quot;</ETag><Size>12</Size><Owner><ID>0560a617a6fa9f34ae6f331c9a614a0b98d6500657ec846eb2d96905b32cf42d</ID><DisplayName>f.ortiz+cloud</DisplayName></Owner><StorageClass>STANDARD</StorageClass></Contents></ListBucketResult>
```

> Listing 27 - Accessing the public bucket via command line after granting public access

This time, we are listing the objects instead of getting an _Access Denied_ error.

It's not a good security practice to grant public access to list buckets. We should fix this by modifying the bucket policy. We can also complete the lab by fixing the **offseclab-private** bucket, which is publicly accessible. Let's complete both tasks as an exercise.

#### Labs

1. Complete all the activities of the lab. The grader will test the following tasks:

```
1. The offseclab-public bucket cannot be listed anonymously. The objects should remain publicly accessible.
2. The offseclab-private bucket and objects are not publicly accessible.
```

Answer

## 7.3.6. Accessing the Data Protection Lab

To follow along for two sections, we'll use the same lab.

For now, let's start the lab, retrieve credentials, and validate that we can list the buckets in the account.

## 7.3.7. Data Encryption

Encryption is a fundamental security practice. It protects data from unauthorized access, ensuring that if data is compromised, the content remains unreadable without the appropriate decryption key.

Since 2023, the S3 service added [server-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html) to the default configuration of buckets and removed the option of disabling this.

Server-side encryption refers to the process where the application or service receiving the data handles its encryption. In this context, the S3 service encrypts objects before saving them on disks in AWS data centers, then decrypts the objects when we retrieve them.

Bucket names must be unique within the region. We use the $RANDOM shell variable to ensure the name is unique, but it can be replaced by any other random alphanumeric value.

Let's create a bucket with the name "offseclab-datadev-$RANDOM-$RANDOM" and check its default configuration by running **aws s3api get-bucket-encryption**, specifying the newly-created **--bucket** name.

```
kali@kali:~$ aws s3 mb s3://offseclab-datadev-$RANDOM-$RANDOM
make_bucket: offseclab-datadev-5872-17049

kali@kali:~$ aws s3api get-bucket-encryption --bucket offseclab-datadev-5872-17049
{
    "ServerSideEncryptionConfiguration": {
        "Rules": [
            {
                "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                },
                "BucketKeyEnabled": false
            }
        ]
    }
}
```

> Listing 28 - Creating a new bucket and getting the default encryption configuration

We received the _ServerSideEncryptionConfiguration_ JSON object. Let's not worry too much about the structure and instead focus on the _ApplyServerSideEncryptionByDefault_ element, which indicates that by default, the bucket uses the _AES256_ encryption algorithm.

This default encryption mode in AWS is known as [Server-side encryption with Amazon S3 managed keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html) (SSE-S3). In this mode, the encryption keys are managed entirely by AWS.

In SSE-S3 mode, Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. Amazon S3 server-side encryption uses 256-bit Advanced Encryption Standard Galois/Counter Mode ([AES-GCM](https://dx.doi.org/10.6028/NIST.SP.800-38D)) to encrypt all uploaded objects.

The key concept about encryption in S3 buckets is that currently, it's a default configuration and objects are always encrypted on the server side.

While we can't disable server-side encryption on the bucket, we do have the option to change the encryption mode. We learned that in the default SSE-S3 mode, the S3 service manages the encryption keys. However, this configuration might not be enough for certain organizational policies or compliance requirements.

We can leverage the _AWS Key Management Service_ (AWS KMS), which allows us to create, manage, and control cryptographic keys across applications and AWS services including S3.

The Server-Side Encryption with AWS KMS keys (SSE-KMS) mode in S3 lets us select keys managed by the KMS service to encrypt the data of our objects.

Similarly, _Dual-layer Server-Side Encryption with AWS KMS keys_ (DSSE-KMS) also integrates with this service to apply two layers of encryption to objects when they are uploaded to Amazon S3.

Alternatively, _Server-Side Encryption with Customer-provided keys_ (SSE-C) is another option if we are required to manage our cryptographic keys outside of the AWS realm. However, this mode can limit the integration of S3 with other AWS services.

We can replace the default encryption mode of the bucket with any of these options. We can also specify the server-side encryption algorithm when uploading objects to override the default configuration.

In general, the default configuration provides enough security, although other configurations may be more suitable for specific compliance requirements.

Finally, we should also understand that in addition to Server-Side Encryption, we can add an extra layer of protection by using client-side encryption. This means that we can apply any encryption algorithm before uploading content to the S3 service (which will encrypt it again on the server-side).

In the next section, we'll continue working with the **datadev** bucket, while learning about another feature for data protection in our buckets.

#### Labs

1. What is the server-side encryption algorithm applied by default in the "*prefix-*offseclab-lab-encrypted" bucket?

```
Write the value of the algorithm without the double quotes.
```

Answer

2. What is the server-side encryption algorithm of the object in the bucket?

Answer

## 7.3.8. Versioning

_Versioning_ in AWS S3 is a key feature that allows for the preservation and recovery of previous versions of an object stored in an S3 bucket.

When versioning is enabled, each time an object is overwritten or deleted, a new version is created, allowing users to access both the current and previous versions of the object. This feature is particularly important for data protection and recovery, as it provides a safeguard against accidental deletions or overwrites.

Let's check how versioning works. By default, this feature is disabled, so we need to enable it by running the **s3api put-bucket-versioning** command. We need to submit two arguments: the **--bucket** name and the **--versioning-configuration** argument, set to **Status=Enabled**.

We'll then run **aws s3api get-bucket-versioning**, specifying the **--bucket** name to validate that versioning is enabled.

```
kali@kali:~$ aws s3api put-bucket-versioning --bucket offseclab-datadev-5872-17049 --versioning-configuration Status=Enabled

kali@kali:~$ aws s3api get-bucket-versioning --bucket offseclab-datadev-5872-17049
{
    "Status": "Enabled"
}

```

> Listing 29 - Enabling bucket versioning in the offseclab-versioned bucket and getting the versioning configuration

The **put-bucket-versioning** command didn't return an error response, indicating that the setting was applied successfully. We confirmed this by getting the versioning configuration and checking that the _Status_ value was _Enabled_.

Next, we'll create a text file with the name **versioned-file.txt**. We can write any content to the file. We'll upload the file to the bucket and next, we'll list the bucket to check that the object is there.

```
kali@kali:~$ echo "version 1" > versioned-file.txt

kali@kali:~$  aws s3 cp versioned-file.txt s3://offseclab-datadev-5872-17049
upload: ./versioned-file.txt to s3://offseclab-datadev-5872-17049/versioned-file.txt

kali@kali:~$ aws s3 ls s3://offseclab-datadev-5872-17049
2024-02-10 12:27:23          10 versioned-file.txt
```

> Listing 30 - Creating a text file containing the string "version 1" and putting it in the bucket

We validated that the bucket now contains one object.

Now, let's modify the file locally and upload it again with the same key name. We'll also list the bucket to check the objects within.

```
kali@kali:~$ echo "version 2" > versioned-file.txt

kali@kali:~$  aws s3 cp versioned-file.txt s3://offseclab-datadev-5872-17049
upload: ./versioned-file.txt to s3://offseclab-datadev-5872-17049/versioned-file.txt

kali@kali:~$ aws s3 ls s3://offseclab-datadev-5872-17049
2024-02-10 12:27:23          10 versioned-file.txt
```

> Listing 31 - Modifying the file and uploading it again to the bucket

After listing the bucket, we'll notice that there's only one object because it was overwritten. However, because versioning is enabled in the bucket, we should have two versions of the object: one version containing the current "version 2" string, and another version containing the previous "version 1" string.

To list all object versions in a bucket, we can run **aws s3api list-object-versions**, submitting the **--bucket** name.

```
kali@kali:~$ aws s3api list-object-versions --bucket offseclab-datadev-5872-17049
{
    "Versions": [
        {
            "ETag": "\"126a8a51b9d1bbd07fddc65819a542c3\"",
            "Size": 6,
            "StorageClass": "STANDARD",
            "Key": "versioned-file.txt",
            "VersionId": "NbEOxypTKOZF2hE4BtPV8kYbXV1t5uHP",
            "IsLatest": true,
            "LastModified": "2024-02-10T17:27:23+00:00",
            "Owner": {
                "DisplayName": "cloudadmin",
                "ID": "0560a617a6fa9f34ae6f331c9a614a0b98d6500657ec846eb2d96905b32cf42d"
            }
        },
        {
            "ETag": "\"3e7705498e8be60520841409ebc69bc1\"",
            "Size": 6,
            "StorageClass": "STANDARD",
            "Key": "versioned-file.txt",
            "VersionId": "jFuUlRv2xDHh8pKXgKRQGsrANLVVmKd8",
            "IsLatest": false,
            "LastModified": "2024-02-10T17:26:09+00:00",
            "Owner": {
                "DisplayName": "cloudadmin",
                "ID": "0560a617a6fa9f34ae6f331c9a614a0b98d6500657ec846eb2d96905b32cf42d"
            }
        }
    ],
    "RequestCharged": null
}
```

> Listing 32 - Listing all objects versions in the offseclab-versioned bucket

We checked that we have two versions belonging to the same object with _Key_ name "versioned-file.txt". The _LastModified_ element contains the date when the version was created. Let's take the first version of the object and copy its _VersionId_.

To retrieve a specific version, we'll run the **s3api get-object** command. We need to submit four arguments: the **--bucket** name, the **--key** name of the object, the **--version-id**, and the filename for the downloaded object (we'll name it **version1.txt**).

We'll then use **cat** to dump the content of the version1.txt file.

```
kali@kali:~$ aws s3api get-object --bucket offseclab-datadev-5872-17049 --key versioned-file.txt --version-id jFuUlRv2xDHh8pKXgKRQGsrANLVVmKd8 version1.txt
{
    "AcceptRanges": "bytes",
    "LastModified": "2024-02-10T17:26:09+00:00",
    "ContentLength": 6,
    "ETag": "\"3e7705498e8be60520841409ebc69bc1\"",
    "VersionId": "jFuUlRv2xDHh8pKXgKRQGsrANLVVmKd8",
    "ContentType": "text/plain",
    "ServerSideEncryption": "AES256",
    "Metadata": {}
}

kali@kali:~$ cat version1.txt
version 1
```

> Listing 33 - Getting an specific object version.

The **get-object** command returned some metadata about the object including its _versionId_, indicating that the object's version was downloaded successfully. We also validated from the content of the file that this was the first version of the object we created.

Versioning is a feature to consider when it's critical to protect the integrity of our data.

#### Labs

1. Get the proof located in the "*prefix-*offseclab-lab-versioned" bucket"

Answer

## 7.3.9. Integration with Other Services

To proceed with the follow-along, please start the lab found within **Resources** at the end of this section. Continue after initiating the lab.

The last feature we'll analyze highlights the power of cloud computing, decoupling functionalities in different services and, then using the API endpoints to interact between them.

Let's assume we are building an application that allows users to upload a file for processing. To make this example simple, the process in our lab will be to capitalize all the content of the text files we upload.

Our application will store the files to process in an S3 bucket where they will be picked, read, processed, and finally placed in another bucket.

Our application will use two S3 buckets: one bucket (_to-process_) to store the files that need to be processed; and another bucket (_processed_) for the processed files.

We'll use the _AWS Lambda_ service to process the files. Lambda is a serverless computing service that runs any code (Lambda functions) we define in response to specific cloud events.

To summarize:

1. Our application enables users to upload text files. These files will be added as objects in our _to-process_ bucket.
2. The bucket event of adding an object will trigger a Lambda function.
3. The Lambda function is programmed to retrieve the object, capitalize its content, and create a new object with the modified content.
4. Finally, the Lambda function will put the processed object in the _processed_ bucket.

The following diagram illustrates the steps above:

![[OffSec/Cloud/Cloud Essentials/z. images/b2952e0d3584db7a5c605c9a7404c565_MD5.jpg]]

Figure 2: Diagram summarizing our application steps

In this lab, we'll configure the steps related to the S3 service: creating the bucket and configuring the S3 trigger for the Lambda function.

Since the Lambda service is beyond the scope of this Learning Module, the Lambda function and its related configuration will be deployed when we start the lab. The following diagram outlines all the tasks, highlighting the ones we'll be working on.

![[OffSec/Cloud/Cloud Essentials/z. images/a80dd7bc93428a06f5e71d619483e3f3_MD5.jpg]]

Figure 3: Steps to configure the Lambda S3-trigger lab

Let's start creating the buckets. One of the buckets is already deployed in this lab. We can list all the buckets to confirm.

We'll use a random value prefix in the bucket name to avoid name collisions with other labs.

```
kali@kali:~$ aws s3 ls
2024-01-30 19:19:00 jmzxkvuo-offseclab-processed
```

> Listing 34 - Listing all the buckets in the initial setup of the lab

We can validate that the **_prefix_-offseclab-processed** bucket is already created. Now, we need to create the **offseclab-to-process** bucket. For convenience, let's use the same prefix for the new bucket.

```
kali@kali:~$ aws s3 mb s3://jmzxkvuo-offseclab-to-process
make_bucket: jmzxkvuo-offseclab-to-process
```

> Listing 35 - Creating the offseclab-to-process bucket

The "make_bucket" message in the response indicates that the bucket was created successfully. By default, this bucket already conforms to security best practices that we want to maintain: it's not publicly available and it's server-side encrypted in the SSE-S3 mode.

To set up the trigger that will execute the Lambda function every time we add an object to the **to_process** bucket, we'll need to configure the Lambda function. This time, we'll use the management console because it provides a more user-friendly interface to check the configuration.

Log in to the AWS Management Console using the _Console URL_, _Username_, and _Password_ provided in the credentials box of the lab.

First, we need to ensure the region us-east-1 (N. Virginia) is selected on the right side of the top menu (next to the IAM user name). Then, we can use the service search box, located at the top left corner, to visit the Lambda console page. Next, using the side menu, we will navigate to AWS Lambda -> Functions. From the list, we'll search and select the _offseclab-s3-trigger_ function to enter the configuration page.

![[OffSec/Cloud/Cloud Essentials/z. images/1fb165e9830fa85b8726517d3cc60c9d_MD5.gif]]

Figure 4: Entering the Management Console and navigating to the Lambda page

In the top part, we'll find the _Function overview_ section with some details about the function.

The other important element is the function's source code, which we can find below in the _Code_ tab. This code will be executed when the function is invoked.

![[OffSec/Cloud/Cloud Essentials/z. images/4334179ef963c7bf30b5d850fb0514b2_MD5.jpg]]

Figure 5: Checking the Lambda Function Overview and Source Code

The following Listing shows the code, highlighting some important parts:

```
1 import boto3
2 import mimetypes
3 
4 s3 = boto3.client('s3')
5 
6 def lambda_handler(event, context):
7     # Get Source bucket name and object key from the S3 event
8     source_bucket_name = event['Records'][0]['s3']['bucket']['name']
9     object_key = event['Records'][0]['s3']['object']['key']
10     
11     # Target bucket to put processed files
12     target_bucket_name = "jmzxkvuo-offseclab-processed"  
13 
14     # Get the object and process it
15     try:
16         # Get the object from the source S3 bucket
17         response = s3.get_object(Bucket=source_bucket_name, Key=object_key)
18         content_type = response['ContentType']
19 
20         # Check MIME type
21         mime_type, _ = mimetypes.guess_type(object_key)
22         if mime_type and mime_type.startswith('text'):
23             
24             # Processing
25             # Read text from the object and convert to uppercase
26             text_content = response['Body'].read().decode('utf-8')
27             text_uppercase = text_content.upper()
28 
29             # Put the processed object to the target S3 bucket
30             s3.put_object(Bucket=target_bucket_name, Key=object_key, Body=text_uppercase.encode('utf-8'), ContentType=content_type)
31             print(f"Converted text in object {object_key} from bucket {source_bucket_name} and saved to bucket {target_bucket_name} in uppercase.")
32         else:
33             print(f"The object {object_key} in bucket {source_bucket_name} is not a text file.")
34 
35     except Exception as e:
36         print(e)
37         print(f"Error processing object {object_key} from bucket {source_bucket_name}. Make sure the source and target buckets exist and your Lambda has the necessary permissions.")
38         raise e
```

> Listing 36 - Analyzing the Lambda function that will process the object

This code has significant space for improvement, but we're keeping it simple on purpose. This way, we can focus on understanding how Lambda and S3 work together.

This program uses the AWS _boto3_ library, which facilitates interaction with the AWS API programmatically using _Python_. Line 1 imports the library, and line 4 loads the S3 functionalities.

Line 6 defines the actual function that will execute when calling this lambda. When AWS invokes this function, it will send an event object.

Lines 8 and 9 use the event object to get the name of the S3 bucket and the object key that triggered this function.

Line 12 specifies the name of the target bucket.

Line 17 collects the content of the uploaded object and saves it in the variable "response".

Lines 21 and 22 use the _mimetypes_ library (imported in line 2) to determine the object type. We want to process only text files.

Lines 26 to 30 read the content of the object, process it, and put the newly-processed file in the target bucket.

The rest of the code handles messages that will be printed to the logs on success (or due to any error) while executing the function.

Now that we have a better understanding of what this function will do, let's configure the trigger.

To add a trigger, we can press the _Add trigger_ button located in the _Function overview_ section. We can also find this button in the _Configuration_ tab below, in the same place where the _Code_ tab is located. The Configuration tab will display a side menu where we can locate the _Triggers_ and another _Add trigger_ button. We can press any of these two buttons.

On the _Add trigger_ page, a single select box will be displayed in the trigger configuration section, prompting us to select the event source. Let's take a moment to scroll through the list and review the different options we have to trigger a Lambda function from other AWS services.

We need this Lambda function to execute every time an object is placed in the **offseclab-to-process** S3 bucket. So, we need to select S3 as the source. New options will be displayed.

![[OffSec/Cloud/Cloud Essentials/z. images/ca692a3805f60fd2582c23f809f9233d_MD5.gif]]

Figure 6: Adding an S3 trigger to the Lambda function

First, we need to select the bucket that will trigger the event. We'll choose the **offseclab-to-process** bucket.

Next, we need to choose the specific event within the bucket that will trigger the function. We'll leave the default choice: _All object create events_.

The next two options let us specify a prefix or suffix to limit the event for objects with keys that start or end with matching characters. We'll leave these options blank.

The last option, _Recursive invocation_, is a warning that reminds us that if the Lambda function triggered by this event creates new objects in this same source bucket, it will cause the function to execute again due to the new object-created event. This can cause a potential recursive invocation loop that runs indefinitely, increasing costs. We'll click the checkbox to acknowledge the warning.

Finally, we'll press the _Add_ button to create the trigger. We'll be redirected to the function page where we can validate that the S3 service now displays as a trigger in the _Function overview_ diagram.

![[OffSec/Cloud/Cloud Essentials/z. images/88f1e1f114a0a99e4f4500961449ae98_MD5.jpg]]

Figure 7: Configuring the function trigger

Because the Lambda is already configured, our lab is now ready for testing. If we add a text file to the _offseclab-to-process_ bucket, the Lambda function will trigger and process the file, turning all characters to uppercase and putting the new object in the **offseclab-processed** bucket.

Let's create a text file with any content. For example, we'll **echo** the word "test" and redirect the output to the **test.txt** file. This will create the file with the string in our directory. We can use **cat** to dump the content of the file to validate that it was created and has content.

Next, we'll upload the file to the **offseclab-to-process** bucket and check that it was uploaded.

```
kali@kali:~$ echo test > test.txt

kali@kali:~$ cat test.txt
test

kali@kali:~$ aws s3 cp test.txt s3://jmzxkvuo-offseclab-to-process
upload: ./test.txt to s3://jmzxkvuo-offseclab-to-process/test.txt

kali@kali:~$ aws s3 ls jmzxkvuo-offseclab-to-process
2024-02-01 10:46:18          5 test.txt
```

> Listing 37 - Creating a test.txt file containing the word "test" and uploading it to the "-to-process" bucket

We validated that the _test.txt_ file containing the word "test" was created and uploaded to the bucket.

Having configured the lab, we'd expect that placing the object in the source bucket would trigger the Lambda function, which retrieves the object, processes it into capital letters, then places it in the _offseclab-processed_ bucket.

Let's check this by listing the bucket.

```
kali@kali:~$ aws s3 ls jmzxkvuo-offseclab-processed
2024-02-01 10:46:20          5 test.txt

```

> Listing 38 - Listing the offseclab-processed bucket

We can confirm that the "processed" bucket contains the test.txt file, even though we didn't put the object there. This indicates that the lambda function worked successfully. Let's check the content of the new object by downloading it.

```
kali@kali:~$ aws s3 cp s3://jmzxkvuo-offseclab-processed/test.txt processed-test.txt
download: s3://jmzxkvuo-offseclab-processed/test.txt to ./processed-test.txt

kali@kali:~$ cat processed-test.txt
TEST
```

> Listing 39 - Downloading the test.txt object and validating it was processed by the Lambda function

After downloading the file, we'll notice it's the capitalized version of the original file, indicating that the S3 trigger we configured worked as expected.

Let's return to the Lambda function page in the management console. We'll check the source code again, located in the _Code_ tab below the _Function overview_ section.

The parts of the code that interact with the S3 service are lines 17 and 30, and we can guess that it's using the _s3:GetObject_ and _s3:PutObject_ actions.

```
16         # Get the object from the source S3 bucket
17         response = s3.get_object(Bucket=source_bucket_name, Key=object_key)
...
29             # Put the processed object to the target S3 bucket
30             s3.put_object(Bucket=target_bucket_name, Key=object_key, Body=text_uppercase.encode('utf-8'), ContentType=content_type)
```

> Listing 40 - Reviewing the lines of the code that interact with the S3 service

Let's briefly consider how the Lambda function obtains authorization to run these actions in the buckets.

When we learned about authorization in S3 buckets, we learned that the bucket belongs to the account, meaning that all IAM identities will have access to all the buckets in the account unless this access is explicitly denied via policy. However, the identities _do_ need to have policies granting permission to interact with the buckets. This is because when we create any identity, by default it doesn't have any permissions, and we must grant the necessary permissions by associating a policy.

When we create a Lambda function, we need to define an _execution role_. This is an IAM role that the function will impersonate to inherit the role's authorization policies.

We can find this execution role in the _Configuration_ tab. This is the same tab where the triggers are located, but this time we'll search for the _Permissions_ option in the side menu.

Here, we'll find that the execution role name is _offseclab-lambda-s3-trigger_. This role was already created for the lab. If we are familiar with IAM, we can press the role name and we'll be redirected to the IAM role page to show more details about the role, including its authorization policies.

Instead, let's use the "Resource summary" section below the role name, which summarizes the permissions.

We'll click on the option box to choose a service. A list with two services will appear. We learn from this that the Lambda function is authorized to run three actions from the _Cloudwatch Logs_ service and two actions from the _S3_ service.

Finally, if we choose the S3 service, we find that the role (and hence the Lambda function) is authorized to execute the _s3:GetObject_ action on any bucket and the _s3:PutObject_ action only on the **offseclab-processed** bucket.

![[OffSec/Cloud/Cloud Essentials/z. images/ce0898d693dd55f53a2b76b275ffd643_MD5.jpg]]

Figure 8: Checking the function's execution role

To summarize, Lambda functions need to assume an IAM role to obtain authorization.

This lab demonstrates how object storage can seamlessly interact with other services. It's our responsibility to ensure that each bucket has the correct authorization, allowing services to perform only the required actions. Depending on the scenario, we'll need to consider additional layers of protection to safeguard the data's integrity based on its criticality.

## 7.4. Wrapping Up

In this Learning Module, we explored the fundamentals of Cloud Object Storage, offering a comprehensive overview and delving into its significance in modern cloud environments. We began by comparing file, block, and object storage, highlighting the unique advantages object storage offers, such as scalability, durability, and cost-effectiveness, which makes it ideal for handling vast amounts of unstructured data. This set the stage for understanding why object storage has become an important component of cloud storage solutions.

Having developed a baseline knowledge about object storage in the cloud, we next explored the configuration and management of object storage within AWS. We highlighted the importance of authorization policies to control access and learned how to make objects publicly available when necessary. We also covered critical aspects of data security and protection, such as encryption to protect data at rest and in transit, and examined versioning as an essential tool for managing the data lifecycle and enhancing data integrity.

Throughout this Module, our focus was not only on acquiring technical knowledge, but also on understanding practical applications and security best practices. We gained valuable insights as a result, equipping ourselves with the skills for utilizing object storage effectively in cloud computing.