In this topic we will cover the following Learning Units:

- Introduce Kubernetes components
- Learn about more Kubernetes features

In _Introduction to Kubernetes I_ we described how Kubernetes can manipulate containers across a cluster (a group of servers) and provided an overview of cluster networking. In this Learning Module, we will explore the internal components of Kubernetes and many additional features.

## 5.1. Kubernetes Components

Kubernetes is complex, but we don't need to fully understand each component to use it.

As we mentioned in Introduction to Kubernetes I, we can leverage _Kubernetes_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1598-1) components to quickly and easily build cloud systems. These service components are each individually customizable, extensible, and replaceable.

In this Learning Unit, we'll explore Kubernetes internals, discuss how components work together, and generally learn more about how Kubernetes works under the hood.

This Learning Unit covers the following Learning Objectives:

1. Learn about the core Kubernetes components
2. Understand the modularity of Kubernetes
3. Discover Kubernetes operators and CRDs

1

(The Linux Foundation, 2022), [https://kubernetes.io/](https://kubernetes.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1598-1)

## 5.1.1. Introducing the Components of Kubernetes

In _Introduction to Kubernetes I_, we created deployments and exposed them with services. Now we'll explore the core services of Kubernetes and introduce additional features and capabilities. While this Learning Module doesn't cover every Kubernetes feature, it covers many core features. We will build clusters with _kubeadm_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-1) and start using some Kubernetes security controls.

Before we dive into additional Kubernetes features, let's provide some additional background.

As mentioned in the previous topic, Kubernetes is a flexible platform for managing containers. Kubernetes is made from microservices that are customizable and replaceable. The _Kubernetes API_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-2) itself resides in the cluster on the control plane, created when we bootstrap the cluster with kubeadm. The Kubernetes API, along with some supporting core and modular components, run primarily in the _kube-system_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-3) namespace. Most of the kube-system components run in the _control plane_;[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-4) however, some also extend out to the worker nodes. The Kubernetes API runs in the kube-system namespace on the control plane, which listens on port 6443 (or 16443 for some implementations) and is bound to the nodes in the control plane. By default, any node in the control plane can receive a Kubernetes API request on port 6443.

If the Kubernetes API services are not running, _kubectl_[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-5) won't send Kubernetes API requests and the control plane will be unavailable. The Kubernetes API stores the state information (from kubectl) in a storage service, the default being _etcd_.[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-6)

In addition to the Kubernetes API service and the etcd storage service, further components of Kubernetes include the _kube-controller-manager_,[7](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-7) _kube-scheduler_,[8](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-8) _kubelet_,[9](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-9) and _kube-proxy_.[10](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-10)

![[OffSec/Cloud/Cloud Essentials/z. images/b6ef74326fea0812ca2c9204be45177a_MD5.jpg]]

Figure 1: Components of Kubernetes

Lets review these components in detail.

The kube-scheduler, also known as _the scheduler_, schedules the deployment of Pods to nodes in the cluster. If the scheduler is waiting for resources from the cluster, the Pod status will be marked _Pending_.

Kubelet (Figure 1) runs as a separate daemon on each node of the cluster. It tells the _Container Runtime Interface_ (CRI)[11](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-11) (which we'll explain in more detail shortly) how to enforce the Pod specifications. These Pod specs are provided to kubelet via other core components, commonly the API server. Kubelet also probes cluster containers with executions, performs health checks, and sends various other network requests.

The CRI uses a type of _Remote Procedure Call_ (RPC)[12](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-12) (a _Google Remote Procedure Call_ or gRPC[13](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-13)) to allow kubelet to create, update, and delete Pod containers.

Kubernetes uses RPCs for inter-component communication. RPC hides some of the complexity between service-to-service interactions and can be useful for distributed communications. RPC isn't a single specification and can be implemented in different ways, such as RPC via HTTP or RPC via TCP. RPC can leverage bi-directional communication, including customized authentication and protocol adoption.

The Kubernetes gRPC implementation leverages TCP and defines the RPC approach used. With gRPC, we can create RPC compatibility between components. An analogy for gRPC would be a delivery service. The delivery service can deliver many types of packages to different destinations leveraging trucks and support systems that can handle the various types of packages required. In this analogy, the Kubernetes microservices and components represent customers that can request specific shipping services.

The CRI abstracts the interaction between kubelet and the container runtimes so that kubelet can interface with any supported container runtime. Kubernetes uses _CRI-O_[14](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-14) and _containerd_[15](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-15) (used by Docker) as the primary Kubernetes container runtime options.

Kube-proxy (Figure 1) is the default network service that routes requests to the correct Pods and upholds some networking rules. Kube-proxy can be extended or replaced with a modular plugin for _Container Network Interface_ (CNI),[16](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-16) but is often run along with a CNI plugin. Kube-proxy is the default routing to a Pod IP, while the CNI is responsible for assigning IPs to Pods. The CNI plugin is a modular component.

The kube-controller-manager (Figure 1) is the core control loop, an endless loop that enforces the state of a Kubernetes cluster. The kube-controller-manager uses the Kubernetes API to check on the cluster and take actions to attempt to make the cluster fit the declared state. The kube-controller-manager contains the core controllers. These core controllers include the node controller, which monitors the run state of the node, as well as a collection of other core controllers. The optional _cloud-controller-manager_[17](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1621-17) sends instructions to the cloud service provider resources. Additional controllers may be added into the cluster as modular components.

1

(The Linux Foundation, 2022), [https://kubernetes.io/docs/reference/setup-tools/kubeadm/](https://kubernetes.io/docs/reference/setup-tools/kubeadm/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-1)

2

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-2)

3

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-3)

4

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/overview/components/](https://kubernetes.io/docs/concepts/overview/components/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-4)

5

(The Linux Foundation, 2021), [https://Kubernetes.io/docs/reference/kubectl/kubectl/](https://Kubernetes.io/docs/reference/kubectl/kubectl/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-5)

6

(etcd, 2022), [https://etcd.io/](https://etcd.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-6)

7

(The Linux Foundation, 2022), [https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-7)

8

(Kubernetes, 2022), [https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-8)

9

(The Linux Foundation, 2022), [https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-9)

10

(The Linux Foundation, 2022), [https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-10)

11

(The Linux Foudnation, 2022), [https://kubernetes.io/docs/concepts/architecture/cri/](https://kubernetes.io/docs/concepts/architecture/cri/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-11)

12

(man7.org, 2022), [https://www.man7.org/linux/man-pages/man3/rpc.3.html](https://www.man7.org/linux/man-pages/man3/rpc.3.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-12)

13

(gRPC, 2022), [https://grpc.io/](https://grpc.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-13)

14

(CNCF, 2022), [https://cri-o.io/](https://cri-o.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-14)

15

(The Linux Foundation, 2022), [https://containerd.io/](https://containerd.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-15)

16

(The Linux Foundation, 2022), [https://www.cni.dev/](https://www.cni.dev/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-16)

17

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/architecture/cloud-controller/](https://kubernetes.io/docs/concepts/architecture/cloud-controller/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1621-17)

## 5.1.2. Modular Components

The individual parts of Kubernetes (components, modules, microservices, etc) are modular. They are individually customizable, and many are fairly easy to replace. We can categorize these into core components that are less likely to be swapped out and are made by the upstream Kubernetes project, and _modular_ components that are more commonly swapped out or not always used, often made from other projects.

Modular components are commonly customized or replaced in various Kubernetes implementations or distributions, while the core components are typically static but might also be configured or replaced by a Kubernetes implementation or distribution. In addition, modular components are generally optional and are not often enabled by default. They often require specific implementations or install steps to configure.

Let's highlight some of the more common components.

First, the modular component _coreDNS_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1600-1) is often installed by default. An alternative includes _kube-dns_,[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1600-2) which is maintained by the upstream Kubernetes project. Both coreDNS and kube-dns provide in-cluster DNS records for Pods, providing DNS names for Pods inside the cluster.

The default Kubernetes API storage component is etcd, which is also a modular component because it is maintained separately from the Kubernetes core components. Etcd contains the cluster state as well as the cluster secrets, which means the security of etcd is critical. Etcd is run inside the control-plane nodes with the other kube-system components by default, but can also be run outside of the cluster in a separate etcd server pool.

Etcd is effective at distributing storage, but by default it is unencrypted at rest. This means the secrets are written to plaintext on disk, which is an obvious security concern. If compliance is essential for a cluster build, the etcd storage should be encrypted. While storage is not encrypted by default, network communication within etcd is encrypted with _Mutual Authentication TLS_ (mTLS)[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1600-3) by default.

Note that a cluster admin with kubectl access can also read all cluster secrets even if storage encryption is enabled.

The CNI is both a specification and a set of libraries for network plugins. CNI plugins (plugins that use the CNI spec and libraries) are network modules we can select and insert into a cluster to provide network functionality.

Different CNI plugins offer different features for in-cluster networking. We need at least one CNI plugin to enable full network functionality in a cluster. We could configure multiple CNIs in a cluster, but it is more common to use just one. The networking provided by a CNI configuration is typically an abstraction of the networking features within the Linux kernel, used to assign and manage IP addresses for Pods. The CNI plugins help us leverage Linux kernel features by abstracting kernel networking into more YAML API calls via kubectl. The CNI plugins, along with kube-proxy, enable the networking within the cluster.

There are more modular components available in Kubernetes that we did not introduce in this Learning Module. We'll explore more modular components in other Learning Modules.

1

(The Linux Foundation, 2022), [https://coredns.io/](https://coredns.io/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1600-1)

2

(The Linux Foundation, 2022), [https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1600-2)

3

(IETF, 2018), [https://www.rfc-editor.org/rfc/rfc8446#section-4.6.2](https://www.rfc-editor.org/rfc/rfc8446#section-4.6.2) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1600-3)

## 5.1.3. Operators, Custom Resource Definitions, and Helm

Let's further explore some key Kubernetes terms and features before we move on.

A _Kubernetes object_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1601-1) is any declared structure in the cluster that we can manipulate with Kubernetes API requests. This is not a programming language object exactly, but rather used to reference Kubernetes configuration states. It may take some time to fully understand Kubernetes objects, but we can begin to think about them as the configurations we declared with our YAML after they are inserted into the Kubernetes API. When we write about Kubernetes objects, they are commonly capitalized (Pod, Deployment, Service, etc), to help differentiate the Kubernetes object from the more generic term.

Once a declaration is inside the cluster API storage, they are _objects_ in our cluster. While some Kubernetes objects are inserted during cluster creation and during bootstrapping, others are aspects like application Deployments, Services, and so on. When we use kubectl to _get_, we are returning Kubernetes objects, and when we use kubectl to _describe_, we are obviously describing Kubernetes objects. This description of Kubernetes objects is an over simplification, but helpful to get us started. For example, a namespace isn't a Kubernetes object, but rather a way to group Kubernetes objects.

_Custom Resource Definitions_ (CRD)[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1601-2) are API objects that can be added into Kubernetes. For example, if we wanted to create an API object named _SuperOrange_, we could create it with a CRD. With that example CRD in place, our YAML kubectl requests could include "apiVersion: SuperOrange/v1" that manipulates the SuperOrange objects like any other Kubernetes object.

_Operators_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1601-3) are a design pattern in Kubernetes for adding controllers to CRDs. This design pattern allows us to add state management, replacing something a person might have to do manually with a controller that takes care of those tasks automatically. Operators help manage components within a Kubernetes cluster. We can use Kubernetes operators to create new state machines with Kubernetes objects. The example CRD SuperOrange Kubernetes object might have a SuperOrange operator. That SuperOrange operator then might include controller code for SuperOrange Kubernetes objects to manage their desired state.

We can bring custom code into Kubernetes with both operators and custom resource definitions. We can then call them with the same YAML format, but with the _apiVersion_ pointed to the operator or CRD. Operators and CRDs can enable more components to be declared with the Kubernetes API, expanding what we can do with the YAML we use with kubectl.

YAML manifests are essentially a condensed view of a Kubernetes system's component configurations. Operators and CRDs allow us to put even more into the Kubernetes API YAML manifests. An organization might leverage many YAML manifests maintained by others outside of our organization as well as YAML manifests maintained within the organization. Kubernetes is complex as it is, but adding more customization increases the complexity. When multiple people or organizations are involved in customizing the same Kubernetes cluster, it can become difficult to keep track of all the components. Luckily, there are tools that can help keep the cluster organized.

We can use _Helm_[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1601-4) to manage Kubernetes states. Helm is much like a package manager for Kubernetes YAML manifests that we can use to roll forward and roll back between versions of Kubernetes states. We don't have to use Helm to manage and package Kubernetes states, but it simplifies the task. With all of the many modular components and microservices, Helm can wrap up manifest YAML with some metadata into what is called a _Helm chart_. For example, we can apply a "customstuff_db/0.2.1" chart, and get all of the cluster database related YAML manifests deployed together in a group. The Helm chart is a versioned collection of related YAML manifests and metadata packed into special _tarball_[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1601-5) file.

Helm charts are particularly helpful when we need to install complicated CRDs and multiple operators across separate teams. We could also use git, tarballs, scripts, or other mechanisms to organize and manage Kubernetes clusters, although most organizations choose to use Helm.

1

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1601-1)

2

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1601-2)

3

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/extend-kubernetes/operator/](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1601-3)

4

(The Linux Foundation, 2022), [https://helm.sh/](https://helm.sh/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1601-4)

5

(Linux man-pages project, 2022), [https://man7.org/linux/man-pages/man1/tar.1.html](https://man7.org/linux/man-pages/man1/tar.1.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1601-5)

## 5.2. Kubernetes Features

In this Learning Unit we'll build on what we have already learned about Deployments and Services, working our way closer to real world Kubernetes proficiency. We'll expand on more types of Kubernetes objects and features that empower us with important security awareness and know-how.

The knowledge we are going to start building in this Learning Unit is required for configuring Kubernetes clusters with security best practices, as well as furthering our ability to do penetration testing on Kubernetes clusters. We will not be covering all Kubernetes features and security best practices in this Learning Unit, but we'll make significant progress in furthering our Kubernetes knowledge.

This Learning Unit covers the following Learning Objectives:

1. Understand Kubernetes Deployments
2. Understand Kubernetes Services
3. Learn about Kubernetes autoscaling
4. Learn about Kubernetes ReplicaSets, DaemonSets
5. Learn about Kubernetes Volumes
6. Learn about Kubernetes ConfigMaps
7. Learn about Kubernetes Jobs and CronJobs
8. Learn about Kubernetes role-based access controls
9. Learn about Kubernetes NetworkPolicies
10. Introduce eBPF features

## 5.2.1. Accessing the Three-node Kubernetes Lab

We will use three VMs in the lab as we work with Kubernetes in this Learning Module.

Before we jump in, let's edit the **hosts** file in our Kali instance to map names to the lab IPs.

The IP addresses may vary in the third octet, so be sure to use the IP address of the actual lab VM as provided by the OffSec Library.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.51.92  k8s
192.168.51.93  k8sWrk1
192.168.51.94  k8sWrk2
```

> Listing 1 - /etc/hosts entries

We'll start the lab machines after editing the **hosts** file. We can then use names instead of IP addresses to access the lab machines from Kali.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

Intro to Kubernetes - k8s three node cluster

## 5.2.2. Kubernetes Deployment Resource Limits

Let's continue our discussion of Kubernetes Deployments we began in the previous Learning Module. To begin, we'll initialize the three-node lab, join the cluster, and write a new Deployment YAML.

The _lab_join_ script on the control plane node joins the cluster and applies the CNI manifest. As the root user on the control plane, simply run **lab_join** to initialize the cluster, join the workers, and apply the Calico CNI manifest.

If the lab cluster is not working, revert the VMs, then run **lab_join** again to return the cluster to a ready state for use.

We will create this YAML based on a generated template. In this example, we will pipe the output to **tee**[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1604-1) to split STDOUT to both the file and STDOUT. We'll use **--dry-run** to generate some YAML to use as a starting point for a Deployment. The **--dry-run** technique is useful for generating some basic YAML when we aren't working with existing YAML.

Let's build out a Deployment from the starting point we generated. We'll add some useful features and remove some unnecessary template lines.

```
root@k8s:~# kubectl create deployment leveltwo --dry-run --image=nginx -o YAML | tee leveltwo.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: leveltwo
  name: leveltwo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: leveltwo
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: leveltwo
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
```

> Listing 2 - starting a Deployment template

Let's edit **leveltwo.yml**, adding resource limits to reserve and limit system resources for the Deployment.

We will remove the curly braces after _resources_, delete the _status_ section, and remove _creationTimestamp_ since we won't use any of these.

The _limit_ section puts a limit of two CPUs and five hundred _mebibytes_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1604-2) of memory on the container, while the _requests_ section requests (to the scheduler) that two CPUs and twenty mebibytes of memory is reserved for the Pod. While a limitation is strictly enforced, a request may not since it depends on the available capacity of the system.

```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: leveltwo
  name: leveltwo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: leveltwo
  strategy: {}
  template:
    metadata:
      labels:
        app: leveltwo
    spec:
      containers:
      - image: nginx
        name: nginx
        resources:
          limits:
            cpu: "2"
            memory: "500Mi"
          requests:
            cpu: "2"
            memory: "20Mi"

...        
```

> Listing 3 - example Deployment with CPU limit

These limits can reduce impact from an application container with a memory leak. If each Deployment in a cluster has well-declared limits, a problem container can not consume all of the resources on a cluster node.

Let's expand on our example Deployment by adding a _strategy_ that declares how changes are applied to the Pods within the Deployment. We will select a rolling updates strategy (setting a limit of three Pod updates at a time) and leave only one Pod unavailable at a time.

We'll begin by replacing the curly braces after _strategy_ with our Deployment strategy declaration. We'll use _maxSurge_ to define the maximum amount of extra Pods that are allowed and _maxUnavailable_ to the maximum number (or percent) of unavailable Pods during a Deployment update using either an integer ("1") for the former or a string ("30%") for the latter.

```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: leveltwo
  name: leveltwo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: leveltwo
  strategy: 
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3
      maxUnavailable: 1
      
  template:
    metadata:
      labels:
        app: leveltwo
    spec:
      containers:
      - image: nginx
        name: nginx
        resources:
          limits:
            cpu: "2"
            memory: "500Mi"
          requests:
            cpu: "2"
            memory: "20Mi"

```

> Listing 4 - example Deployment strategy

Before we apply this manifest, we'll add _imagePullPolicy: Never_ to prevent our lab from attempting to connect to the registry over the internet, which would fail in this lab since internet access is blocked.

```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: leveltwo
  name: leveltwo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: leveltwo
  strategy: 
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: leveltwo
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Never
        name: nginx
        resources:
          limits:
            cpu: "2"
            memory: "500Mi"
          requests:
            cpu: "2"
            memory: "20Mi"
```

> Listing 5 - example Deployment

Now we'll finally use kubectl to declare this Deployment. In the past, we used _kubectl apply_, which would _create_ objects that didn't exist as well as update existing objects. Instead, we will use **kubectl create**, which will simply make new objects instead of updating existing ones.

```
root@k8s# kubectl create -f leveltwo.yml
deployment.apps/leveltwo created

root@k8s# kubectl create -f leveltwo.yml
Error from server (AlreadyExists): error when creating "leveltwo.yml": deployments.apps "leveltwo" already exists
```

> Listing 6 - kubectl create

Now that we have created our new Deployment, we can examine our cluster to inspect what happened. We will first run **kubectl get pods** to check on the state of our Pods. We didn't specify a namespace in this Deployment, so everything is happening in the default namespace.

```
root@k8s# kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
leveltwo-7f9d7557bb-npgbx   0/1     Pending   0          3s
```

> Listing 7 - kubectl get Pods

Our Pod is in a "Pending" status. Let's run **kubectl describe** to gather more information.

```
root@k8s# kubectl describe pods
...(cut)...
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  37s (x3 over 112s)  default-scheduler  0/2 nodes are available: 2 Insufficient cpu.

```

> Listing 8 - kubectl describe Pods

According to the output, the kube-scheduler was unable to schedule our Pods into the cluster because the cluster does not have enough allocated CPU.

In this case, we can adjust our YAML to lower the CPU request to correct the issue. In other cases, we may really want to have two CPUs reserved for a single Pod and to declare it that way.

We can do much more with Deployments, but let's practice what we've discussed so far by creating Deployments with specific resource limits and requests.

How to use the cloud_grader:

Many of the cloud challenges that involve making or configuring something related to the Learning Module will require the construction of a _challenge state_. The Offensive Security cloud_grader is a way to measure a challenge state in a Learning Module to determine whether the required configuration has been accomplished. A challenge question may have a prompt describing the requirements for the challenge state required to complete that challenge.

After creating the challenge state in the lab, the _root_ user on the control plane node can run **cloud_grader** to measure the state of the lab. If the completed challenge state is measured, a message like "OS{80f6eb78c3bdf5d36eddaafcc46edcf2}" will be printed. We can copy the string, including the "OS" and curly braces, and paste it as the answer to the challenge.

In order to create a completed challenge state, the objects in the lab Kubernetes cluster must match the described aspects in the challenge prompt. If the cloud_grader encounters extra _ReplicaSets_, extra _Roles_, or if Pods are in the process of terminating or restarting, the challenge state and the string will not match (meaning our solution is "incorrect").

After completing a challenge, please be sure to run **kubectl delete -f manifest.yml** to clean up after a challenge before moving on to the next one. This will ensure that you have a clean challenge state as you enter the next challenge.

1

(Linux man-pages project, 2022), [https://www.man7.org/linux/man-pages/man1/tee.1.html](https://www.man7.org/linux/man-pages/man1/tee.1.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1604-1)

2

(IEC, 2022), [https://www.iec.ch/prefixes-binary-multiples](https://www.iec.ch/prefixes-binary-multiples) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1604-2)

#### Labs

1. Create a new Deployment named _maroon-prod-falcon_ in a namespace named _maroon_. Ensure that there are no other Deployments in the maroon namespace. Set the Deployment to use the _prod-falcon:blue_ container image with 1 replica, limit the Deployment to 1 CPU, and request 0.1 CPU. Additionally, limit the memory to 200 mebibytes. Expose the Deployment using a LoadBalancer Service with NodePort 30348. Run **/usr/sbin/cloud_grader** on the control plane afterwards to check your work and get a flag to submit below.

The _prod-falcon:blue_ image is the same container used in part one, and we still have the same reference manifests available in **/var/lib/production_blue/**. Feel free to review those manifests as you work on new ones for the challenges in part two.

Answer

## 5.2.3. Annotations and Service Labels

In this section, we'll discuss annotations and service labels that assist with service discovery.

In a traditional network environment, services are bound to a particular port and each application that wishes to address that service must include a hardcoded IP address and port for that service.

As the network grows to include more layers of protection, each component must be aware of the service map. For example, one or more external firewalls may point to load balancers, which in turn point to the applications, each potentially requiring IP and port configurations so that incoming requests are routed to the correct service. This type of design can make network configuration a tedious, and often manual process.

As we learned in Introduction to Kubernetes I, Kubernetes leverages advanced service discovery that uses labels and metadata in the cluster state instead of IP addresses. This means we don't need to hardcode values into each application and components can easily discover services in the cluster.

Within Kubernetes, metadata is fairly complex, akin to object variables or Pod traits. Operators, CRDs, Services, and CNI can use these metadata traits to interact with applications based on the concept of a _metadata key-value pair_.

Annotations are key-value pairs that can be used for a separate group of human-focused labels. We can also make up our own annotations that contain notes or human-readable data. Most annotations should be used as notes for people, however they can be used to activate features as well.

Let's explore an example of some simple annotations we could use to associate notes or metadata with a Kubernetes object:

```
annotations:
  oci-upstream-references: "https://hub.docker.com/"
  myCustomThing: "I am glad to announce that the platform can speak!"
```

> Listing 9 - example annotations

Rather than making up annotations as we did in our previous example, let's present the _standard annotations_, which are suggested, but not necessarily required. Annotations are purposefully open-ended so we can use them in a variety of situations but they provide clarity as a cluster grows in complexity.

Since it's best practice to include standard annotations, let's review an example that uses them.

In this example, we'll include the standard annotations that we use for service discovery under _labels_, and a few meant for documentation under _annotations_. There are exceptions, but in general, we will use labels to define rules in our cluster, and we will use annotations to include notes in our cluster.

```
root@k8s# cat /var/lib/misc/annotations.yml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-nginx
  namespace: green
  labels:
    app.kubernetes.io/name: honeypot
  annotations:
    app.kubernetes.io/instance: nginx-honeypot-x
    app.kubernetes.io/version: 0.0.1
    app.kubernetes.io/component: honeypot
    app.kubernetes.io/part-of: honeypot-x
    app.kubernetes.io/managed-by: devsecops-manifest
    app.kubernetes.io/created-by: devsecops-kubectl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: green-nginx
  template:
    metadata:
      labels:
        app: green-nginx
        dangerzone: active
        app.kubernetes.io/name: honeypot
    spec:
      containers:
      - name: nginx
        imagePullPolicy: Never      
        image: "nginx:latest"
        ports:
        - name: nginx
          containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: honeyx
  namespace: green
  annotations:
    app.kubernetes.io/name: honeypot-web
    app.kubernetes.io/instance: nginx-honeypot-x
    app.kubernetes.io/version: 0.0.1
    app.kubernetes.io/component: honeypot
    app.kubernetes.io/part-of: honeypot-x
    app.kubernetes.io/managed-by: devsecops-manifest
    app.kubernetes.io/created-by: devsecops-kubectl
spec:
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30333
      protocol: TCP
  type: NodePort
  selector:
    app.kubernetes.io/name: honeypot
```

> Listing 9 - example annotations

In this example, we added an _app.kubernetes.io/name: honeypot_ label to the Deployment. This is a functional label that maps the Service to the Pod for service discovery. In this case, we added annotations that describe how the objects were created and how they are to be maintained for anyone that might explore them.

## 5.2.4. Horizontal Pod Autoscaling

A _HorizontalPodAutoscaler_ (HPA)[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1607-1) is a type of Kubernetes configuration object that can automatically add additional Pods (horizontally scale) based on metrics or conditions. So far in this Learning Module, our Pod replicas were static unless we changed or deleted them with a Kubernetes API.

With HPA, Kubernetes can dynamically duplicate Pods to better handle a detected load. This is generally only needed in large-scale deployments.

As we discussed in Introduction to Kubernetes I, we have two options when it comes to creating, modifying, or deleting objects. We can use one-line _imperative commands_ or we can take a _declarative_ approach that leverages YAML files.

Let's use the declarative approach to enable HPA via a YAML file:

```
root@k8s# cat /var/lib/misc/hpa.yml
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: green-scaler
  namespace: green
  annotations:
    app.kubernetes.io/name: nginx
    app.kubernetes.io/instance: green-nginx-1
    app.kubernetes.io/version: 0.0.1
    app.kubernetes.io/component: frontend
    app.kubernetes.io/managed-by: manifest
    app.kubernetes.io/created-by: offsec
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
...
```

> Listing 10 - example autoscaler

The _spec_ block in the example YAML contains the declared HPA. We set _scaleTargetRef_ to point to the type of Kubernetes object we want to _autoscale_,[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1607-2) and declare the Deployment, giving it a name of "nginx". Then we declare _minReplicas_ and _maxReplicas_ to set the minimum and maximum number of replicas to use for autoscaling, respectively. The _targetCPUUtilizationPercentage_ declaration increases and decreases the Pod count based on the CPU usage. There are many more complex autoscaling rules and metrics we could define but they are out of scope for this introduction.

Autoscaling can be used as a defensive technique. For example, if HPA engages because of a heavy load, we may be dealing with a denial-of-service attack, which can be potentially mitigated. This has become a standard technique for cloud providers, which commonly autoscale load balancers.

1

(The Linux Foundation, 2022), [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1607-1)

2

(The Linux Foundation, 2022), [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1607-2)

## 5.2.5. DaemonSets, ReplicaSets, Taints, and Tolerations

In this section, we will discuss DaemonSets, ReplicaSets, taints, and tolerations.

_ReplicaSets_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1608-1) define a set of identical Pods that the scheduler can assign to nodes. They increase efficiency and can be scheduled based on the cluster state. We've used ReplicaSets in our examples up to this point because they are built-in to Deployments, but they can be used outside of a Deployment. Let's use this in an example.

```
root@k8s# cat /var/lib/misc/replicaset.yml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-b
  namespace: green
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:     
      containers:
      - name: nginx-b
        image: nginx
        imagePullPolicy: Never        
...
```

> Listing 11 - example replicaset

The stand-alone _ReplicaSet_ is similar to what we have used in our previous Deployments. However, ReplicaSets are more limited than Deployments since they can only create an initial set of replicas and can not push changes to Pods.

While ReplicaSets and Deployments declare the replicas of a given Pod required in the cluster, _DaemonSets_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1608-2) declare that each node in the cluster should have the same amount of deployed Pods. For example, we can declare that each node is to have one or more copies of the Pod declared in the DaemonSet.

Kube-proxy is an example of a DaemonSet in the kube-system, which includes the control plane nodes. We can also make our own DaemonSets if we need to ensure a Pod is on each worker node or across a specific set of nodes.

Logging aggregators, security containers, distributed caching systems, and various networking services are excellent use cases for DaemonSets since we want these within close proximity to our other Pods. There are no network hops between these because each node of the cluster includes its own DaemonSet Pod. By contrast, Pods in a Deployment or ReplicaSet may result in Pods landing on a different cluster node.

We could, however, use taints to alter this behavior.

_Taints_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1608-3) are Kubernetes node markers that direct Pods away from a node. _Tolerations_[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1608-4) allow us to override taints. For example, our control plane is tainted by default so that our application Pods are scheduled onto the worker nodes. We could even use a toleration to override the _node-role.kubernetes.io/master_ taint so that our DaemonSet Pods deploy onto control plane nodes as well.

The _node-role.kubernetes.io/master_ taint is commonly created automatically during cluster bootstrapping, along with the _master_ node role for the control plane nodes. Additional node roles may be added or modified to then be used for additional taints and tolerations.

Here is an example of assigning the _worker_ node role to the _myserver_ node, where myserver is the hostname of a node in a cluster. We don't necessarily need to do this in this Learning Module's lab, but we will leverage this in a later Learning Module.

```
$ kubectl label node myserver node-role.kubernetes.io/worker=worker
```

> Listing 12 - example imperative node role assignment

We won't spend much time discussing node roles in this Learning Module. For now, it is enough to know that the control plane often has node roles by default and is typically tainted by default with a _NoSchedule_ taint.

Let's examine a DaemonSet that uses a toleration for the default control plane taint to deploy Pods into the control plane.

```
root@k8s# cat /var/lib/misc/daemonset.yml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: green-scaler
  namespace: green
  annotations:
    app.kubernetes.io/name: nginx
    app.kubernetes.io/instance: green-nginx-1
    app.kubernetes.io/version: 0.0.1
    app.kubernetes.io/component: frontend
    app.kubernetes.io/managed-by: manifest
    app.kubernetes.io/created-by: offsec
spec:
  selector:
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: nginx
        image: nginx
        imagePullPolicy: Never        
...
```

> Listing 13 - example DaemonSet

The example DaemonSet has tolerations declared within the _spec_ block. The _key_ value is the node role to match for the toleration. We can then specify different match methods for the toleration. Our example uses the _Exists_ match method, with the _effect_ set to the type of taint (_NoSchedule_ in this case), meaning that we tolerate the NoSchedule taint for existing nodes matching "node-role.kubernetes.io/master".

We can create taints and tolerations to treat nodes differently from the perspective of the scheduler. We can use taints and tolerations together to ensure Pods are not being scheduled to destinations we want those Pods to avoid.

If we taint a node, we make that node un-schedulable with a _cordon_.[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1608-5) We can cordon a node manually with **kubectl cordon**. This does not affect running Pods on the Node. Taking this a step further, we could use **kubectl drain** to evict Deployments and ReplicaSets from a Node. This will not, however, remove DaemonSets or mirror Pods (created from stand-alone Pods).

There is usually a taint on the control plane so that our applications don't end up there. We would use _drain_ to cordon nodes for maintenance activities, so that Deployment applications can be scheduled to other cluster nodes while we do maintenance. Let's cordon one of our lab worker nodes and check on the behavior.

If we have DaemonSets on the node we drain, we will get a warning about them during the drain, even though the cordon succeeds. We'll pass **--ignore-daemonsets** flag to **kubectl** to disregard the warning about the DaemonSets.

```
root@k8s# kubectl drain k8swrk1
node/k8swrk1 cordoned
error: unable to drain node "k8swrk1" due to error:cannot delete DaemonSet-managed pods (use --ignore-daemonsets to ignore): green/green-scaler-5gs49  continuing command...
There are pending nodes to be drained:
 k8swrk1
cannot delete DaemonSet-managed pods (use --ignore-daemonsets to ignore): green/green-scaler-5gs49

root@k8s# kubectl drain k8swrk1 --ignore-daemonsets
node/k8swrk1 cordoned
WARNING: ignoring DaemonSet-managed pods: green/green-scaler-5gs49
evicting pod green/green-scaler-5gs49
...(cut)...

root@k8s# kubectl get nodes
NAME     STATUS                     ROLES                  AGE   VERSION
k8s   Ready,SchedulingDisabled   control-plane,master   70d   v1.23.3
k8swrk2   Ready                      -          70d   v1.23.5
k8swrk1    Ready,SchedulingDisabled   -          70d   v1.23.5

root@k8s# kubectl uncordon k8swrk1
node/k8swrk1 uncordoned
```

> Listing 14 - cordon nodes

The STATUS column lists taints after the first comma. In our example, the taint is _SchedulingDisabled_. We can also use several other types of built-in taints. Taints are applied automatically by the node controller, so when a node in the cluster is not reachable, the node controller will taint that node with "node.kubernetes.io/unreachable".

Another type of Pod management we want to mention is a _StatefulSet_,[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1608-6) which is a state-sensitive type of Pod management. Pods that belong to a StatefulSet can be treated as a sort of longer-lived server: a stable resource in the cluster with a constant Pod name, DNS name, and storage. Pods in a StatefulSet are more persistent and less ephemeral than other types of Pods, which may be useful for an application such as a database.

1

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1608-1)

2

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1608-2)

3

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1608-3)

4

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1608-4)

5

(The Linux Foundation, 2022), [https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1608-5)

6

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1608-6)

## 5.2.6. Kubernetes Volumes

When data from Pods needs to persist past the life of the Pod, or shared between Pods, we'll often mount _Volumes_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1609-1) as a storage medium.

If Pods rely on a mounted Volume, we should be careful that the Volume mount is not a single point of failure and that it does not negatively impact performance.

Volume mounts can be plain disks, distributed block storage, object storage, or any storage system. There are many types of supported Volumes, but additional Volume technologies may be added via CRD to expand Volumes to custom protocols. In most cases, we will use object storage and block storage specific to public cloud providers. Kubernetes Volumes are an expansive topic we can't fully cover here, but let's consider a simple example.

In our lab cluster, we will use the least useful type of Volume, the _local_ Volume, which is used to mount directories on worker nodes. This is not very common in the real world; however, small-scale or single-node Kubernetes clusters may benefit from local mounts. More commonly, we would create external distributed Volumes, which are more resilient to outages, scale more effectively, reduce configuration requirements for nodes, and reduce the chance of reliance on a single disk or node.

Let's make a new Deployment that mounts a local Volume. Before we mount the local Volume, we'll create two directories to mount them on. On each worker node in our lab cluster, we'll create a **/mnt/cluster/log/** directory, then declare a Deployment that uses them.

```
kali@kali:~$ ssh offsec@k8sWrk1
...

offsec@k8sWrk1:~$ sudo -i
[sudo] password for offsec: 

root@k8sWrk1:~# mkdir -p /mnt/cluster/log

root@k8sWrk1:~# exit

kali@kali:~$ ssh offsec@k8sWrk2
...

offsec@k8sWrk2:~$ sudo -i
[sudo] password for offsec: 

root@k8sWrk2:~# mkdir -p /mnt/cluster/log

root@k8sWrk2:~# exit
```

> Listing 15 - creating a directory to mount

After creating the directories for our local Volume, we can construct some YAML that uses them. We'll use a Deployment that includes a local Volume mount on the NGINX logging path, resulting in the mounted container path being overridden by our Volume.

Here is the YAML for a Deployment that includes a local Volume mount:

```
root@k8s# cat /var/lib/misc/volumes.yml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-qa-nginx
  namespace: green
spec:
  replicas: 1
  selector:
    matchLabels:
      app: green-qa-nginx
  template:
    metadata:
      labels:
        app: green-qa-nginx
    spec:
      containers:
      - name: green-qa-nginx
        image: "nginx:latest"
        imagePullPolicy: Never
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
        - mountPath: /var/log/nginx/
          name: nginxlog

      volumes:
        - name: nginxlog
          hostPath:
            path: /mnt/cluster/log/
            type: Directory
...           
```

> Listing 16 - example directory mount Deployment

The _volumeMounts_ block includes the _mountPath_, which points to the location of the mount within the container. The _volumes_ block defines the type of Volume. With a local Volume, we define the _path_, which is the directory to mount on the host itself.

We'll use **kubectl apply** to declare the Deployment then inspect the directories. Before applying the example, ensure that the green namespace is in place.

```
kali@kali:~$ ssh offsec@k8sWrk1
Password:

offsec@k8s:~$ sudo -i

root@k8s:~# kubectl apply -f /var/lib/misc/volumes.yml
deployment.apps/green-qa-nginx created

root@k8s:~# exit

offsec@k8s:~$ exit

kali@kali:~$ ssh offsec@k8sWrk1 "ls -larth /mnt/cluster/log"
Password:
total 12K
drwxr-xr-x 3 root root 4.0K Jul  8 19:20 ..
-rw-r--r-- 1 root root    0 Jul  8 19:23 access.log
drwxr-xr-x 2 root root 4.0K Jul  8 19:23 .
-rw-r--r-- 1 root root  445 Jul  8 19:23 error.log

kali@kali:~$ ssh offsec@k8sWrk2 "ls -larth /mnt/cluster/log"
Password:
total 8.0K
drwxr-xr-x 3 root root 4.0K Jul  8 19:20 ..
drwxr-xr-x 2 root root 4.0K Jul  8 19:20 .
```

> Listing 17 - check worker Volume files

In this case, various NGINX logs have been created in the directories.

If we delete our Deployment, we will still have these NGINX logs on the mounted directories. Instead of mounting the logging directory, we could mount the application path or a file processing workspace used for data consolidation.

1

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1609-1)

#### Labs

1. Create a directory on each worker node of the lab cluster named **/mnt/maroon/workspace/**. Create a _falcon-persist_ Deployment in the _maroon_ namespace that has a single replica of the _prod-falcon:blue_ image. Create a ClusterIP Service in the maroon namespace using ClusterIP port 3 and targetPort 8000.

Use a Volume to mount the _prod-falcon:blue_ image in the _falcon-persist_ Deployment to **/mnt/maroon/workspace** with the container path of **/var/tmp**. Ensure no additional Deployments or Services are running, then run **/usr/sbin/cloud_grader** on the control plane lab VM to check your work and get a flag to submit below.

Answer

2. Update the _falcon-persist_ Service to use NodePort 30348 and expose the _falcon-persist_ Deployment. Use cURL to make an API request to the _falcon_ Service to send in the provided string "some data". Ensure the _offsec_ user can write to the **/mnt/maroon/workspace** directory on each worker node before copying over the file by setting the Unix ownership and/or permissions on the directory of each node.

```
curl -X POST --data-binary "some data" k8sWrk1:30348/api/encrypt/0 > falcon1.enc && scp falcon1.enc offsec@k8sWrk1:/mnt/maroon/workspace/ && scp falcon1.enc offsec@k8sWrk2:/mnt/maroon/workspace/
```

Ensure no extra Kubernetes objects are in place, then run **/usr/sbin/cloud_grader** afterwards to check your work and get a flag to submit below.

Answer

## 5.2.7. ConfigMaps

So far we have deployed containers into Pods and configured Kubernetes around the container image. We could also insert and manipulate files inside of the individual containers with _ConfigMaps_,[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1612-1) Kubernetes objects that can insert data (such as an HTML page or config file) into a container.

Any individual or system with full Kubernetes API access permissions can leverage ConfigMaps to modify data within containers in that API access scope. For example, an adversary with Kubernetes API access might update a Deployment to include a ConfigMap to deface a website.

We will use a type of ConfigMap Volume (written within a Deployment YAML) to override a system path within a container.

Unlike typical storage Volumes, ConfigMaps default to being stored within the Kubernetes API storage, such as etcd. Let's work through an example using a ConfigMap to create an HTML web page that contains a metrics report.

```
root@k8s# cat /var/lib/misc/configmap.yml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-page
  namespace: green
data:
  index.html: |
    <html>
    <head>
    </head>
    <body style="color:green; padding:2em; text-align:center; letter-spacing:0.5em;">
    <h1>Green Report</h1>
    <ol type="1" style="text-align:left; letter-spacing:0.2em;">
      <li>Stasis: 4.5%</li>
      <li>Granular: 1.2%</li> 
      <li>Marked: 0.0%</li> 
    <p style="text-align:right;">Thank you!</p>
    </body>
    </html> 
...
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: green
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        app.kubernetes.io/name: nginx
    spec:
      containers:
      - name: nginx
        image: "nginx:latest"
        imagePullPolicy: Never        
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
            - name: nginxwebroot
              mountPath: /usr/share/nginx/html
      volumes:
      - name: nginxwebroot
        configMap:
          name: custom-page
...
```

> Listing 18 - example ConfigMap

Our example has two API requests concatenated together, separated by the standard YAML syntax. The first is a Kubernetes API request to create the ConfigMap and the second is a Kubernetes API request to leverage that ConfigMap in a Deployment. The ConfigMap section uses the _data_ node of the YAML to map files to data. We map **index.html** to the HTML using the YAML block scalar syntax. The Deployment can then use a ConfigMap Volume, declaring the _configMap_ section in the request as the name of the ConfigMap created in the first request.

We typically don't need many ConfigMaps; however, they pose both an interesting benefit and risk. Using a ConfigMap like our example, we might choose to include only template config files for the applications in the public git repo, and then use ConfigMaps to populate them with the real values via Kubernetes API request, instead of a public git request to a more exposed repo that has the application. Separating final configurations from source code can be a benefit, because decoupling the management can help improve agility for the organization. That same agility and flexibility that ConfigMaps enable can be a security benefit in some cases, but is more likely to be a security risk. We'll learn more about these ConfigMap risks in other Learning Modules.

1

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/configuration/configmap/](https://kubernetes.io/docs/concepts/configuration/configmap/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1612-1)

#### Labs

1. Create a ConfigMap for an NGINX Deployment in the _maroon_ namespace. Name the Deployment "maintenance-page" and use a single replica of the _nginx_ image. Expose NGINX on port 30348 with a NodePort Service. Ensure no other Deployments or Services are in the maroon namespace. Using the ConfigMap, set the **index.html** to the following exactly:

```
<html>
<head></head>
<body style="padding:6em; text-align:center; color:blue; font-family:monospace; letter-spacing:0.3em; background-color:#d6d6d6"><h1>In Maintenance</h1>
<h2> This application is not available until maintenance is complete.</h2>
<p>Please contact our support group at noreply@localhost</p>
</body>
</html>
```

The required HTML for the ConfigMap is located in the lab VM in **/var/lib/misc/maintenance.html**. Run **/usr/sbin/cloud_grader** afterwards to check your work and get a flag to submit below.

Answer

## 5.2.8. CronJobs in Kubernetes

Much like the _cron_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1614-1) service in GNU/Linux, we can also execute time-based or one-off executions within Kubernetes via the _Job_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1614-2) feature. We could use this to run reports, hydrate caches or perform other one-off tasks that aren't easily accomplished in other ways.

For example, we can create a CronJob to run each minute by declaring a _schedule:_ of "* * * * *". This will result in a Pod being created each minute and running the declared _command_:

```
root@k8s# cat /var/lib/misc/falcon-check.yml
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: image-check-batch
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: batch-cycle
            image: prod-falcon:blue
            imagePullPolicy: 'Never'
            command:
            - /bin/sh
            - -c
            - b2sum /var/lib/salsa_falcon/salsa_falcon.py
          restartPolicy: OnFailure
...          

root@k8s# kubectl apply -f /var/lib/misc/falcon-check.yml
cronjob.batch/batch-cycle created

root@k8s# kubectl get pods -n green
NAME                                READY   STATUS      RESTARTS        AGE
batch-cycle-27495249-wkwql          0/1     Completed   0               2m55s
batch-cycle-27495250-hzngq          0/1     Completed   0               115s
batch-cycle-27495251-gnt6z          0/1     Completed   0               55s

root@k8s# kubectl logs batch-cycle-27495251-gnt6z -n green
6ea76653daa46fc5bb6be84755e8b9f584c45dc4de44b057ababda302cd789987e41bc6851ca85b308c171b69f6bbe3e5b14b8e475c8832016d54ebf869a2bac  /var/lib/salsa_falcon/salsa_falcon.py
```

> Listing 19 - example CronJob

In this example, the CronJob deploys the _falcon_ image and checks the _BLAKE2_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1614-3) hash of the application. If an adversary tampered with the application in the container image, we could then detect it by observing or measuring the hash change. The command output is sent to the Kubernetes log.

Since this generates a lot of output, we could adjust the schedule to run daily at 2 AM by setting _schedule_ to "* 2 * * *".

The syntax for CronJob schedules is as follows.

```
 ┌───────────── Minute, use digits 0 though 59.
 │ ┌───────────── Hour, use digits 0 though 23.
 │ │ ┌───────────── Day of month, use digits 1 though 31.
 │ │ │ ┌───────────── Month, use digits 1 through 12.
 │ │ │ │ ┌───────────── Day of week, use digits 1 though 7. 
 │ │ │ │ │              1 is Sunday in this case.                     
 │ │ │ │ │              Some systems have 7 as Sunday!
 │ │ │ │ │              Abbreviations of sun, mon, tue, wed, thu, fri, sat
 │ │ │ │ │              are also supported for day of week in Kubernetes.
 * * * * *
```

> Listing 20 - CronbJob scheduling

A Kubernetes CronJob could also be used maliciously. For example, an attacker could exfiltrate service data from a cluster every minute, sending it to another location.

To restrict this, we could restrict Kubernetes Jobs through kubectl config files that leverages Role Based Access Controls, which we will discuss in the next section.

1

(Linux man-pages project, 2022), [https://www.man7.org/linux/man-pages/man5/crontab.5.html](https://www.man7.org/linux/man-pages/man5/crontab.5.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1614-1)

2

(The Linux Foundation, 2022), [https://kubernetes.io/docs/concepts/workloads/controllers/job/](https://kubernetes.io/docs/concepts/workloads/controllers/job/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1614-2)

3

(BLAKE2, 2022), [https://www.blake2.net/](https://www.blake2.net/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1614-3)

## 5.2.9. Role Based Access Controls

Once a cluster is set up and configured, organizations often want to include Kubernetes API actions within their automation. For example, we may want to use a continuous deployment server like _Jenkins_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1615-1) to automatically update Deployments when YAML is changed in source control. In this case, if we put the **admin.conf** on the Jenkins server, we essentially give Jenkins full control over the cluster. However, it's more secure to use a _ServiceAccount_ with more limited permissions.

We can segment permissions in this way with _Role Based Access Controls_ (RBAC)[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1615-2) within a Kubernetes cluster between Kubernetes clients, systems, or users. To do this, we would first create Roles by _RoleBinding_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1615-3) YAML requests, associating a declared Role within the cluster to a ServiceAccount. This would create RBAC for the ServiceAccount. We could then create a new kubectl config that is limited to that Role.

Let's demonstrate this. We'll use Kubernetes RBAC to create a Role that only allows listing and describing actions on Deployment and Service objects in the cluster.

Before we begin, we should review all of the potential Kubernetes API objects for the current cluster with **kubectl api-resources**.

```
root@k8s# kubectl api-resources
NAME                               SHORTNAMES           APIVERSION                             NAMESPACED   KIND
bindings                                                v1                                     true         Binding
componentstatuses                  cs                   v1                                     false        ComponentStatus
configmaps                         cm                   v1                                     true         ConfigMap
endpoints                          ep                   v1                                     true         Endpoints
events                             ev                   v1                                     true         Event
limitranges                        limits               v1                                     true         LimitRange
namespaces                         ns                   v1                                     false        Namespace
nodes                              no                   v1                                     false        Node
persistentvolumeclaims             pvc                  v1                                     true         PersistentVolumeClaim
persistentvolumes                  pv                   v1                                     false        PersistentVolume
pods                               po                   v1                                     true         pod
podtemplates                                            v1                                     true         podTemplate
replicationcontrollers             rc                   v1                                     true         ReplicationController
resourcequotas                     quota                v1                                     true         ResourceQuota
secrets                                                 v1                                     true         Secret
serviceaccounts                    sa                   v1                                     true         ServiceAccount
...(cut)...
```

> Listing 21 - kubectl api-resources

Each name in the left column of the output is a Kubernetes Resource we can use for Kubernetes API calls with kubectl. The _SHORTNAMES_ column can be used in kubectl commands.

We can now build our RBAC YAML to limit ServiceAccounts to specific API objects. Any user with a ServiceAccount config is limited to the scope of that ServiceAccount.

We can also apply a ServiceAccount to an automation system to add more layers of identity and authorization and force users to submit API requests through that automation.

Our example _watcher_ role is for typical read-only needs with only the ability to check on Service and Deployment objects.

```
root@k8s# cat /var/lib/misc/watcher-rbac.yml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: watcher
  namespace: default
...
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: sdr
rules:
- apiGroups: ["", "apps"]
  resources:
  - deployments
  - services
  verbs: ["list", "get"]
...
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: watcher-apps
  namespace: green
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sdr
subjects:
- kind: ServiceAccount
  name: watcher
  namespace: default
...
```

> Listing 22 - example RBAC YAML

The first section of this YAML defines a new Role named _watcher_. The second section creates a ClusterRole named _sdr_ and defines which APIs and resources it can reach. The third section binds the Role to the ClusterRole, defining the _watcher_ as a _subject_ in the _sdr_ ClusterRole. We can apply these YAML segments individually, or group them as we did in this example.

More specifically, the first section of this YAML sets the _kind_ to "ServiceAccount", and only sets two metadata values, _name_ and _namespace_. The next block of YAML sets the _kind_ to "ClusterRole". We will name the ClusterRole _sdr_, and here we will define the restrictions. In this case, we're only allowing interaction with specific _resources_ (deployments and services), and only allowing the kubectl _verbs_ of _get_ and _list_. The last block of YAML then binds the ServiceAccount to the role with _kind_ of "RoleBinding". The RoleBinding takes the accounts in _subjects_ and binds them to the roles described in the _RoleRef_ section. With RoleBinding, we can bind multiple accounts to multiple Roles; however, our example binds one account named _watcher_ to one Role named _sdr_.

In order to create a kubectl config file for this ServiceAccount, we will need to extract the credentials from Kubernetes. We will use kubectl with ServiceAccount[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1615-4) (_sa_) objects as cluster admin to extract the credentials for our _watcher_ account. The ServiceAccount objects, including their corresponding access credentials, are stored in the API storage along with all other Kubernetes objects.

We can query the Kubernetes API for all _sa_ objects by passing the **-A** argument to **kubectl get sa**.

```
root@k8s# kubectl get sa -A
...(cut)...
```

> Listing 23 - getting all ServiceAccounts

Note that a cluster admin kubectl config allows access to secrets for all ServiceAccount objects, and any other secrets in the cluster. The abbreviation "sa" here is the short name for the Kubernetes object type of ServiceAccount. When using kubectl, we can always specify the short name for any Kubernetes object.

If we want to identify an _sa_ that we want to access, we can use **kubectl describe sa**, setting **-A** to return all namespaces. Then, if the ServiceAccount has a secret used for access, we can run **kubectl get secret**, providing the secret name with an appended uid, followed by **--namespace=default** to define the namespace used by the ServiceAccount and **-o yaml** to output the secret data in YAML format:

```
root@k8s# kubectl describe sa -A | grep watcher-token
Mountable secrets:   watcher-token-2prkt
Tokens:              watcher-token-2prkt

root@k8s# kubectl get secret watcher-token-2prkt --namespace=default -o yaml
apiVersion: v1
data:
  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1EVXhOakl4TURnME5sb1hEVE15TURVeE16SXhNRGcwTmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTWszCmd6RW51S1cvMkxVZEhJYzF5am0xM0RIdTVOc1NyS1ZlUHJMd0R5NThHRTFCMFBkOGZTd1UxUkxJK1hPeENYa3oKQTBWY2JVTGZER0tzRVUzYUVJOGJYemZrdUdyaDY2aFA3WVdUeWpDUDNad1NCbGVZclIwemhuQnpNQWptLzdkbgpzRWV3dHNDUGdRQ1pnV2toMlZMcVdpcmxoZXNVUDZRSVoxWUVydXE0cndZeE1CUlo5NnBXcE03Wkp5aW00RHVtClo2TnUvMUM5RndLdDNXQUdMSFNtV2UzNEw3Y3h6UTAxMXd0OVBvdU9xV25YKzVJaDBaOHVWTkp2ZWkxM1VBVTYKNzljT243Mnl6dGFITVhXeFExaDBYK0hlMlRMZTRnMDlpM3kvck5uR3ZWdEl2bUtZMlgxOHdJQWJiVXkycDlKMgphWDEwZ0VxZ1dHbjBxU2toRU8wQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZLRFI0OGtCejgzV0FnSjJVbVk5bTNSL01KcGRNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ295Q1FMb1Q0Y0lFaFRvTXBaZQpyMFAxeTB6aDJHWGs1cVBJd3VHcFFRTGtjZ0xlRHdiMThJQUIwanhkZnhhUG5CZno4b2I0Y3FHaTU3ZlRpWlNOClZzbmFqM1RQZDFraVU1S0g3dkJxZUxOZW9ZZTdtL1BGZXp6QVFPVGc4QmYzdnJ6c2pleC95dENsVFVLVUJJUmoKNUU4ZzlMUzU2cDZrU2s0L25NMjhzZTM0LzRQQUp3N0JwNHh0RU5Sb1VYV0JFQk0zTWJOTjR2Z1pFaXR1bitOQwo3Slh6d0Q1eFBVZmlzTEtuODNKdVBMUFpBMVlwM25OQ3JzdmZJQnh4YkcxTjhFS2J4cjdDbm5FaVlJdmJicjkrCi9nMW0zYXVTTmpWT1ZwTUE5cE12ejh2eUFpUkNvS2lMY1VRNy9INGNYekZqQlVrb2tCMVBsMUtJanJhRXpoYk0Kc253PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
  namespace: ZGVmYXVsdA==
  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkluSm1WVmxrTjFkRGFFVXplbk5tTFZGRGRURmxMV1JSWmpKS2JWVXdXa3B4Y1Zwd1UxWnBVVFp1VjJzaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbmRoZEdOb1pYSXRkRzlyWlc0dE1uQnlhM1FpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pZDJGMFkyaGxjaUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbUUxTXpnek1Ua3hMVGM1Wm1FdE5EVXdOaTFpWkRjekxUWTVZVEkzTldGa1ltRmlaaUlzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT25kaGRHTm9aWElpZlEuSXhYOUYwaFpoVWQtNmtOXzI4T1NDNWVGM2s5YW0zZUlBZms0d095SXJGVUpRbmdwaU9jb1lub2NJcnFJblg4cm5OcGNCNEpuMVVjOTVHem1talVKODhNa243RW5NRnFhMTFQOEE2ZU5PT2R3OGdwSzlaSTZIWUxRT0lJdGo4RXJpSUUyVjM4dld3QnpuMk1JZm5QM1dUVm9HQlpDbzNYOHI5QjBsckFqSWtKZjU2cWRpZ1NjUURURlpzOUcwM2VWc25qaUFub05ON3NoRkY1Z1RITjFkVEhHekJYdk01RU03aG1NeFl4ZkxFMl9WV0UwZHFLcWx4NktxVlFsR1NTbmRXdDljekNPU2hWWDVBbVB5SHloVk8tbW9taDI5WmF0d0k0ZV9yM1ZWVTE0T3dWS19RaXNfMEZoOFdrZlhZcnZwNlM4WWtINVYybUZoRGVGMXVRS09B
kind: Secret
metadata:
  annotations:
    kubernetes.io/service-account.name: watcher
    kubernetes.io/service-account.uid: a5383191-79fa-4506-bd73-69a275adbabf
  creationTimestamp: "2022-05-16T21:12:34Z"
  name: watcher-token-2prkt
  namespace: default
  resourceVersion: "1061"
  uid: 258d4500-e788-4171-84e8-af9998e7a73a
type: kubernetes.io/service-account-token
```

> Listing 24 - get ServiceAccount token

We can then use the output of this to construct or update a kubectl config file to give the ServiceAccount to the needed Kubernetes API clients. Note that when we extract the token with **kubectl get secret** the token is base64 encoded twice. The format we want in the config needs only one layer of base64 encoding. Alternatively, the token can be extracted with **kubectl describe secret** which will output with only one layer of base64 encoding.

```
root@k8s# kubectl describe secret watcher-token-2prkt
Name:         watcher-token-2prkt
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: watcher
              kubernetes.io/service-account.uid: 766f8533-ab2d-4b87-b955-929740329066

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1099 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InJmVVlkN1dDaEUzenNmLVFDdTFlLWRRZjJKbVUwWkpxcVpwU1ZpUTZuV2sifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6IndhdGNoZXItdG9rZW4tMnBya3QiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoid2F0Y2hlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImE1MzgzMTkxLTc5ZmEtNDUwNi1iZDczLTY5YTI3NWFkYmFiZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OndhdGNoZXIifQ.IxX9F0hZhUd-6kN_28OSC5eF3k9am3eIAfk4wOyIrFUJQngpiOcoYnocIrqInX8rnNpcB4Jn1Uc95GzmmjUJ88Mkn7EnMFqa11P8A6eNOOdw8gpK9ZI6HYLQOIItj8EriIE2V38vWwBzn2MIfnP3WTVoGBZCo3X8r9B0lrAjIkJf56qdigScQDTFZs9G03eVsnjiAnoNN7shFF5gTHN1dTHGzBXvM5EM7hmMxYxfLE2_VWE0dqKqlx6KqVQlGSSndWt9czCOShVX5AmPyHyhVO-momh29ZatwI4e_r3VVU14OwVK_Qis_0Fh8WkfXYrvp6S8YkH5V2mFhDeF1uQKOA
```

> Listing 25 - describe ServiceAccount token

The version that starts with "ey" is the version we'll want in our kubectl config for the ServiceAccount. This format is a JWT with each section of the JWT base64 encoded, delimited by periods. The first section of the JWT contains the algorithm and key id, while the second section contains the claims, and the third section which contains the signature.

We can create ServiceAccounts that have a smaller scope or specific scope, and give those credentials to operators, special Pods, developers, managers, service support, or external automation systems for integrations and deployments.

We can create the kubectl config for the ServiceAccount in several ways. For this example, we will manually create a file and place it in **/home/offsec/.kube/config**, then switch users to the offsec user and test the ServiceAccount with kubectl.

Let's examine the default cluster admin config on the control plane server in our lab cluster. We'll switch things up and use a JWT[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1615-5) token instead of a mTLS certificate and key pair for authentication. The example template file is coded for a ServiceAccount name of _watcher_.

```
root@k8s# cat ~/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ERXpNVEF6TVRZek5sb1hEVE15TURFeU9UQXpNVFl6Tmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTVUwClM3ODMvMkRhODJSWTY0bXRlMmpyYU1FS2lsdEtuU0d5VS8xN05jT1U4Z3RwRlVaUEc1aXpRNGFHL01KTVh4U2gKc3N0aHJwb1B1cDNmR3pwaTFFYUFrenpwZWo0WDhEQjlJQUg5K1pxdlZrclU1cm1VNkQyYTgzVngybnlwVjZsbgpsUnhsNFRlZHV6eWlWNXF2aU96RC8wMHRkZXFBWGtPcExpZ3VKVWZwUy9oN2c5TmRtdW1mUzNoYitZdGxmOU1oClFjcWtWaUsvK0JZam1GM2RLRmUvMXpzL3BUOUtBeHdqUXFqQVdNUkw0MUovb1J1VVJjOGdSVEhXUmR4K1M2OWgKZ2JFOTRlTUpGYzhsbkMrRXNyVXFDTzNpQzZkMndBZkpiNld5Vm1VRy82TWtHTHMrUVg4VGtMdHY0OFFDelhnagp5cHBDVC9JYnV0eHEydm9DYUFzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZQQW9paUQ1dWYzTnBkdjN5N1NwNGl6R2NOcENNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBTE1IbVRBYXZVbUxQVThvb3hmdApYYTRKVVBCR1BPUjhTMmY5akQ3TDAyU2N3dmlPWHZjUWF2eDdPZFVPamNjVnlBUHhBcndvTW11QTRvNGFyV21JCmJ6RUh6cEQvWjhmUjhxK2NGaXhMNW9YSURZaXdhR25GK3paVys0RE9jZmZ6SG4xczNVajdIa0dmR3IzSjZnbm4KczdNZzkvZFg2aDhScWtaMEVvdjM0SDMzNkFPV0RCdStRem9nVTdLZGR2c2tpS3liVm50VnlzYkNvaFJiMXlyVQpCd25CeXZJTkNNQXpjdlRMa0htWGxRYXJQKzlFMXk0R05UOFJRUnZWblcySWNuUGRCV1JjK0RlOUtjcUR4aDM1CjBUN2wzVHdlYkN4VldmMVlhamdtdGRWc3JQYVFWY3ZwOEFyeGliNmd4TW52b04rOE9WZEZGT2c4ZmM0OTRKLzAKdm9BPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.51.92:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: 
...(cut)...
root@k8s# cat /var/lib/misc/template-config.yml
apiVersion: v1
kind: Config
clusters:
- name: kubernetes
  cluster:
    certificate-authority-data: "_REPLACE_WITH_API_CA_"
    server: "_REPLACE_WITH_API_ENDPOINT_"
contexts:
- name: watcher
  context:
    cluster: kubernetes
    namespace: green
    user: watcher
current-context: watcher
users:
- name: watcher
  user:
    token: "_REPLACE_WITH_API_TOKEN_"

```

> Listing 26 - account configs

The values encompassed in "${}" in the example kubectl config are the minimum required variables for a usable kubectl config.

A single config file might include configurations for multiple clusters or ServiceAccounts. We can use kubectl to reference multiple accounts in the same config file. We can list the accounts in the file with **kubectl config get-contexts**, and switch with **kubectl config use-context somecontext**.

To extract the CA certificate for the Kubernetes API that we need, we could gather it from the cluster admin config we have, or we could use **kubectl** to print out the base64-encoded cluster CA certificate:

```
root@k8s# kubectl get cm/cluster-info --namespace kube-public -o yaml
apiVersion: v1
data:
  kubeconfig: |
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ERXpNVEF6TVRZek5sb1hEVE15TURFeU9UQXpNVFl6Tmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTVUwClM3ODMvMkRhODJSWTY0bXRlMmpyYU1FS2lsdEtuU0d5VS8xN05jT1U4Z3RwRlVaUEc1aXpRNGFHL01KTVh4U2gKc3N0aHJwb1B1cDNmR3pwaTFFYUFrenpwZWo0WDhEQjlJQUg5K1pxdlZrclU1cm1VNkQyYTgzVngybnlwVjZsbgpsUnhsNFRlZHV6eWlWNXF2aU96RC8wMHRkZXFBWGtPcExpZ3VKVWZwUy9oN2c5TmRtdW1mUzNoYitZdGxmOU1oClFjcWtWaUsvK0JZam1GM2RLRmUvMXpzL3BUOUtBeHdqUXFqQVdNUkw0MUovb1J1VVJjOGdSVEhXUmR4K1M2OWgKZ2JFOTRlTUpGYzhsbkMrRXNyVXFDTzNpQzZkMndBZkpiNld5Vm1VRy82TWtHTHMrUVg4VGtMdHY0OFFDelhnagp5cHBDVC9JYnV0eHEydm9DYUFzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZQQW9paUQ1dWYzTnBkdjN5N1NwNGl6R2NOcENNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBTE1IbVRBYXZVbUxQVThvb3hmdApYYTRKVVBCR1BPUjhTMmY5akQ3TDAyU2N3dmlPWHZjUWF2eDdPZFVPamNjVnlBUHhBcndvTW11QTRvNGFyV21JCmJ6RUh6cEQvWjhmUjhxK2NGaXhMNW9YSURZaXdhR25GK3paVys0RE9jZmZ6SG4xczNVajdIa0dmR3IzSjZnbm4KczdNZzkvZFg2aDhScWtaMEVvdjM0SDMzNkFPV0RCdStRem9nVTdLZGR2c2tpS3liVm50VnlzYkNvaFJiMXlyVQpCd25CeXZJTkNNQXpjdlRMa0htWGxRYXJQKzlFMXk0R05UOFJRUnZWblcySWNuUGRCV1JjK0RlOUtjcUR4aDM1CjBUN2wzVHdlYkN4VldmMVlhamdtdGRWc3JQYVFWY3ZwOEFyeGliNmd4TW52b04rOE9WZEZGT2c4ZmM0OTRKLzAKdm9BPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
        server: https://192.168.51.92:6443
      name: ""
    contexts: null
    current-context: ""
    kind: Config
    preferences: {}
    users: null
kind: ConfigMap
metadata:
  creationTimestamp: "2022-04-04T03:16:57Z"
  name: cluster-info
  namespace: kube-public
  resourceVersion: "224588"
  uid: 211a383f-8d42-4332-9656-af136b329dd0
```

> Listing 27 - kubectl get ca certificate

After gathering the ServiceAccount token, Kubernetes API endpoint, and Kubernetes API CA certificate, we can create the new config file for a ServiceAccount. We'll create a new directory for the _offsec_ user and copy our template into a new file.

```
root@k8s# mkdir /home/offsec/.kube && chown offsec:offsec /home/offsec/.kube

root@k8s# cat /var/lib/misc/template-config.yml > /home/offsec/.kube/config
```

> Listing 28 - creating a kubectl config

After creating **/home/offsec/.kube/config**, we can use the text editor of our choice to populate the **/home/offsec/.kube/config** file with the base64-encoded cluster CA certificate, ServiceAccount token, and control plane IP and port. The control plane IP and port goes in the "server" field of the config, just like we have in our cluster admin config. Make sure the token used in the config has only one layer of base64 encoding, not two. We can then test to make sure the _watcher_ ServiceAccount is now functioning for the _offsec_ user.

```
root@k8s# exit
offsec@k8s:/home/offsec$ kubectl get deployments -n green
AME                READY   UP-TO-DATE   AVAILABLE   AGE
green-prod-falcon   1/1     1            1           6h
green-qa-nginx      1/1     1            1           6h

offsec@k8s:/home/offsec$ kubectl get deployments -n kube-system
Error from server (Forbidden): deployments.apps is forbidden: User "system:serviceaccount:default:watcher" cannot list resource "deployments" in API group "apps" in the namespace "kube-system"
```

> Listing 29 - testing a kubectl config

When the example watcher ServiceAccount is used, it can only access the configured scope within the cluster. When attempting to access the kube-system, we are forbidden because we did not include that scope in the ServiceAccount. We configured this ServiceAccount to only be able to "list" and "get" the "deployment" and "service" objects within the "green" namespace.

Note that while we are doing this work manually there are various automation solutions. However, it's best to understand the underpinnings of automated solutions.

Based on the potential risk and complexity of RBAC, best practice dictates a minimal RBAC for the given situation. For example, we should consider a single custom ServiceAccount per automation scope and should block users and other external systems from direct Kubernetes API access.

1

(Jenkins, 2022), [https://jenkins.io](https://jenkins.io) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1615-1)

2

(The Linux Foundation, 2022), [https://kubernetes.io/docs/reference/access-authn-authz/rbac/](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1615-2)

3

(The Linux Foundation, 2022), [https://kubernetes.io/docs/reference/access-authn-authz/rbac/#api-overview](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#api-overview) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1615-3)

4

(The Linux Foundation, 2022), [https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1615-4)

5

(IETF, 2015), [https://www.rfc-editor.org/rfc/rfc7519](https://www.rfc-editor.org/rfc/rfc7519) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1615-5)

#### Labs

1. Create a _data-harvester_ ServiceAccount that is restricted to the "maroon" namespace. Use "apps", "extensions", and "" in the _apiGroups_ declaration. Bind _data-harvester_ to a _harvest_ ClusterRole that allows any _verbs_ by using "*" for verbs, and only with Deployments. Ensure any other Kubernetes objects are removed from the lab cluster, then run **/usr/sbin/cloud_grader** on the control plane node to check your work and get a flag to submit below.

Answer

## 5.2.10. Kubernetes Network Policy

_NetworkPolicies_ define what is allowed, and what is not, in terms of network access within the cluster. NetworkPolicy, like all networking in Kubernetes, can leverage the Linux in-kernel networking to abstract the network away from IP addresses and into labels.

For example, we can declare that "frontend" labelled Pods are the only Kubernetes objects that can reach "backend" labelled Pods:

```
root@k8s# cat /var/lib/misc/network_policy.yml
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: backend-access
  namespace: green
spec:
  podSelector:
    matchLabels:
      app: green-prod-falcon
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: nginx
...
```

> Listing 30 - example NetworkPolicy

The example NetworkPolicy applies to Pods with the _app_ label set to "green-prod-falcon". The NetworkPolicy states that the ingress to green-prod-falcon is only allowed from Pods with the _app_ label set to "nginx".

NetworkPolicy is a native way to control the access between Pods in the cluster. We can use Network Policy to design networks however we like within the scope of the cluster. A CNI plugin may extend the NetworkPolicy related functionality in different ways.

When running Kubernetes, many different core and modular components may make changes to the networking. This means that using iptables directly may not be a good idea on Kubernetes nodes as Kubernetes writes iptables rules automatically. Instead of manually managing host filtering, we can manage host filtering via Kubernetes API YAML as well.

NetworkPolicy is considered a best practice and is ideally written along with Deployments and Services to define how network access within the cluster should behave. This is a benefit to organizations as it enables developers and others to better understand and assist with the networking configuration. For example, we might allow developers to write NetworkPolicy within a specific namespace they use for development or testing, and then suggest the desired NetworkPolicy YAML manifests for use in production for their application scope.

#### Labs

1. Create a _pink-exoservices_ NetworkPolicy in the _maroon_ namespace. The pink-exoservices NetworkPolicy is to allow all access to Pods labelled "nginx", and allow only nginx Pods to access "blue-prod-falcon" labelled Pods. Create three Deployments in the _maroon_ namespace that deploy "nginx", "prod-falcon:blue", and "prod-falcon:green" Pods with one replica of each. Name the Deployments "nginx", "blue-prod-falcon", and "green-prod-falcon". Use NodePort Services to expose port 30380 to NGINX, and NodePort 30333 for blue-prod-falcon and 30334 for green-prod-falcon. Ensure no additional Deployments or Services are in the _maroon_ namespace then run **/usr/sbin/cloud_grader** afterwards to check your work and get a flag to submit below.

Answer

2. In the _maroon_ namespace, create a DaemonSet that is named _test_ with 1 of "prod-falcon:blue" and no tolerations. Expose the _test_ DaemonSet with a NodePort Service on port 30348. Create a NetworkPolicy that is named _pink-exoservices_ that only allows Pods with the label "app: bogon" to access the Pods in the _test_ DaemonSet. Ensure no other Kubernetes objects from other challenge questions, including Roles, are in place then run **/usr/sbin/cloud_grader** on the control plane node to check your work and get a flag to submit below.

Answer

## 5.2.11. Seccomp and eBPF in Kubernetes

The most radical and exciting technology in Kubernetes is Extended Berkeley Packet Filter (eBPF). We won't be explaining eBPF fully in this Learning Module, but this will serve as a gentle introduction. eBPF is the successor to BPF, the Linux kernel feature that enabled _tcpdump_[1](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-1) and _Wireshark_[2](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-2) to attach to a network interface to sniff traffic.

The two areas of eBPF that we will introduce with Kubernetes are _eBPF for syscalls_[3](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-3) and _eBPF for networking_. _Seccomp_[4](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-4) has been part of the Linux kernel for some time and uses eBPF to trace system calls (syscalls) to the kernel for resources. Along with this tracing, we can also restrict and manipulate syscalls. Kubernetes can leverage seccomp to limit Pods from fewer than the available 334 syscalls available in modern Linux. By comparison, Docker uses a default seccomp profile that throttles all but 51 syscalls. Kubernetes does not implement a default seccomp profile, although has a beta version as of Kubernetes v1.25. As of the time of this writing, we would need to implement our own seccomp profiles for Kubernetes in most clusters.

Networking with eBPF is newer but has gained significant adoption recently. This allows us to use custom networking outside of iptables and netfilter and even implement custom protocols. Practically speaking, we can use eBPF to eliminate _Network Address Translation_ (NAT) within the Kubernetes cluster, collect network metrics, create label-based routing and security, and performance optimizations like _Direct Server Return_ (DSR)[5](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-5) and _Express Data Path_ (XDP).[6](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-6) _Calico_[7](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-7) CNI is a great way to get started with eBPF, providing ready-to-use, production-quality eBPF networking for Kubernetes.

Networking with eBPF is commonplace on the internet. Google, Amazon, CapitalOne, and many others use eBPF networking. We can use eBPF networking with a single imperative patch via Calico CNI using the _calicoctl_[8](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fn-local_id_1622-8) binary, which we'll do in a later Learning Module.

From a security perspective, keep in mind that if Kubernetes can access eBPF, then a cluster admin or root user can also access eBPF. An adversary with system access might load eBPF maps into a node kernel to exfiltrate data or manipulate network traffic. The security of eBPF is an important area to consider as we build cloud systems. eBPF provides us with great security tools, but is also an attack vector.

Although the default security posture of Kubernetes is not necessarily strong, it contains world-class security capability and tools. With the techniques we have explored in Introduction to Kubernetes I and Introduction to Kubernetes II, we have built a foundation to explore some common security components of Kubernetes clusters and can begin using Kubernetes in the real world.

1

(The Tcpdump Group, 2022), [https://www.tcpdump.org/](https://www.tcpdump.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-1)

2

(Wireshark, 2022), [https://www.wireshark.org/](https://www.wireshark.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-2)

3

(Linux man-pages project, 2022), [https://man7.org/linux/man-pages/man2/syscalls.2.html](https://man7.org/linux/man-pages/man2/syscalls.2.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-3)

4

(Linux Kernel Project, 2022), [https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html](https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-4)

5

(Tigera, 2022), [https://projectcalico.docs.tigera.io/maintenance/ebpf/enabling-ebpf#try-out-dsr-mode](https://projectcalico.docs.tigera.io/maintenance/ebpf/enabling-ebpf#try-out-dsr-mode) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-5)

6

(Tigera, 2022), [https://www.tigera.io/learn/guides/ebpf/ebpf-xdp/](https://www.tigera.io/learn/guides/ebpf/ebpf-xdp/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-6)

7

(Project Calico, 2022), [https://www.tigera.io/project-calico/](https://www.tigera.io/project-calico/) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-7)

8

(Tigera, 2022), [https://projectcalico.docs.tigera.io/maintenance/clis/calicoctl/install](https://projectcalico.docs.tigera.io/maintenance/clis/calicoctl/install) [↩︎](https://portal.offsec.com/learning-paths/cloud-essentials-167535/learning/introduction-to-kubernetes-ii-40889/kubernetes-features-40911/seccomp-and-ebpf-in-kubernetes-41266#fnref-local_id_1622-8)