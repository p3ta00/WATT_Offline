In _Container for Cloud I_, we explored using Docker and Podman to start containers and then discussed how to build container images. When dealing with containers, Docker, Podman, and Kubernetes will usually be our tools of choice. However, as we dive deeper, we'll notice that there is a lot of overlap and interoperability between these tools.

The overlap and interoperability is both due to the tools using the same Linux primitives as well as the standardization done by the _Open Container Initiative_. Knowing how these tools work behind the scenes is crucial to understanding how containers are secured and how to escape out of them.

In this Leaning Module, we'll manually create a container using the same features that Docker, Podman, and Kubernetes use. It's important to note that we're only using this rare process as a teaching tool and manually creating containers this way in production is inadvisable.

The following Learning Units will be covered in this Learning Module:

- Containers, The Hard Way
- Open Container Initiative

Each learner moves at their own pace, but this Learning Module should take approximately 7 hours to complete.

## 2.1. Containers, The Hard Way

This Learning Unit covers the following Learning Objectives:

- Examine the history and current state of containers
- Create a semi-sandboxed environment with chroot
- Use Linux namespaces to manually create a container
- Limit resources with Control Groups
- Understand Linux Capabilities

While there are many tools available to simplify container creation, comprehending the mechanics is critical. It will help us understand container vulnerabilities, how to better build images, debug containers, and much more.

Linux does not have a single feature called a "container". Instead, what we know as a container is actually combination of features that prevent a process from accessing other processes and resources. Because this occurs at the kernel level, we can control a container's level of restriction.

While we might think of containerization as a new technology, the first restriction that resembled a container was actually introduced in the 1970s with _chroot_ (change root).[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_593-1) The chroot system call allows a process to run in a new location in the filesystem. However, this process can still interact with other processes, view and change network configurations, and more.

In 1999, FreeBSD released _Jails_.[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_593-2) Jails are built on top of chroot, but offer much more segmentation, specifically locking down the parts that chroot did not. Jails are very similar to containers. However, they were not supported by Linux.

In 2002, Linux introduced the concept of _namespaces_[3](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_593-3) to separate kernel resources when needed. For example, a process could be placed in a separate network namespace and the process would not be aware of the host network or any other processes network. There are currently eight different namespaces available in Linux: mount, process ID, network, Interprocess Communication, UTS, User ID, control groups, and time.

In 2006, Google introduced process containers to Linux, which later became known as _cgroups_.[4](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_593-4) This feature allows the limiting of memory and CPU resources to a particular process.

Using cgroups and namespaces, the _Linux Containers_ (LXC) project was released. LXC allowed users to create containers that would run in separate namespaces and would have resource limits imposed by cgroups. Later, Docker would make use of LXC to create the modern container.

In this Learning Unit, we will use chroot, namespaces, cgroups and more to manually create a container.

Each learner moves at their own pace, but this Learning Unit should take approximately 5.5 hours to complete.

1

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/Chroot](https://en.wikipedia.org/wiki/Chroot) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_593-1)

2

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/FreeBSD_jail](https://en.wikipedia.org/wiki/FreeBSD_jail) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_593-2)

3

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/Linux_namespaces](https://en.wikipedia.org/wiki/Linux_namespaces) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_593-3)

4

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/Cgroups](https://en.wikipedia.org/wiki/Cgroups) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_593-4)

## 2.1.1. Accessing the Lab

In this Learning Module, we'll use a single server to learn about containers named _containers_. This is a bare-bones Ubuntu machine with the required tools installed.

To access the server, we have created an **/etc/hosts** file entry on our Kali Linux VM.

```
kali@kali:~$ sudo mousepad /etc/hosts

kali@kali:~$ cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       kali

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.121.131  containers
```

> Listing 1 - /etc/hosts entry

For now, we'll simply set up our **hosts** file on our Kali machine and start the virtual machine.

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

containers

## 2.1.2. Change Root

Change root (chroot) is one of the most primitive types of sandboxing available on Linux. While it does provide some form of security control, chroot is known to be easily bypassed. Using chroot, we can indicate a process's new root directory. Chroot is also commonly used to fix non-bootable systems by mounting the root to a working system during system installation or system recovery.

Using chroot means that a process is forced to only have access to a specific directory in a file system. For this to work, the chosen directory must contain all the binaries needed to run the process, and ideally, the process would not be able to escape from the directory.

```
/
├── bin
│   ├── bash
│   ├── ls
|   └──...
├── etc
│   ├── crontab
│   ├── passwd
│   └── ...
└── home
    └── ubuntu
        ├── superSensitiveFile.txt
        └── newroot
        ├── bin
        │   ├── bash
        │   ├── ls
        |   └──...
        └── etc
            ├── crontab
            ├── passwd
            └── ...
```

> Listing 2 - Linux filesystem with new root in home directory

In Listing 2, we'll find a Linux filesystem where **/home/ubuntu/** contains a directory called **newroot** and a file called **superSensitiveFile.txt**. If we were to use chroot to specify **/home/ubuntu/newroot** as the root directory, the process should not have access to anything above **newroot**.

However, while limited to the directory, the process in the chroot still has unfettered kernel access to the network, mount, and other kernel features. We won't discuss chroot escapes in this section, but it's important to expect them. If an attacker were to hack a process running in a chroot, we should assume they are able to escape to the rest of the host.

Let's use chroot to run bash in a new root directory. The _containers_ host has two directories under **/home/student/roots/**.

```
student@kali:~$ ssh student@containers
student@containers's password: 
...

student@container:~$ ls roots/
bionic   min
```

> Listing 3 - Connecting to containers host via SSH

The first directory is called **bionic** and has a minimal filesystem of Ubuntu 18.04. The other is **min**, which is the most minimal file system needed to run bash and ls. We'll examine the **min** directory first.

Let's investigate the directory structure of **min** using **tree**, which will display the entire directory structure.

```
student@container:~$ tree roots/min/
roots/min/
├── bin
│   ├── bash
│   └── ls
├── lib
│   ├── libc.so.6
│   ├── libdl.so.2
│   ├── libpcre2-8.so.0
│   ├── libpthread.so.0
│   ├── libselinux.so.1
│   └── libtinfo.so.6
├── lib64
│   └── ld-linux-x86-64.so.2
└── secret
```

> Listing 4 - Running tree on the min directory

The directory has three sub-directories. The **bin** directory contains the **bash** and **ls** binaries. The **lib** and **lib64** directories contain the libraries required for bash and ls to run. There is also a secret file for us to try to read in the chroot.

Let's run **chroot** with **sudo**. We'll specify the directory we want to run in (**/home/student/roots/min**) and the file we want to execute (**bash**). However, because the file we execute is relative to the new root, the full path is **/bin/bash**, which is actually referencing **/home/student/roots/min/bin/bash**.

Once we're in the chroot, we'll list the files in the root directory with **ls** and try to use **cat**.

```
student@container:~$ sudo chroot /home/student/roots/min/ /bin/bash

bash-5.1  # ls -alh
total 20K
drwxr-xr-x 5 0 0 4.0K Mar 24 23:41 .
drwxr-xr-x 5 0 0 4.0K Mar 24 23:41 ..
drwxr-xr-x 2 0 0 4.0K Mar 24 23:41 bin
drwxr-xr-x 2 0 0 4.0K Mar 25 19:49 lib
drwxr-xr-x 2 0 0 4.0K Mar 24 23:41 lib64
-rw-r--r-- 1 0 0   15 Mar 24 23:41 secret

bash-5.1# cat /secret
bash: cat: command not found

bash-5.1  # exit
exit

student@container:~$
```

> Listing 5 - Running chroot in min directory

We were able to list the directory, but the cat command failed. This is because the new root doesn't have the cat binary. Let's copy it over and try again.

We'll use **cp** to copy **/usr/bin/cat** to **/home/student/roots/min/bin/**. Once that's complete, we'll **chroot** back into the directory and try again.

```
student@container:~$ sudo cp /usr/bin/cat /home/student/roots/min/bin/

student@container:~$ sudo chroot /home/student/roots/min/ /bin/bash

bash-5.1# cat /secret
hello, student

bash-5.1  # exit
exit

student@container:~$ 
```

> Listing 6 - Copying in cat and executing it

This time, the command worked.

As we discussed, a process in a chroot can still make privileged kernel calls unless other controls are in place. It's difficult to make those calls in a minimal environment with almost no binaries. While the minimal access to binaries is a security benefit to chroot, this is only security through obfuscation as an attacker could transfer new binaries in. To demonstrate that we can run kernel system calls, let's switch to the **bionic** directory as the new chroot.

```
student@container:~$ sudo chroot /home/student/roots/bionic/ /bin/bash
[sudo] password for student: 

root@container:/# ifconfig 
...

ens192: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.121.131  netmask 255.255.255.0  broadcast 192.168.121.255
        ether 00:50:56:8a:d5:32  txqueuelen 1000  (Ethernet)

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        loop  txqueuelen 1000  (Local Loopback)

root@container:/# 
```

> Listing 7 - chroot into bionic and run ifconfig

We were able to obtain the IP of the host. This might not seem like much, but being able to run ifconfig and find the host IP shows that we can make kernel calls. It's important to note that we can also find the hostname, which is another sign that we can make syscalls to the kernel.

One key difference between modern containers and chroot is the ability to analyze other processes. Let's try to list running processes with **ps**. We'll use the **a** argument to show processes for all users and **u** to display the processes' owner.

```
root@container:/# ps au
Error, do this: mount -t proc proc /proc
```

> Listing 8 - Running ps in chroot - Error

Unfortunately, we received an error because **/proc** isn't mounted. The **proc** directory contains information about processes. Fortunately, the error displays the exact command we need to fix the issue. Let's **mount** **/proc** and try again.

```
root@container:/# mount -t proc proc /proc

root@container:/# ps au
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         714  0.0  0.1   7352  1832 ?        Ss+  Mar22   0:00 /sbin/agetty -o -p -- \u --keep-baud 115200,38400,9600 ttyS0 vt220
root         724  0.0  0.1   5828  1564 ?        Ss+  Mar22   0:00 /sbin/agetty -o -p -- \u --noclear tty1 linux
1001       42352  0.0  0.5  10296  5500 ?        Ss   17:58   0:00 -bash
root       48086  0.0  0.4  11024  4676 ?        S    20:51   0:00 sudo chroot /home/student/roots/bionic/ /bin/bash
root       48087  0.0  0.3  20252  3780 ?        S    20:51   0:00 /bin/bash
root       48096  0.0  0.3  36144  3348 ?        R+   20:53   0:00 ps au
```

> Listing 9 - Running ps in chroot - Successfully

This time, we'll find the process that started the chroot. Since we can query for this process, it should immediately signal to us that we are not in a true sandbox.

While chroot did provide some form of segmentation, it is not perfect. To address many of these concerns, the concept of _Namespaces_ was introduced to enable separating the network, process, and other namespaces per process.

#### Labs

1. True or False: chroot alone is a strong security control.

Answer

2. True or False: A binary running in a chroot can still make privileged kernel calls.

Answer

3. Which directory needs to be mounted in a chroot in order to interact with other processes?

Answer

## 2.1.3. Linux Namespaces

Linux allows for processes to be partitioned based on various kernel features. While we'll discuss the technical details of these namespaces, let's step out of the technology world for a moment and instead think about how a city is designed. This will help us understand how namespaces work.

A city can contain various types of areas including parks, houses, and businesses. There is usually a central location for water treatment, power generation, internet connectivity, etc. The fiber for internet, wires for power, and plumbing for water treatment are all connected to the individual houses. The inhabitants of these houses all have different jobs: some work for the power company, some work for the water treatment plant, and some are morally compromised and trying to undermine the system.

While the infrastructure is shared, an individual house does not know how much power their neighbor is using, what sites they are visiting, or what they ate last night. However, there are also public parts of the infrastructure, like parks, where all inhabitants can find exactly where the power is going. If the morally-compromised inhabitant wants information about the individuals, they would have to compromise the power plant, internet provider, or water treatment plant.

If we think of a Linux host as the city, we can think of the houses and its inhabitants as individual processes. The electricity, plumbing, and internet connectivity are the namespaces, segmented based on the home.

As of Linux 5.6, there are eight different namespaces that a process can be segmented on: _mount_,[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-1) _process ID_ (PID),[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-2) _network_,[3](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-3) _interprocess communication_ (IPC),[4](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-4) _Unix time-sharing_ (UTS),[5](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-5) _user ID_,[6](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-6) _control group_ (cgroup),[7](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-7) and _time_.[8](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-8) There is also a proposed namespace for _syslog_.

Let's start by reviewing the PID namespace. To understand the PID namespace, we first need to understand that when a process is started in Linux, it is assigned a unique process ID. Each process, except for the first, has a parent process that started it. The process ID is incremented for each new process. For example, the first process in our Ubuntu machine is _systemd_. Starting systemd will start subsequent processes, such as http or ssh. We can verify this using the **ps** command. We'll use the **a** argument to show processes for all users and **x** to display the processes not attached to the terminal. We'll pipe the output into **head**, displaying only the first two lines.

```
student@container:~$ ps ax | head -n 2
PID TTY      STAT   TIME COMMAND
  1 ?        Ss     0:04 /sbin/init

student@container:~$ ls -alh /sbin/init
lrwxrwxrwx 1 root root 20 Jan  9 21:56 /sbin/init -> /lib/systemd/systemd
```

> Listing 10 - Listing Processes

We'll find that the first process is indeed systemd (**/sbin/init** is a symlink to **/lib/systemd/systemd**). Let's now check the PID of our current shell. We'll do this by using the Unix variable **$$**, which stores the PID of the current process, and using the **echo** command to output it.

```
student@container:~$ echo $$
1292
```

> Listing 11 - Displaying the Current PID

The current PID for our bash shell is 1292, which we'll note for later. Next, let's run another bash shell in a new namespace. This time, we'll use the _unshare_ command. As the name implies, **unshare** will stop sharing any namespace we specify. To unshare the pid namespace, we'll use the **--pid** argument. We'll also need to instruct unshare to fork as a new process using **--fork**. Finally, we will specify the command **/bin/bash** to run at the end. For now, we'll run this with root. Later, we'll discuss how not to run as root.

```
student@container:~$ sudo unshare --fork --pid /bin/bash

root@container:/home/student# echo $$
1
```

> Listing 12 - Creating a new PID Namespace

When we run bash in a different PID namespace, our shell is running as the first process. This is to be expected, since it was the command we specified to run in the new namespace. While not completely isolated, we can consider this shell a containerized environment. However, we'll need to do much more to make it completely isolated.

Because our shell is the first process, it must mean that it cannot access the other processes, right? Let's test this theory by running **ps ax** within our "containerized" environment.

```
root@container:/home/student# ps ax
    PID TTY      STAT   TIME COMMAND
      1 ?        Ss     0:04 /sbin/init
      2 ?        S      0:00 [kthreadd]
      3 ?        I<     0:00 [rcu_gp]
...
```

> Listing 13 - Running PS in PID Namespace

This is an unexpected result! Why is bash listed as the first process, while ps still found systemd as the first process?

In Linux, nearly everything can be classified as a file, including the processes that were started. The files that contain information about the process, among other things, are located in the **/proc** directory. The ps command is referencing the original PID namespace located in **/proc**.

To further isolate the process, we'll need to create a new mount namespace so that we can mount an empty **/proc** directory. It's important to note that **/proc** is not a normal filesystem and is instead a "virtual filesystem" because the files themselves don't contain the data. Instead, these files represent kernel and configuration information that is available to the user space.

Let's exit our current unshare process and start unshare in a new mount namespace. We'll add **--mount** to start bash in a new mount namespace.

```
root@container:/home/student# exit
exit

student@container:~$ sudo unshare --fork --pid --mount /bin/bash

root@container:/home/student# 
```

> Listing 14 - Starting in a new Mount NameSpace

With our own mount namespace, we can now mount our own **/proc**. We'll use the **mount** command for this and set the **-t** (or **--types**) flag to **proc** to indicate that we're trying to mount a proc filesystem. Next, we'll provide the source **none**, since no physical disk partition needs to be linked, and finally we'll provide the directory we want mounted, which is **/proc**. We'll also run **ps aux** before and after the mount to show the difference.

```
root@container:/home/student# ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  1.1 103196 11320 ?        Ss   May10   0:08 /sbin/init
root           2  0.0  0.0      0     0 ?        S    May10   0:00 [kthreadd]
root           3  0.0  0.0      0     0 ?        I<   May10   0:00 [rcu_gp]
...
root@container:/home/student# mount -t proc none /proc

root@container:/home/student# ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.3   8960  3956 pts/1    S    10:20   0:00 /bin/bash
root           9  0.0  0.3  10612  3336 pts/1    R+   10:20   0:00 ps aux
```

> Listing 15 - Mounting New /proc

Before the mount, we were finding the host's **/proc** filesystem. Since we'll find **/sbin/init** as the first entry, we know this is the systemd process. After we mount the new **/proc** filesystem and run **ps aux**, we'll find that our new system is limited to only the processes in our isolated PID namespace.

In our unshare shell, let's mount a new _tmpfs_ filesystem to **/mnt/our_new_tmp**. We'll first create the directory, then mount it and write a file to it. We'll also use **findmnt** to list the mounts to confirm it was created. Later, we'll check the host and confirm that we are indeed in our own mount namespace.

```
root@container:/home/student# mkdir /mnt/our_new_tmp

root@container:/home/student# mount -t tmpfs none /mnt/our_new_tmp

root@container:/home/student# echo "Hello!" > /mnt/our_new_tmp/offsec

root@container:/home/student# findmnt  | grep our_new_tmp
  └─/mnt/our_new_tmp                  none             tmpfs      rw,relatime,uid=1001,gid=1001
```

> Listing 16 - Creating a New tmpfs

Now let's start a new SSH session to the host and try to find this mount point. We'll start a new session to keep this unshare session running in the background.

```
student@kali:~$ ssh student@containers
student@containers's password: 
...

student@container:~$ findmnt  | grep our_new_tmp

student@container:~$ ls -alh /mnt/our_new_tmp/
total 8.0K
drwxr-xr-x 2 root root 4.0K May 11 14:38 .
drwxr-xr-x 3 root root 4.0K May 11 14:38 ..
```

> Listing 17 - No tmpfs Found on Host

After searching with findmnt and listing **/mnt/our_new_tmp/**, we do not find our file. This is exactly what the separate mount namespace was meant to do!

However, we originally had access to the host's **/proc** filesystem and now, we also have access to the host's **/tmp** mount. In fact, all the files we are currently interacting with are mounts from the host. Even in a different namespace, we have access to these files because a mount point is shared with new namespaces by default unless specifically directed otherwise. Using this information, there _are_ methods to escape our isolated mount environment, but they are outside the scope of this Learning Module.

Container runtimes use namespaces to isolate parts of the system as well as other restrictions to prevent a malicious process from accessing the host's filesystem. Mounts are a very commonly used method to escape a container. We should keep this in mind as we're building secure systems and attacking insecure ones.

Since we have direct access to the underlying system, one obvious escape out of our environment would be to rewrite a file on the host. Let's address this by setting up a new root directory. We'll start by exiting out of our current unshare environment. Let's run the **unshare** command again, but since our command is getting long, we'll shorten the long form of **--fork**, **--pid**, and **--mount** to **-f**, **-p**, and **-m**, respectively. We'll also add **--root** (or **-R**) and specify a new root location. Because we already have an Ubuntu 18.04 root filesystem in **/home/student/roots/bionic**, we'll use that. Finally, let's also add **--mount-proc**, which will automatically mount **/proc** for us.

```
root@container:/# exit
exit

student@container:~$ sudo unshare -fpm --root /home/student/roots/bionic --mount-proc /bin/bash

root@container:/# cat /etc/os-release 
NAME="Ubuntu"
VERSION="18.04 LTS (Bionic Beaver)"
...
```

> Listing 18 - Using the New Root FS with Unshare

Now we are running in new PID namespace, new mount namespace, and in an isolated directory from the host filesystem.

The _root_ argument was not always available in the unshare binary. Unshare uses the chroot system call to change the root.[9](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-9) This is not ideal because chroot only applies to one process, resetting the reference to **/**. By default, the popular container runtimes use _pivot_root_ instead of chroot. Unlike chroot, pivot_root replaces the entire **/** file system in the mount namespace instead of changing the root directory, making it more secure than chroot. However, for the sake of demonstration, using pivot_root is not necessary.

If we review the bash prompt, we'll find that our isolated environment still recognizes that we are in a host with the hostname of "container". While this might not seem like a big deal, it means that we are sharing the same _Unix Time Sharing_ (UTS) namespace as the host. Despite what its name implies, UTS is used to store the hostname.

To create a new UTS namespace, we can use **--uts** (or **-u**). By default, the hostname will be copied from the host and bash will be executed immediately. This means bash will automatically populate the hostname in the prompt. Although this doesn't break anything, for the sake of clarity, we'll switch to _sh_ instead of _bash_.

```
student@container:~$ sudo unshare -fpmu --root /home/student/roots/bionic --mount-proc /bin/sh

# hostname 
container

# hostname offsec

# hostname
offsec

# exit

student@container:~$ hostname
container
```

> Listing 19 - Running a New UTS Namespace

Now when we change the hostname in our isolated environment, our host's hostname does not change. The next namespace we want to isolate is the network namespace. We can confirm we are sharing the same network namespace as the host by running **ifconfig**. We'll also switch back to using bash.

```
student@container:~$ sudo unshare -fpmu --root /home/student/roots/bionic --mount-proc /bin/bash

root@container:/# ifconfig 
...
ens192: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.121.131  netmask 255.255.255.0  broadcast 192.168.121.255
        inet6 fe80::250:56ff:fe8a:d532  prefixlen 64  scopeid 0x20<link>
        ether 00:50:56:8a:d5:32  txqueuelen 1000  (Ethernet)
        RX packets 341  bytes 40422 (40.4 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 287  bytes 38121 (38.1 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
...
```

> Listing 20 - Running ifconfig in Isolated Environment

Even though we're running a new UTS namespace, the environment still indicates that we're using the "container" hostname. This occurs because it is copied over from the host each time. We'll ignore this since we know we're still under a new UTS namespace.

We'll find that we are receiving the host IP. Let's isolate our environment from the network namespace by using use the **-n** (or **--net**) argument to run a new network namespace.

```
student@container:~$ sudo unshare -fpmun --root /home/student/roots/bionic --mount-proc /bin/bash

root@container:/# ifconfig -a 
lo: flags=8<LOOPBACK>  mtu 65536
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```

> Listing 21 - Running a new Network Namespace

Now when we run ifconfig, we only get the loopback. While this does mean we don't have networking anymore, we could add networking by creating a _tap_ device[10](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_599-10) and assigning it to the process.

The last namespace that we will demonstrate is the user namespace, which allows us to segment the user IDs from the subprocess and the parent process. Essentially, this allows a process to appear like it's running as one user, when behind the scenes it's running as another user.

This is useful, for example, to make a process appear it has root permissions without running the process as root on the underlying host.

![[OffSec/Cloud/Cloud Administration Skill Path/z. images/bbfb0385b2ed4654ab5c9232afa3cd1f_MD5.jpg]]

Figure 1: User Namespace Mapping

In a custom user namespace, we no longer have to use sudo with unshare. We can create a new user namespace with the **U** (or **--user**) argument.

```
student@container:~$ unshare -fpmunU --root /home/student/roots/bionic --mount-proc /bin/bash

nobody@container:/$ id
uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)
```

> Listing 22 - Creating a New User Namespace

Although we are in a new user namespace, we have a problem. When the namespace was created, no user was mapped to the user running the process. Because of this, our system handles the process as an undefined user.

To fix this, we need to map a user to the root user in the process. Since we're running unshare as _student_, we'll map the root user for the process as the student user. With unshare, this can be done with the **-r** or (**--map-root-user**) argument.

```
student@container:~$ unshare -fpmunUr --root /home/student/roots/bionic --mount-proc /bin/bash

root@container:/# cat /proc/self/uid_map
         0       1001          1
```

> Listing 23 - Mapping student to root

When we add the argument to map the user, we get a "root" shell. However, we didn't run the command with sudo. Wouldn't this cause a vulnerability since we can now read and write files as the root user?

If we check the contents of **/proc/self/uid_map**, we'll find that the user ID running this process (0 or root) is mapped to 1001, or student. This essentially means that while our userid is 0 in this process, we only have the same permissions as as the student user.

Let's verify this by creating a file as the root user, exiting out of the shell, and reviewing the file as the student user.

```
root@container:/# touch /tmp/offsec

root@container:/# ls -alh /tmp/offsec
-rw-rw-r-- 1 root root 0 May 12 23:07 /tmp/offsec

root@container:/# exit
exit

student@container:~$ ls -alh /home/student/roots/bionic/tmp/offsec 
-rw-rw-r-- 1 student student 0 May 12 16:07 /home/student/roots/bionic/tmp/offsec
```

> Listing 24 - Verifying that the Mapping is Correct

Since we are running in a chroot, we'll need to find the file in **/home/student/roots/bionic/tmp/offsec** when we exit out of the shell. When we list the file in the process where we are running in a different user namespace, ls indicates that the file is owned by root. However, when we exit the process, we'll find that the file is indeed owned by student.

This means that files not readable by the student user are also not readable by the root user in the user namespace. Let's try to read a file as the root user in a container while using a different user namespace.

We'll first create a file called **secrets** in the **tmp** directory of the chroot. Then we'll change the permissions to be read-only by the owner and finally, change the owner to be the root user.

```
student@container:~$ echo "Super Secret" > /home/student/roots/bionic/tmp/secrets

student@container:~$ sudo chmod 400 /home/student/roots/bionic/tmp/secrets

student@container:~$ sudo chown root:root /home/student/roots/bionic/tmp/secrets

student@container:~$ ls -alh /home/student/roots/bionic/tmp/secrets
-r-------- 1 root root 13 May 12 16:11 /home/student/roots/bionic/tmp/secrets
```

> Listing 25 - Creating File Owned by Root

Now, let's attempt to read the file in an isolated user namespace.

```
student@container:~$ unshare -fpmunUr --root /home/student/roots/bionic --mount-proc /bin/bash

root@container:/# ls -alh /tmp/
total 12K
drwxrwxrwt  2 nobody nogroup 4.0K May 12 23:11 .
drwxr-xr-x 22 nobody nogroup 4.0K May 10 22:02 ..
-rw-rw-r--  1 root   root       0 May 12 23:07 offsec
-r--------  1 nobody nogroup   13 May 12 23:11 secrets

root@container:/# cat /tmp/secrets 
cat: /tmp/secrets: Permission denied
```

> Listing 26 - Attempting to Read Root Owned File

Although we are the "root" user, we cannot take action as a real root user because the unshare changed the user and group bits of the file.

While the user namespace is a very useful isolation technique, it's important to note that Docker does not use user namespaces by default. However, Podman does. We can verify this by running both **podman** and **docker** and reading the contents of **/proc/self/uid_map** to check the mappings.

```
student@container:~$ docker container run --rm centos:7 cat /proc/self/uid_map
         0          0 4294967295

student@container:~$ podman container run --rm centos:7 cat /proc/self/uid_map
         0       1001          1
         1     165536      65536
```

> Listing 27 - Checking User Mapping in Docker and Podman

When we cat the file in Docker, we'll find the default mapping without the use of a user namespace. Alternatively, in Podman, we'll find that the root user (0) is mapped to the student user (1001).

While namespaces represent a massive step forward in segmenting a process, they are not the complete solution. For example, we can segment a process into all the different namespaces, and the process can still make privileged calls to the kernel. Our isolated process can still run the **reboot** command.

```
student@container:~$ sudo unshare -fpmun --mount-proc /bin/bash

root@container:/home/student# reboot 
Failed to connect to bus: No data available

root@container:/home/student# Connection to containers closed by remote host.
Connection to containers closed.

```

> Listing 28 - Running Reboot in Namespaced Environment

It's important to note that we had to remove the user namespace because the user we would be running as student does not have permission to run the reboot command. We also had to remove the chroot, since the **bionic** directory doesn't have the reboot binary.

However, Docker doesn't use user namespaces by default. To prevent privileged kernel system calls, it limits access using Linux kernel capabilities and a seccomp profile. Let's investigate these capabilities next.

1

(Michael Kerrisk, 2021), [https://man7.org/linux/man-pages/man7/mount_namespaces.7.html](https://man7.org/linux/man-pages/man7/mount_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-1)

2

(Michael Kerrisk, 2020), [https://man7.org/linux/man-pages/man7/pid_namespaces.7.html](https://man7.org/linux/man-pages/man7/pid_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-2)

3

(Michael Kerrisk, 2020), [https://man7.org/linux/man-pages/man7/network_namespaces.7.html](https://man7.org/linux/man-pages/man7/network_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-3)

4

(Michael Kerrisk, 2019), [https://man7.org/linux/man-pages/man7/ipc_namespaces.7.html](https://man7.org/linux/man-pages/man7/ipc_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-4)

5

(Michael Kerrisk, 2019), [https://man7.org/linux/man-pages/man7/uts_namespaces.7.html](https://man7.org/linux/man-pages/man7/uts_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-5)

6

(Michael Kerrisk, 2021), [https://man7.org/linux/man-pages/man7/user_namespaces.7.html](https://man7.org/linux/man-pages/man7/user_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-6)

7

(Michael Kerrisk, 2020), [https://man7.org/linux/man-pages/man7/cgroup_namespaces.7.html](https://man7.org/linux/man-pages/man7/cgroup_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-7)

8

(Michael Kerrisk, 2021), [https://man7.org/linux/man-pages/man7/time_namespaces.7.html](https://man7.org/linux/man-pages/man7/time_namespaces.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-8)

9

(GitHub, 2022), [https://github.com/util-linux/util-linux/blob/986b80d0b2f30684f3bc96e4ae310b1c3c3b5ab5/sys-utils/unshare.c#L1071](https://github.com/util-linux/util-linux/blob/986b80d0b2f30684f3bc96e4ae310b1c3c3b5ab5/sys-utils/unshare.c#L1071) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-9)

10

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/TUN/TAP](https://en.wikipedia.org/wiki/TUN/TAP) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_599-10)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

containers

#### Labs

1. Run the binary found in **/home/student/exercises/namespace_client** in a Docker container with the default Docker namespaces. If run correctly, you will get a message indicating you were successful and a flag will be displayed.

Answer

2. Run the binary found in **/home/student/exercises/namespace_client** in a Docker container with the default Docker namespaces but without a network namespace. If run correctly, you will get a message indicating you were successful and a flag will be displayed.

Answer

3. Run the binary found in **/home/student/exercises/namespace_client** in a Podman container with the default Podman namespaces but without a network namespace. If run correctly, you will get a message indicating you were successful and a flag will be displayed.

Answer

4. Run the binary found in **/home/student/exercises/namespace_client** in a Podman container with the default Podman namespaces plus a user namespace. If run correctly, you will get a message indicating you were successful and a flag will be displayed.

Answer

5. Use **unshare** to run the binary found in **/home/student/exercises/namespace_client** in an isolated environment with a new mount, PID, and UTS namespace only. If run correctly, you will get a message indicating you were successful and a flag will be displayed. You must use the "fork before launching" option (-f) in unshare to run the binary with unshare.

Answer

6. Use **unshare** to run the binary found in **/home/student/exercises/namespace_client** in an isolated environment with a new mount, PID, IPC, and UTS namespace only. If run correctly, you will get a message indicating you were successful and a flag will be displayed. You must use the "fork before launching" option (-f) in unshare to run the binary with unshare.

Answer

7. Use **unshare** to run the binary found in **/home/student/exercises/namespace_client** in an isolated environment with a new custom cgroup, mnt, pid, user, and UTS namespace only. If run correctly, you will get a message indicating you were successful and a flag will be displayed. You must use the "fork before launching" option (-f) in unshare to run the binary with unshare.

Answer

8. Use **unshare** to run the binary found in **/home/student/exercises/namespace_client** in an isolated environment with a new custom time namespace only. If run correctly, you will get a message indicating you were successful and a flag will be displayed. You must use the "fork before launching" option (-f) in unshare to run the binary with unshare.

Answer

## 2.1.4. Linux Capabilities

Traditionally, we think of Linux as having privileged or unprivileged processes. While an unprivileged process with the right user permissions can write files to the system, list directories, check the date and time, and much more, a privileged process can do just about anything. In addition, an unprivileged process cannot, for example, edit files owned and restricted by other users, or load modules into the kernel. This level of permission is much too broad as a user might need to run a very specific task, but does not otherwise need full root access. An example of this is an unprivileged user wanting to update the time of their host, which requires privileged access.

For these reasons, Linux has organized some system-level tasks into just over 40 categories known as _capabilities_.[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_608-1) Using these capabilities, we can allow an unprivileged process to run a very specific system-level task without giving it full privileged access.

Let's use _Nmap_[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_608-2) as an example. While some Nmap scans do not require root access, others do. This is because the kernel limits a process's ability to create raw sockets with custom network packets. We can observe this restriction by trying to run a scan with Nmap.

We'll first run a regular TCP connect Nmap scan. In this scan, Nmap instructs the kernel to make a TCP connection. This type of scan does not require additional privileges, which is why curl, Firefox, and other network clients can make network connections without having to run as root.

```
student@container:~$ nmap 127.0.0.1 -sT
Starting Nmap 7.80 ( https://nmap.org ) at 2022-05-13 20:01 EDT
Nmap scan report for localhost (127.0.0.1)
Host is up (0.00016s latency).
Not shown: 998 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
8090/tcp open  opsmessaging

Nmap done: 1 IP address (1 host up) scanned in 0.06 seconds

```

> Listing 29 - Running TCP connect scan on Nmap as student

As expected, Nmap was able to complete the scan successfully. Next, let's attempt a TCP SYN scan by specifying the **-sS** flag. A TCP SYN scan only sends half of the TCP connection, making it much faster and quieter. However, the kernel does not support half-open TCP connections. Nmap gets around this by creating a raw socket with the target. The kernel requires elevated privileges for this.

```
student@container:~$ nmap 127.0.0.1 -sS
You requested a scan type which requires root privileges.
QUITTING!
```

> Listing 30 - Running TCP SYN scan on nmap as student - Failure

Let's run this scan as root to show that it does indeed work. We'll run the same command, this time using **sudo**.

```
student@container:~$ sudo nmap -sS 127.0.0.1
Starting Nmap 7.80 ( https://nmap.org ) at 2022-05-13 19:53 EDT
Nmap scan report for localhost (127.0.0.1)
Host is up (0.0000050s latency).
Not shown: 998 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
8090/tcp open  opsmessaging

Nmap done: 1 IP address (1 host up) scanned in 0.06 seconds

```

> Listing 31 - Running nmap as root

As expected, our command worked and we were able to scan localhost.

In some scenarios, we might want an unprivileged user to be able to run a command that requires the ability to make raw sockets. A traditional fix for this is configuring the binary to have the _setuid permission_.[3](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_608-3) Setuid will run the binary as the owner of the file, which is root in the case of Nmap.

Let's make a copy of the Nmap binary and set the setuid permission. We'll first find the location of the binary by using **which** and list its permissions with **ls -alh**. Next, we'll copy **/usr/bin/nmap** to a new file and name it **/usr/bin/nmap-setuid**.

```
student@container:~$ which nmap
/usr/bin/nmap

student@container:~$ ls -alh /usr/bin/nmap
-rwxr-xr-x 1 root root 2.9M Mar 23  2020 /usr/bin/nmap

student@container:~$ sudo cp /usr/bin/nmap /usr/bin/nmap-setuid
```

> Listing 32 - Copying the nmap binary

Once copied, we'll add the setuid permission with **chmod** and the argument **+s**. Finally, we'll check the permission to confirm the change and run a TCP SYN scan without using root.

```
student@container:~$ sudo chmod +s /usr/bin/nmap-setuid 

student@container:~$ ls -alh /usr/bin/nmap-setuid 
-rwsr-sr-x 1 root root 2.9M May 13 20:12 /usr/bin/nmap-setuid

student@container:~$ nmap-setuid 127.0.0.1 -sS
Starting Nmap 7.80 ( https://nmap.org ) at 2022-05-13 20:13 EDT
WARNING: Running Nmap setuid, as you are doing, is a major security risk.
WARNING: Running Nmap setgid, as you are doing, is a major security risk.

Nmap scan report for localhost (127.0.0.1)
Host is up (0.00037s latency).
Not shown: 998 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
8090/tcp open  opsmessaging

Nmap done: 1 IP address (1 host up) scanned in 0.06 seconds
```

> Listing 32 - Setting Setuid Permission on New Nmap Binary

While we've allowed a low privileged user to run a TCP SYN scan, we've also ensured that all users on this system can run nmap as root. This is very dangerous because now nmap might be used to escalate privileges.

There is another way to achieve a similar result. Instead of giving the user running the nmap binary full root access, we could specify only the privileged kernel capabilities that nmap requires.

We'll first copy the nmap binary to a new file so that we still have the original. Next, we'll set the capability using **setcap**. We'll need to provide the capability we want to set (**cap_net_raw**) and the capability set (**eip**). Exploring the capability set definition is out of scope for this Learning Module, but it _is_ important to know that it's required when using setcap. Finally, we'll run the nmap SYN scan, adding **--privileged** to indicate that we have set the capabilities of the binary.

```
student@container:~$ sudo cp /usr/bin/nmap /usr/bin/nmap-cap

student@container:~$ sudo setcap cap_net_raw+eip /usr/bin/nmap-cap 

student@container:~$ nmap-cap --privileged  127.0.0.1 -sS
Starting Nmap 7.80 ( https://nmap.org ) at 2022-06-02 18:37 UTC
Nmap scan report for localhost (127.0.0.1)
Host is up (0.0000060s latency).
Not shown: 998 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
8090/tcp open  opsmessaging
```

> Listing 33 - Setting Capabilities on Nmap

With the capability set, we are able to scan without having to run the binary as a root user. While this does expose the nmap binary more than with no capabilities, it is more secure than an over-privileged binary.

The CAP_NET_RAW capability is one of many. Below is a list of some other common capabilities:

- **CAP_CHOWN**: Allows a process to make changes to a file's owners
- **CAP_NET_ADMIN**: Performs administrative tasks on the network configuration (IP, firewall, etc.)
- **CAP_NET_BIND_SERVICE**: Allows binding of ports below 1024
- **CAP_NET_RAW**: Allows the ability to use RAW sockets
- **CAP_SYS_ADMIN**: The "root" privilege of capabilities, which can be dangerous because it is overloaded and enables a broad set of features
- **CAP_SYS_BOOT**: Allows the host to be rebooted
- **CAP_SYS_MODULE**: Loads and unloads kernel modules
- **CAP_SYS_TIME**: Allows a process to set the system clock
- **CAP_SYS_CHROOT**: Allows the use of chroot
- **CAP_AUDIT_WRITE**: Allows writing to the kernel auditing log

Modern containers use capabilities to add defensive layers and increase segmentation between the host and other containers. We can find Docker's default capabilities by reviewing the source code.[4](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_608-4)

```
func DefaultCapabilities() []string {
	return []string{
		"CAP_CHOWN",
		"CAP_DAC_OVERRIDE",
		"CAP_FSETID",
		"CAP_FOWNER",
		"CAP_MKNOD",
		"CAP_NET_RAW",
		"CAP_SETGID",
		"CAP_SETUID",
		"CAP_SETFCAP",
		"CAP_SETPCAP",
		"CAP_NET_BIND_SERVICE",
		"CAP_SYS_CHROOT",
		"CAP_KILL",
		"CAP_AUDIT_WRITE",
	}
}
```

> Listing 34 - Default Capabilities in Docker

Docker uses an allowlist approach to capabilities. By default, all capabilities are blocked, except for the 14 listed above. This approach is more secure and ensures a consistent environment.

Podman also has a default list of capabilities. However, it's a bit more restricted with only 11 enabled by default.

```
student@container:/home/student$ grep "default_capabilities"  /usr/share/containers/containers.conf -A 12
default_capabilities = [
  "CHOWN",
  "DAC_OVERRIDE",
  "FOWNER",
  "FSETID",
  "KILL",
  "NET_BIND_SERVICE",
  "SETFCAP",
  "SETGID",
  "SETPCAP",
  "SETUID",
  "SYS_CHROOT"
]
```

> Listing 35 - Default Capabilities in Podman

One of the capabilities available in Docker but not Podman is _CAP_NET_RAW_, which we reviewed earlier. Let's try to run **nmap** in both container engines and examine the differences.

```
student@container:/home/student$ docker run --rm -it instrumentisto/nmap 127.0.0.1 -sS
Starting Nmap 7.92 ( https://nmap.org ) at 2022-05-16 21:09 UTC
Nmap scan report for localhost (127.0.0.1)
Host is up (0.0000060s latency).
All 1000 scanned ports on localhost (127.0.0.1) are in ignored states.
Not shown: 1000 closed tcp ports (reset)

Nmap done: 1 IP address (1 host up) scanned in 0.15 seconds

student@container:/home/student$ podman run --rm -it instrumentisto/nmap 127.0.0.1 -sS
Starting Nmap 7.92 ( https://nmap.org ) at 2022-05-16 21:09 UTC
Couldn't open a raw socket. Error: Operation not permitted (1)
```

> Listing 36 - Podman not Allowing Nmap

While Docker was able to scan localhost, Podman was not. This is because Podman is missing the required capability.

Both Podman and Docker allow the user running the container to control capabilities using the **--cap-add** flag to add a capability, **--cap-drop** to remove a capability, and **--privileged** to remove all capability filtering from the container. Let's try this out by removing the **CAP_NET_RAW** capability on Docker and adding it to Podman.

```
student@container:/home/student$ docker run --cap-drop=CAP_NET_RAW --rm -it instrumentisto/nmap 127.0.0.1 -sS
Starting Nmap 7.92 ( https://nmap.org ) at 2022-05-16 21:12 UTC
Couldn't open a raw socket. Error: Operation not permitted (1)

student@container:/home/student$ podman run --cap-add=CAP_NET_RAW --rm -it instrumentisto/nmap 127.0.0.1 -sS
Starting Nmap 7.92 ( https://nmap.org ) at 2022-05-16 21:12 UTC
Nmap scan report for localhost (127.0.0.1)
Host is up (0.0000060s latency).
All 1000 scanned ports on localhost (127.0.0.1) are in ignored states.
Not shown: 1000 closed tcp ports (reset)

Nmap done: 1 IP address (1 host up) scanned in 0.14 seconds
```

> Listing 37 - Adding Capability to Podman and Removing Capability from Docker

Now Docker cannot scan localhost, but Podman can. We need to consider the balance between security and usability. Some capabilities, like CAP_NET_RAW, might not lead to a direct privilege escalation. Others, like CAP_SYS_MODULE, which allows loading of kernel modules, _can_ lead to higher privileges. Any time the capability set is expanded, the developer is exposing more of the underlying kernel and increasing the chance of privilege escalation.

Deciding which capabilities should and should not be enabled is a delicate choice.

1

(Michael Kerrisk, 2021), [https://man7.org/linux/man-pages/man7/capabilities.7.html](https://man7.org/linux/man-pages/man7/capabilities.7.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_608-1)

2

(Nmap, 2022), [https://nmap.org/](https://nmap.org/) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_608-2)

3

(Wikipedia, 2022), [https://en.wikipedia.org/wiki/Setuid](https://en.wikipedia.org/wiki/Setuid) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_608-3)

4

(Moby, 2019), [https://github.com/moby/moby/blob/1308a3a99faa13ff279dcb4eb5ad23aee3ab5cdb/oci/caps/defaults.go#L6-L19](https://github.com/moby/moby/blob/1308a3a99faa13ff279dcb4eb5ad23aee3ab5cdb/oci/caps/defaults.go#L6-L19) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_608-4)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

containers

#### Labs

1. The binary in **/home/student/exercises/open_socket** opens a raw socket. Set the appropriate capability on this binary to get the flag. When the capability is set and the binary is run, the flag should appear.

Answer

2. The binary in **/home/student/exercises/file_owner** runs the _chown_ syscall. Find and set the appropriate capability on this binary to get the flag. When the capability is set and the binary is run, the flag should appear.

Answer

3. The binary in **/home/student/exercises/file_perms** runs a syscall that changes the permissions of a file. Find and set the appropriate capability on this binary to get the flag. When the capability is set and the binary is run, the flag should appear.

Answer

4. The binary in **/home/student/exercises/unknown_cap** requires a capability. Loop through all capabilities on this binary to get the flag. When the capability is set and the binary is run, the flag should appear.

Answer

## 2.1.5. Secure Computing Mode (seccomp)

While namespaces and capabilities are very useful for isolating a process, additional layers of security are always beneficial. One such layer is the _secure computing mode_ (_seccomp_) feature of Linux.

Seccomp and capabilities have a lot of overlap. Similarly to how capabilities break down the broad "privileged" and "unprivileged" paradigm into more granular control, Seccomp takes the granularity to an even more precise level. With seccomp, we can limit a processes' ability to run any of the over 400 syscalls provided by the kernel.

For example, the _CAP_SYS_ADMIN_ capability allows a process to run many system calls including _mount_, _pivot_root_, _sethostname_, and more. For example, we can specify a policy with seccomp that will only block the _sethostname_ system call. This means that a process can run with the CAP_SYS_ADMIN capability, but is further limited by a seccomp policy that doesn't allow the process to change the hostname.

To understand how this is used in a containerized environment, let's review how Docker and Podman use seccomp.

At the time of this writing, there isn't an easy-to-use utility to apply a seccomp profile to a process like unshare for namespaces and setcap for capabilities. The methods to apply a seccomp profile manually are out of scope for this section.

Docker has a default seccomp profile that denies all syscalls and permits only those deemed necessary. The default profile can be found in the source code.[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_613-1) Podman also uses a variation of this same seccomp profile. Let's break down the Docker profile into sections.

```
{
	"defaultAction": "SCMP_ACT_ERRNO",
	"defaultErrnoRet": 1,
	"archMap": [
		{
			"architecture": "SCMP_ARCH_X86_64",
			"subArchitectures": [
				"SCMP_ARCH_X86",
				"SCMP_ARCH_X32"
			]
		},
...
```

> Listing 38 - Seccomp Profile - defaultAction

First, the seccomp profile defines the default action, which is _SCMP_ACT_ERRNO_. This will provide a "Permission Denied" error when attempting run a syscall that is not in the default list. After the default action, an _archMap_ variable is set. Different architectures have different syscalls, which is why it's important to differentiate between them. However, this section is of minimal interest to us.

On lines 57-408, we can find a long list of syscalls that are permitted.

```
	"syscalls": [
		{
			"names": [
				"accept",
				"accept4",
				"access",
				"adjtimex",
				"alarm",
				"bind",
...
```

> Listing 39 - Seccomp Profile - syscalls

This section shows which syscalls a container is allowed to make.

The list provides over 300 syscalls that can be run and while this does reduce the attack surface significantly, it could be more secure. Most containers only need 40 to 70 syscalls to run.[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_613-2)

Both Podman and Docker allow for custom seccomp policies to be written and applied when a container is started. We can do this by adding the **--security-opt** argument and specifying **seccomp** to be equal to the path of the profile.

Let's try this out using a custom profile. We've created a copy of the default Docker seccomp profile, but removed _socket_. When we try to run our nmap container, it should fail because it cannot use the _socket_ syscall.

```
student@container:/home/student$ docker run --security-opt seccomp=/home/student/no_socket_profile.json --rm -it instrumentisto/nmap 127.0.0.1 -sS
Starting Nmap 7.92 ( https://nmap.org ) at 2022-05-16 23:59 UTC
route_dst_netlink: cannot create AF_NETLINK socket: Operation not permitted
```

> Listing 40 - Nmap cannot run Without Sockets Syscall

As expected, nmap failed to run. This is because our profile blocked it from using _socket_.

Seccomp provides a strong additional layer to ensure isolation within containers. While we've covered namespaces, capabilities, and seccomp, there are other options we have not yet explored that add to the layers of security. One option is Control Groups, which we'll examine next.

1

(Movy, 2022), [https://github.com/moby/moby/blob/7de9f4f82de417097f6fab150288ca2f1c0a9d91/profiles/seccomp/default.json](https://github.com/moby/moby/blob/7de9f4f82de417097f6fab150288ca2f1c0a9d91/profiles/seccomp/default.json) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_613-1)

2

(Aqua, 2018), [https://blog.aquasec.com/aqua-3.2-preventing-container-breakouts-with-dynamic-system-call-profiling](https://blog.aquasec.com/aqua-3.2-preventing-container-breakouts-with-dynamic-system-call-profiling) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_613-2)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

containers

#### Labs

1. According to the x86 syscall table for Linux 5.15 (https://raw.githubusercontent.com/torvalds/linux/v5.15/arch/x86/entry/syscalls/syscall_64.tbl), how many "common" and "64" bit syscalls are in Linux? This is not counting the ones labeled "x32".

Answer

2. Run **strace -cfS name /home/student/exercises/seccomp_test** to obtain a list of all syscalls used in the binary. The seccomp profile in **/home/student/exercises/seccomp_test.json** is missing one of the syscalls. Which syscall is missing in the profile?

Answer

3. Run the _picky_app_ container in Docker with the seccomp profile found in **/home/student/exercises/bad_profile.json**. This profile is not hardened and will need to be hardened to run the container. If you've successfully hardened the profile, a flag will appear.

Answer

## 2.1.6. Control Groups

Control groups (_cgroups_) allow the host resources (CPU, memory, disk access, etc.) to be limited and monitored. Instead of a process being able to use 100% of the CPU and 100% of the RAM, we can use cgroups to limit a process to a specific amount of each. Essentially, cgroups divide the host's resources among various processes.

Initially, this might not sound like a security feature when compared to namespaces, capabilities, and seccomp. However, the purpose of containers is to isolate a process from other processes. If one process is using 100% of the CPU and slowing down other processes, it might cause a denial of service against another process. This means that cgroups are also considered a security feature.

How cgroups are implemented in the kernel is a relatively complicated process. There is also a fairly low adoption rate of cgroups outside of containerization.[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_617-1) This means the way we interface with cgroups is not straightforward, which further lowers adoption. We also need to consider that version 1 and version 2 of cgroups exist.

In this section, we will focus on version 1 since it has a higher adoption rate at the time of this writing. While there are differences between the two, most of the concepts we cover will be valid to version 2. However, the directory organization might be different.

Let's work on limiting a process's resources manually. We'll start by reviewing a configuration that we created and process the configuration with the cgroups configuration parser, _cgconfigparser_.[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_617-2) The configuration parser will take the configuration file, parse it, and create all the necessary files on the filesystem. Next, we'll review a rules file that informs the system which files and users should be in the cgroup. We'll run _cgrulesengd_,[3](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_617-3) which will parse the rules file and apply the rules to new processes when they encounter the rule. Finally, we'll use benchmarking tools, _sysbench_[4](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_617-4) and _hdparm_,[5](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_617-5) to analyze the impact of the cgroups parser.

Let's start by reviewing the configuration file, **/etc/cgconfig.conf**.

```
student@container:/home/student$ cat /etc/cgconfig.conf 
group sysbench {
     cpu {
         cpu.cfs_quota_us=1000;
     }
     memory {
         memory.limit_in_bytes = 100m;
     }
}

group limitio{
        blkio {
                blkio.throttle.read_bps_device = "8:0         2097152";
        }
}
```

> Listing 41 - Reviewing the cgroups Configuration

In this configuration, we create two groups, _sysbench_ and _limitio_. The sysbench configuration uses two controllers, _cpu_ and _memory_. The _cfs_quota_us_ configuration is set to 1000. This configuration limits a process so it can only use 1% of the CPU. The memory limit is a bit more straightforward. This configuration limits a process to only 100 megabytes of RAM.

The _limitio_ has a single configuration for the _blkio_ controller, which controls the input/output of block devices (such as hard drives). The configuration limits the hard drive read speed to 2MB/s.

The numbers in the limitio group are using the Major and Minor[6](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_617-6) pairing of device[7](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_617-7) identification.

Now that we've reviewed the configuration, let's parse it. To do this, we'll use **cgconfigparser** with the **--load** argument and provide the **/etc/cgconfig.conf** file. We'll have to run this with **sudo** since it will be writing files to privileged directories. Once we run our command, we'll use **lscgroup** to list all the cgroups on the system and **grep** for the two we've created.

```
student@container:/home/student$ sudo cgconfigparser --load /etc/cgconfig.conf

student@container:/home/student$ lscgroup | grep -e "limitio" -e "sysbench"
cpu,cpuacct:/sysbench
memory:/sysbench
blkio:/limitio
```

> Listing 42 - Parsing the Configuration File

After running the parser, we'll find that three entries were made, as expected. The parser translated our configuration file into a set of files and directories that the kernel understands. These files can be found in the **/sys/fs/cgroup/*** directory.

```
student@container:/home/student$ cat /sys/fs/cgroup/memory/sysbench/memory.limit_in_bytes
104857600

student@container:/home/student$ cat /sys/fs/cgroup/cpu/sysbench/cpu.cfs_quota_us
1000

student@container:/home/student$ cat /sys/fs/cgroup/blkio/limitio/blkio.throttle.read_bps_device
8:0 2097152
```

> Listing 43 - Discovering Cgroup Config on File System

Within each file, we'll find the same setting that we had in our configuration.

Now that we've officially created the cgroups, we need something that will migrate processes into the cgroup restriction. For this, we'll use the control group rules daemon, _cgrulesengd_. We've created a rule file in **/etc/cgrules.conf**.

```
student@container:/home/student$ cat /etc/cgrules.conf
#<user>:<cmd>           <controllers>           <destination>
*:hdparm                blkio                   limitio/
*:sysbench              cpu,memory              sysbench/
```

> Listing 44 - Reviewing the Control Group Rules

We have two rules in this file. The syntax of the rules starts with the user to target. In both of the rules, there's a wildcard (*****) that represents any user. Next, we encounter the command we want to target. In the first rule, we're targeting the hdparm command, and in the second, the sysbench command. Next, we'll find the controllers that represent the cgroup limit we want to impose. For the first rule, it's blkio, and for sysbench, it's both CPU and memory. Finally, we have the destination, which represents the name of the control group to use. This has to match the groups created in **/etc/cgconfig.conf** or one that already exists.

In our case, the first rule will send the hdparm command into the limitio cgroup where it will be controlled by blkio. For the second, the sysbench command will be sent to the sysbench cgroup where the CPU and memory will be limited.

While it is meant to be run as a daemon, we will execute **cgrulesengd** in our terminal in order to review the logs. We'll start the rules engine with **--debug** to cover all the logs.

```
student@container:/home/student$ sudo cgrulesengd --debug
CGroup Rules Engine Daemon log started
Current time: Wed May 18 13:33:18 2022

Opened log file: -, log facility: 0,log level: 7
Proceeding with PID 58823
Rule: *:hdparm
  UID: any
  GID: any
  DEST: limitio/
  CONTROLLERS:
    blkio

Rule: *:sysbench
  UID: any
  GID: any
  DEST: sysbench/
  CONTROLLERS:
    cpu
    memory
...
```

> Listing 45 - Starting the Rules Engine

When we start the rules engine, we'll find that the rules are parsed and the service is started. Next, let's open a new SSH session and run the sysbench and hdparm commands. We'll use **sysbench** to stress test the **cpu** and verify that we are indeed limited. We'll use **hdparm** to check the read speed of the hard drive. To avoid any caches, we'll use the **--direct** flag. Because we want the speed, we'll use **-t** (for "timing"). Finally, we'll specify the device we're testing (**/dev/sda** in our case).

```
student@container:~$ sysbench cpu run
sysbench 1.0.20 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
...

CPU speed:
    events per second:     33.90
...

student@container:~$ sudo hdparm --direct -t /dev/sda
/dev/sda:
 Timing O_DIRECT disk reads:   6 MB in  3.01 seconds =   2.00 MB/sec
```

> Listing 46 - Running the Benchmarking tools

The events per second metrics might be different depending on the CPU of the underlying system running the VM.

The sysbench benchmark listed that we have 33.90 events per second; this is very low and to be expected when we only allow one percent of the CPU. As for hdparm, we'll find that we were indeed limited to 2 MB/sec of read speed. Let's go back to our rules engine and check the logs.

```
EXEC Event: PID = 58960, tGID = 58960
Scanned proc values are 1001 1001 1001 1001
Scanned proc values are 1001 1001 1001 1001
Found matching rule * for PID: 58960, UID: 1001, GID: 1001
Executing rule * for PID 58960... Will move pid 58960 to cgroup 'sysbench/'
Adding controller cpu
Adding controller memory
OK!
Cgroup change for PID: 58960, UID: 1001, GID: 1001, PROCNAME: /usr/bin/sysbench OK
...
EXEC Event: PID = 58968, tGID = 58968
Scanned proc values are 0 0 0 0
Scanned proc values are 0 0 0 0
Found matching rule * for PID: 58968, UID: 0, GID: 0
Executing rule * for PID 58968... Will move pid 58968 to cgroup 'limitio/'
Adding controller blkio
OK!
Cgroup change for PID: 58968, UID: 0, GID: 0, PROCNAME: /usr/sbin/hdparm OK
```

> Listing 47 - Logs from Rules Engine

In the logs, we'll find that the rules engine locates the processes and moves them to the appropriate cgroup. Next, let's stop the rules engine and run the same benchmarks to compare the differences.

```
student@container:~$ sysbench cpu run
sysbench 1.0.18 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
...
CPU speed:
    events per second:   646.39
...

student@container:~$ sudo hdparm --direct -t /dev/sda

 Timing O_DIRECT disk reads: 914 MB in  3.00 seconds = 304.54 MB/sec
```

> Listing 48 - Running the benchmark without cgroups

As expected, our CPU speed and hard drive read times are much faster.

While cgroups can be used to manage CPU, RAM, disk, and devices (such as hardware), opting to use cgroups for devices is unusual because this approach lacks the ability to limit resources or perform monitoring. However, it _does_ limit access to the devices.

Access to a device could potentially be used to escalate privileges. From a security perspective, the device cgroup is very interesting. For example, if a cgroup is misconfigured and allows access to all devices, a container would be able to access the host's hard drive and read and write files. However, exploiting this is outside the scope of this Learning Module.

Instead, let's investigate how to set cgroups in Docker and their output on the host system. We'll replicate the _cpu.cfs_quota_us_ setting from our configuration using the **--cpu-quota** argument.

We'll use **docker run** to create a new container, use **-d** to run the container in the background, **--rm** to delete the container after it closes, set **cpu-quota** to **1000**, use the **ubuntu** image, and **sleep** for **900** seconds (15 minutes).

```
student@container:~$ docker run -d --rm --cpu-quota=1000 ubuntu sleep 900
a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
```

> Listing 49 - Running Docker with CPU quota

In order to list all the cgroups applied to the container process, we need to obtain the PID of the container. We can do this by running **docker top** and passing in the ID of the container. The list of cgroups can be found in **/proc/<PID>/cgroup**.

```
student@container:~$ docker top a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
root                59338               59314               0                   19:57               ?                   00:00:00            sleep 900

student@container:~$ cat /proc/59338/cgroup 
12:rdma:/
11:devices:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
10:blkio:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
9:memory:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
8:freezer:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
7:cpu,cpuacct:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
6:pids:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
5:net_cls,net_prio:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
4:cpuset:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
3:perf_event:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
2:hugetlb:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
1:name=systemd:/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c
0::/system.slice/containerd.service
```

> Listing 50 - Obtaining PID of Docker Container

We'll find that the cgroup name also contains the container ID. Let's try to find the cgroup restriction we set in that directory. We already know that **/sys/fs/cgroup/cpu/** is the root directory for all CPU cgroups. From the previous listing, we know that the cgroup name for this container is **/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c/**. Let's search in that directory for **cfs_quota_us**.

```
student@container:~$ cat /sys/fs/cgroup/cpu/docker/a266e46735b71a729a1be19bf752c14b7be94ce11d6961e4e4ddc289c49e0f0c/cpu.cfs_quota_us
1000
```

> Listing 51 - Discovering the cfs_quota_us Value

As expected, we found the same value we set when we started the Docker container.

Cgroups are a very powerful feature that can prevent resource-intensive operations from affecting other containers on the same system. They are also an important pillar of what makes a container.

1

(Red Hat, 2020), [https://www.redhat.com/sysadmin/cgroups-part-one](https://www.redhat.com/sysadmin/cgroups-part-one) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_617-1)

2

(die.net, 2022), [https://linux.die.net/man/8/cgconfigparser](https://linux.die.net/man/8/cgconfigparser) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_617-2)

3

(die.net, 2022), [https://linux.die.net/man/8/cgrulesengd](https://linux.die.net/man/8/cgrulesengd) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_617-3)

4

(akopytov, 2021), [https://github.com/akopytov/sysbench](https://github.com/akopytov/sysbench) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_617-4)

5

(Michael Kerrisk, 2021), [https://man7.org/linux/man-pages/man8/hdparm.8.html](https://man7.org/linux/man-pages/man8/hdparm.8.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_617-5)

6

(Linux Kernel Organization, 2022), [https://www.kernel.org/doc/html/v4.11/admin-guide/devices.html](https://www.kernel.org/doc/html/v4.11/admin-guide/devices.html) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_617-6)

7

(Red Hat, 2022), [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-devices](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-devices) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_617-7)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

containers

#### Labs

1. Use Docker to run the _cgroup_check_ container and set the cpu shares to 1000 and the memory limit to 90mb. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

2. Use Docker to run the _cgroup_check_ container and set the CPU period to 2000 and the CPU quota to 1000. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

3. Use Docker to run the _cgroup_check_ container and limit the bytes per second read on the **/dev/sda** drive to 2mb and set the memory swappiness to 30. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

4. Use Podman to run the _cgroup_check_ container and limit the cpu period to 3000. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

5. Use Podman to run the _cgroup_check_ container and increase the memory swappiness to 80 and set the _soft_ memory limit to 50mb. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

6. Use Podman to run the _cgroup_check_ container and set the bytes per second read speed to 2mb, the write speed (in bytes per second) to 1mb, cpu shares to 5, and the memory to 30mb. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

7. Use _cgrulesengd_ and _cgconfigparser_ to configure a rule to limit the **/home/student/exercises/cgroup_check** binary. Set the memory limit to 50mb and the device read speed (bytes per second) to 2mb. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

8. Use _cgrulesengd_ and _cgconfigparser_ to configure a rule to limit the **/home/student/exercises/cgroup_check** binary. Set the cpu quota to 6000 and the device write speed (bytes per second) to 2mb. Leave the rest of the cgroup configurations default. A successful configuration will display the flag.

Answer

## 2.2. Open Container Initiative

This Learning Unit covers the following Learning Objectives:

- Understand the image specification
- Learn about the runtime specification
- Understand the distribution specification

A major reason we can use so many tools to accomplish the same thing is because containerization has been standardized under the _Open Container Initiative_ (OCI). This standardization originated with Docker but has since moved under the Linux foundation. Rather than competing between Docker, Podman, Kubernetes, Buildah, and others, this standardization enhances their compatibility.

There are three components to the OCI: the image specification, runtime specification, and distribution specification.

Each learner moves at their own pace, but this Learning Unit should take approximately 90 minutes to complete.

## 2.2.1. OCI Image Specification

When we pull an image from the registry, we are downloading a tarball that is extracted and used as the root file system for a container. However, containers are much more than a flat filesystem of the image.

In this section, we will use _skopeo_[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_627-1) to review a container image and understand the OCI _image specification_.[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_627-2) We could simply review the image specification documentation, but specifications are usually dry and boring. Let's manually unpack an image so we can learn about the specification in a hands-on way.

It's important to note that the OCI image specification was based off the Docker V2 image manifest. Images downloaded from the Docker registry will typically follow the Docker V2 image manifest. While we won't be converting images in this section, using tools like skopeo, we can convert a Docker image into an OCI image.

In Podman, we've created an image called "offsec". The image was built from the **Dockerfile** found in **/home/student/offsec/**. Let's review the file and run the container.

```
student@container:/home/student$ ls -alh /home/student/offsec/
total 16K
drwxrwxr-x  2 student student 4.0K May 25 11:22 .
drwxr-xr-x 11 student student 4.0K May 25 11:12 ..
-rw-r--r--  1 student student  135 May 25 11:22 Dockerfile
-rw-r--r--  1 student student   15 May 25 11:12 secret

student@container:/home/student$ cat /home/student/offsec/Dockerfile 
FROM alpine@sha256:4ff3ca91275773af45cb4b0834e12b7eb47d1c18f770a0b151381cd227f4c253

COPY secret /secret

ENTRYPOINT ["cat", "/secret"]

student@container:/home/student$ podman run -it --rm offsec
Hack the cloud!
```

> Listing 52 - Reading the offsec Image Dockerfile

The directory contains two files, **Dockerfile** and **secret**. The **Dockerfile** imports a specific Alpine image (the hash references an exact version), copies the secret file, and sets the entrypoint to cat out the secret file. When we run the image, we get the contents of the secret file.

Next, let's save this container image so that we can review its contents. To save the image, we'll use **podman** and the **save** command. We'll output the image to the **offsec-oci** directory specified by the **-o** flag. Next, we'll specify that we want the directory formatted using the OCI standard with the **--format oci-dir** argument. Finally, we'll specify the image **offsec**.

```
student@container:/home/student$ podman save -o offsec-oci --format oci-dir offsec
Copying blob 24302eb7d908 done  
Copying blob 1e7669f5f765 done  
Copying config b8f347291e done  
Writing manifest to image destination
Storing signatures
```

> Listing 53 - Saving the offsec Image in OCI Format

With the image saved to an easily readable location, let's inspect it. First we'll use **skopeo** and the **inspect** command to list some information about the image. We'll designate the location of the directory and instruct skopeo to read it with the OCI transport by specifying **oci:///home/student/offsec-oci/**.

```
student@container:/home/student$ skopeo inspect oci:///home/student/offsec-oci/
{
    "Digest": "sha256:a1aa1c7a7821f9be28c43c2bc75edaad4127818582158070b1fb29c89bf88fd1",
    "RepoTags": [],
    "Created": "2022-05-25T18:23:54.025683481Z",
    "DockerVersion": "",
    "Labels": {
        "io.buildah.version": "1.23.1"
    },
    "Architecture": "amd64",
    "Os": "linux",
    "Layers": [
        "sha256:ff309671352dd8bb4f487772b942ab34acb26811d2ab2b6fa659a9ef84b11cb4",
        "sha256:a994d323375dbc6794a995b61336887591741ecd6e87bdf7db3e7aed430e9538"
    ],
    "Env": [
        "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
    ]
}
```

> Listing 54 - Inspecting Image with Skopeo

The output reveals several interesting pieces of information. First, we'll find that the image was built with buildah via Podman. Next, we find this image contains an array of layers. While this information is useful, we'll need to further investigate to understand OCI's image specification.

Skopeo was able to obtain this information because the specification defines where the information should be found. Let's list the contents of the image using the **tree** command.

```
student@container:/home/student$ tree /home/student/offsec-oci/
/home/student/offsec-oci/
├── blobs
│   └── sha256
│       ├── a1aa1c7a7821f9be28c43c2bc75edaad4127818582158070b1fb29c89bf88fd1
│       ├── a994d323375dbc6794a995b61336887591741ecd6e87bdf7db3e7aed430e9538
│       ├── b8f347291eb4767580fc3a31981aa4fcc74bb9e6d5fe118ab7bab90321f4f4a4
│       └── ff309671352dd8bb4f487772b942ab34acb26811d2ab2b6fa659a9ef84b11cb4
├── index.json
└── oci-layout

2 directories, 6 files
```

> Listing 55 - Running tree on offsec-oci

To understand the contents of this directory, we need to understand the difference between location-addressable storage and content-addressable storage.

We are all familiar with the filesystem in Linux and Windows where we have a root directory with multiple sub-directories. Any one of those directories might contain more sub-directories or files. This is called location-addressable content. In location-addressable content, for example, the **passwd** file is located in the **etc** directory, thus its location is **/etc/passwd**.

In contrast, content-addressable storage contains a reference of all the content and a unique value to reference that content. This is what we are finding in the listing above. The actual files are stored in the **blobs** directory and the unique IDs are the hashes we'll find in the **sha256** directory. As we'll find, the name and type of these files is found in **index.json** and the subsequent files.

Within the **/home/student/offsec-oci/** directory, we also find **index.json** and **oci-layout** files. This structure is standard to the OCI specification.

Let's investigate these two files. According to the OCI specification, both must be JSON objects, so we'll pipe them into **jq** for better formatting.

```
student@container:/home/student$ cat offsec-oci/oci-layout | jq
{
  "imageLayoutVersion": "1.0.0"
}

student@container:/home/student$ cat offsec-oci/index.json | jq
{
  "schemaVersion": 2,
  "manifests": [
    {
      "mediaType": "application/vnd.oci.image.manifest.v1+json",
      "digest": "sha256:a1aa1c7a7821f9be28c43c2bc75edaad4127818582158070b1fb29c89bf88fd1",
      "size": 503,
      "annotations": {
        "org.opencontainers.image.ref.name": "localhost/offsec:latest"
      }
    }
  ]
}
```

> Listing 56 - Viewing the oci-layout and image index

The **oci-layout** file contains a variable specifying which version of the image-layout the client should expect.

The **index.json** file contains more information. It holds the image index specification and points to the location of the image manifest. In this case, we have a single manifest, but a single image might contain multiple manifests if it supports multiple architectures such as ARM and x86. The manifest definition must have a _mediaType_ variable set to the expected content type. The _digest_ variable holds the hash indicating where we can find the manifest in the **blobs/sha256** directory. The manifest object also contains the reference name of the image. In this case, it's set to _localhost/offsec:latest_, but it might also be set to only the tag "latest", depending on the tool used to save the image.

Let's **cat** out the manifest file and pipe it into **jq**.

```
student@container:/home/student$ cat offsec-oci/blobs/sha256/a1aa1c7a7821f9be28c43c2bc75edaad4127818582158070b1fb29c89bf88fd1 | jq
{
  "schemaVersion": 2,
  "config": {
    "mediaType": "application/vnd.oci.image.config.v1+json",
    "digest": "sha256:b8f347291eb4767580fc3a31981aa4fcc74bb9e6d5fe118ab7bab90321f4f4a4",
    "size": 1123
  },
  "layers": [
    {
      "mediaType": "application/vnd.oci.image.layer.v1.tar+gzip",
      "digest": "sha256:ff309671352dd8bb4f487772b942ab34acb26811d2ab2b6fa659a9ef84b11cb4",
      "size": 2897375
    },
    {
      "mediaType": "application/vnd.oci.image.layer.v1.tar+gzip",
      "digest": "sha256:a994d323375dbc6794a995b61336887591741ecd6e87bdf7db3e7aed430e9538",
      "size": 130
    }
  ]
}
```

> Listing 57 - Reviewing the Image Manifest

While the index specification can point to multiple image manifests, a manifest contains the reference to the configuration and layers of a single image. In this case, the configuration is stored in another JSON file (according to the _mediaType_ variable) and the reference is once again a hash to the file. The layers, however, are stored in a tarball. Let's list the contents of the last layer and the configuration.

We won't extract the tarball, but we'll use **tar** to list its contents. We'll use **-z** to specify that it's a tarball compressed with GZIP, **-t** to list the contents, **-v** to be more verbose, and **-f** to use an archive file. Finally, we'll specify the location of the tarball.

```
student@container:/home/student$ tar -ztvf offsec-oci/blobs/sha256/a994d323375dbc6794a995b61336887591741ecd6e87bdf7db3e7aed430e9538
-rw-r--r-- root/root        15 2022-05-25 11:12 secret
```

> Listing 58 - Listing the Tarball

In this layer, we'll find the secret file that was added by the **Dockerfile** we reviewed earlier. As mentioned, layers only contain the files changed from the base. In this case, the **secret** file was added to the root.

Next, let's review the configuration.

```
student@container:/home/student$ cat offsec-oci/blobs/sha256/b8f347291eb4767580fc3a31981aa4fcc74bb9e6d5fe118ab7bab90321f4f4a4 | jq
{
  "created": "2022-05-25T18:23:54.025683481Z",
  "architecture": "amd64",
  "os": "linux",
  "config": {
    "Env": [
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
    ],
    "Entrypoint": [
      "cat",
      "/secret"
    ],
    "Labels": {
      "io.buildah.version": "1.23.1"
    }
  },
  "rootfs": {
    "type": "layers",
    "diff_ids": [
      "sha256:24302eb7d9085da80f016e7e4ae55417e412fb7e0a8021e95e3b60c67cde557d",
      "sha256:1e7669f5f765fc2f0bd66998b7487885b6e7804ccd461b996348d40bd09c3f0f"
    ]
  },
  "history": [
    {
      "created": "2022-05-23T19:19:30.413290187Z",
      "created_by": "/bin/sh -c #(nop) ADD file:8e81116368669ed3dd361bc898d61bff249f524139a239fdaf3ec46869a39921 in / "
    },
    {
      "created": "2022-05-23T19:19:31.970967174Z",
      "created_by": "/bin/sh -c #(nop)  CMD [\"/bin/sh\"]",
      "empty_layer": true
    },
    {
      "created": "2022-05-25T18:23:53.730856385Z",
      "created_by": "/bin/sh -c #(nop) COPY file:47f928a6dab65c3eb0a80bb865a188f6dd6eb9909201c0f5c4ffefb0b84ad796 in /secret ",
      "comment": "FROM docker.io/library/alpine@sha256:4ff3ca91275773af45cb4b0834e12b7eb47d1c18f770a0b151381cd227f4c253"
    },
    {
      "created": "2022-05-25T18:23:54.026254451Z",
      "created_by": "/bin/sh -c #(nop) ENTRYPOINT [\"cat\", \"/secret\"]",
      "empty_layer": true
    }
  ]
}
```

> Listing 59 - OCI Configuration

The configuration reveals several items of interest to us. First, we find the entrypoint, which indicates that we want to run **cat /secret** when the container starts. Then we'll find an array of the history leading to the image's current state. The first entry adds the contents of the _alpine_ distribution to the root directory. Next, the _CMD_ is set to **/bin/sh**. Finally, we'll find our changes to the Alpine image, including the addition of the **secret** file and the entrypoint update.

Together, the manifest, layers, and configuration comprise an OCI image. These images can be saved, shared, and executed.

The container image is just one part of the OCI specification, which also covers how the container should be run. In order for the container to run, we'll need to convert the image into a bundle. Let's investigate that in the next section.

1

(Containers, 2022), [https://github.com/containers/skopeo](https://github.com/containers/skopeo) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_627-1)

2

(Open Container Initiative, 2022), [https://github.com/opencontainers/image-spec/blob/main/spec.md](https://github.com/opencontainers/image-spec/blob/main/spec.md) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_627-2)

## Resources

Some of the labs require you to start the target machine(s) below.

Please note that the IP addresses assigned to your target machines may not match those referenced in the Module text and video.

containers

#### Labs

1. Inspect the image named _customhttpd_ in Podman to discover the flag.

Answer

2. Inspect the image named _ubuntu:zsh_ in Podman to discover the flag.

Answer

3. Inspect the image named _customcurl_ in Podman to discover the flag.

Answer

## 2.2.2. OCI Runtime Specification

Now that we understand the container image specification, let's review the runtime specification. We know that we can use OCI-compliant tools to create OCI images. In this section, we'll manually run the container using an OCI-compliant container runtime, _runc_.[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_631-1) This is the same runtime that Docker uses and can also be used by Podman.

There are three parts of the runtime specification that we need to closely review. The first is the _bundle_ that contains the filesystem and configuration used to run the container. The next is the _configuration_ in the bundle, which specifies the entrypoint, mounts, namespaces, capabilities, etc. The final part is the _lifecycle_ of the containers from start to deletion.

Let's start by first creating a bundle of the offsec image we've been working with so far. We'll use **umoci**[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_631-2) with the **unpack** command. We can specify the image location with the **--image** argument and set its value to the **/home/student/offsec-oci** directory that we created when we reviewed the image specification. We'll also have to specify the tag we want to use. While we should just be able to reference the "latest" tag, because the OCI specification doesn't define how to parse the tags, we need to specify the full reference name of **localhost/offsec:latest**. Finally, we need to specify where we want to save the bundle, which, in this case, is the **offsec-bundle** directory. We'll run all of this with **sudo**. Once we create the bundle, we'll list the directory to find its contents.

```
student@container:/home/student$ sudo umoci unpack --image /home/student/offsec-oci:localhost/offsec:latest offsec-bundle

student@container:/home/student$ sudo ls -alh offsec-bundle/
total 72K
drwx------  3 root    root    4.0K May 26 11:52 .
drwxr-xr-x 14 student student 4.0K May 26 11:52 ..
-rw-r--r--  1 root    root    3.1K May 26 11:52 config.json
drwxr-xr-x 19 root    root    4.0K Dec 31  1969 rootfs
-rw-r--r--  1 root    root     52K May 26 11:52 sha256_a1aa1c7a7821f9be28c43c2bc75edaad4127818582158070b1fb29c89bf88fd1.mtree
-rw-r--r--  1 root    root     389 May 26 11:52 umoci.json
```

> Listing 60 - Creating a Bundle with Umoci

Now that we've created a bundle and listed its contents, we can review what was created. There are three files and one directory in the bundle. The **config.json** file and **rootfs** directory are most interesting to us. The configuration for the container is found in **config.json**, and the root of the filesystem is found in **rootfs**. If this is the root directory, we should find the **secret** file.

```
student@container:/home/student$ sudo ls -alh offsec-bundle/rootfs
total 80K
drwxr-xr-x 19 root root 4.0K Dec 31  1969 .
drwx------  3 root root 4.0K May 26 11:52 ..
drwxr-xr-x  2 root root 4.0K May 23 09:51 bin
drwxr-xr-x  2 root root 4.0K May 23 09:51 dev
drwxr-xr-x 16 root root 4.0K May 23 09:51 etc
drwxr-xr-x  2 root root 4.0K May 23 09:51 home
drwxr-xr-x  7 root root 4.0K May 23 09:51 lib
drwxr-xr-x  5 root root 4.0K May 23 09:51 media
drwxr-xr-x  2 root root 4.0K May 23 09:51 mnt
drwxr-xr-x  2 root root 4.0K May 23 09:51 opt
dr-xr-xr-x  2 root root 4.0K May 23 09:51 proc
drwx------  2 root root 4.0K May 23 09:51 root
drwxr-xr-x  2 root root 4.0K May 23 09:51 run
drwxr-xr-x  2 root root 4.0K May 23 09:51 sbin
-rw-r--r--  1 root root   15 May 25 11:12 secret
drwxr-xr-x  2 root root 4.0K May 23 09:51 srv
drwxr-xr-x  2 root root 4.0K May 23 09:51 sys
drwxrwxrwt  2 root root 4.0K May 23 09:51 tmp
drwxr-xr-x  7 root root 4.0K May 23 09:51 usr
drwxr-xr-x 12 root root 4.0K May 23 09:51 var

student@container:/home/student$ sudo cat offsec-bundle/rootfs/secret
Hack the cloud!
```

> Listing 61 - Listing the Bundle File System

We find the **secret** file and its contents match what we expected. Next, let's review the configuration file.

```
student@container:/home/student$ sudo cat offsec-bundle/config.json | jq
{
  "ociVersion": "1.0.0",
  "process": {
    "terminal": true,
    "user": {...},
    "args": [ "cat", "/secret" ],
    "env": [
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
      "TERM=xterm",
      "HOME=/root"
    ],
    "cwd": "/",
    "capabilities": {
      "bounding": [
        "CAP_AUDIT_WRITE",
        "CAP_KILL",
        "CAP_NET_BIND_SERVICE"
      ],...
    },...
  },
  "root": {
    "path": "rootfs"
  },
  "hostname": "umoci-default",
  "mounts": [
    {
      "destination": "/proc",
      "type": "proc",
      "source": "proc"
    },...
  ],
...
  "linux": {
...
    "namespaces": [
      { "type": "pid" },
      { "type": "network" },
      { "type": "ipc" },
      { "type": "uts" },
      { "type": "mount" }
    ],...
  }
}
```

> Listing 62 - Reviewing the Configurations JSON file

This configuration file is fairly long, so we'll focus on the most noteworthy sections. When umoci unpacks the container image, it will take the configuration specified (the entrypoint, for example) and merge it with a set of defaults to generate the configuration file. The configuration, therefore, contains the _args_ variable, which is set to the entrypoint that cats out the secret file.

We'll also encounter the configuration for the environment variables that can be set in a **Dockerfile** when a container is built. In addition to entrypoint and environment variables, we'll find a set of capabilities that can be set when running the container in Docker and Podman.

In this configuration file, we can also find the root of the container, which is indeed the **rootfs** directory we reviewed previously. Next, we'll spot a list of mounts that are configured when the container is started. As expected, we'll find **/proc** mounted since it's required for a container to run properly. Finally, we'll also notice a list of namespaces to be created when running this container.

To summarize, umoci has configured this bundle so that runc can parse and run it. Traditionally, all the settings covered here can be set when a container is started with Docker or Podman. In the background, Podman or Docker will create a bundle, then execute it with its container runtime.

However, starting a container involves multiple steps called the container lifecycle.

![[OffSec/Cloud/Cloud Administration Skill Path/z. images/abfd918a73d8e0cfa538c1d6c3082a5d_MD5.jpg]]

Figure 2: OCI Container Lifecycle

The container lifecycle includes four official statuses:

1. **Creating**: the environment is in the process of being created based on the **config.json**
2. **Created**: the environment is created, but the container process (entrypoint) has not been executed
3. **Running**: the container process has been executed, but has not completed
4. **Stopped**: the container processes has exited

The steps in the lifecycle are progressed by running the _create_, _start_, and _stop_ commands. Between the steps of the container lifecycle, we can use _hooks_,[3](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_631-3) which allow the runtime to inject certain functions to make specific configurations at various times of the lifecycle. For example, the network namespace might be specified during the _CreateRuntime_ hook,[4](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_631-4) or the _pivot_root_ system call might be called in the _CreateContainer_ hook.[5](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_631-5) A runtime might include additional states, but these four states are those defined by the OCI specification.

Now that we understand the lifecycle, let's run the container. We'll supply the **run** command to **runc** and specify the root of the bundle directory with **-b offsec-bundle/**. Finally, we'll give the container the name **offsec**.

```
student@container:/home/student$ sudo runc run -b offsec-bundle/ offsec
Hack the cloud!
```

> Listing 63 - Starting a Container using Runc

As expected, the container ran and executed the entrypoint. During this short process, runc ran the container through the entire container lifecycle. Next, let's change **config.json** to run a _sleep_ command for 30 seconds rather than simply cat out the file. This will allow us to step through the lifecycle of the container during its various states.

To accomplish this, we'll use **nano** to edit **config.json**. We'll need to change two sections. First, we'll set the _terminal_ variable to _false_ to create the container without having to specify a terminal. Next, we'll change the _args_ variable to _[ "sleep", "30" ]_ to run the sleep command for 30 seconds.

```
student@container:/home/student$ sudo nano offsec-bundle/config.json

student@container:/home/student$ sudo cat offsec-bundle/config.json | grep -E "args|terminal" 
                "terminal": false,
                "args": [ "sleep", "30" ],
```

> Listing 64 - Changing config.json

With these changes in place, we can create the container using the **create** command, specifying the bundle again. After it creates the container, we'll **list** it.

```
student@container:/home/student$ sudo runc create -b offsec-bundle/ offsec

student@container:/home/student$ sudo runc list
ID          PID         STATUS      BUNDLE
offsec      15955       created     /home/student/offsec-bundle
```

> Listing 65 - Creating and Listing runc Container

When we create the container and list the containers, we'll find a single entry with the status _created_, indicating this container is in the second stage of the lifecycle. We missed the _creating_ stage because runc was able to create the environment for the container very quickly.

Next, let's start the container with the **start** command and specify the **offsec** ID. We'll quickly run another **list** command afterwards.

```
student@container:/home/student$ sudo runc start offsec

student@container:/home/student$ sudo runc list
ID          PID         STATUS      BUNDLE
offsec      15955       running     /home/student/offsec-bundle
```

> Listing 66 - Starting a Container with runc

If the list command was executed within 30 seconds of the container's sleep command, we should find that the status of the container is _running_, the third step of the lifecycle.

After 30 seconds, we'll run the **list** command again to review the final stage.

```
student@container:/home/student$ sudo runc list
ID          PID         STATUS      BUNDLE
offsec      0           stopped     /home/student/offsec-bundle
```

> Listing 67 - Listing Containers in Runc

Thirty seconds later, the sleep command has completed and exited. This means that the container is now in the final stage of the lifecycle, _stopped_. Finally, we'll run the **delete** command to delete the container. There is no deleted stage, so the **list** command output should be empty.

```
student@container:/home/student$ sudo runc delete offsec

student@container:/home/student$ sudo runc list
ID          PID         STATUS      BUNDLE
```

> Listing 68 - Deleting Container in Runc

At this point, we've explored the various OCI lifecycle stages. Because of the OCI runtime specification, high-level container engines (like containerd, CRI-O, etc.) can replace the low-level container runtime with others that might fit their needs and the containers should still run. This interoperability allows for much more flexibility.

1

(Open Container Initiative , 2022), [https://github.com/opencontainers/runc](https://github.com/opencontainers/runc) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_631-1)

2

(Aleksa Sarai et al., 2016), [https://umo.ci/](https://umo.ci/) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_631-2)

3

(Open Container Initiative, 2022), [https://github.com/opencontainers/runtime-spec/blob/main/config.md#posix-platform-hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#posix-platform-hooks) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_631-3)

4

(Open Container Initiative, 2022), [https://github.com/opencontainers/runtime-spec/blob/main/config.md#createRuntime-hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#createRuntime-hooks) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_631-4)

5

(Open Container Initiative, 2022), [https://github.com/opencontainers/runtime-spec/blob/main/config.md#createcontainer-hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#createcontainer-hooks) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_631-5)

#### Labs

1. What is the lifecycle status where the container environment was created but the entrypoint has not been executed?

Answer

2. What is the name for the default container runtime for Docker?

Answer

3. What is name of the directory that contains the filesystem and the configuration that runc uses to run the container?

Answer

## 2.2.3. OCI Distribution Specification

The last specification of the Open Container Initiative is the _distribution_ specification, which describes how the registries for container images should respond. This is the newest of the specifications and subject to more changes than the others.

The distribution specification was originally based on the _Docker Registry HTTP API V2_.[1](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_639-1) As with the others, Docker has been closely involved in building this specification as well.

The specification defines that there are four requirements[2](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fn-local_id_639-2) for a registry to conform to:

1. **Pull**: Clients are able to pull from the registry
2. **Push**: Clients are able to push to the registry
3. **Content Discovery**: Clients are able to list or otherwise query the content stored in the registry
4. **Content Management**: Clients are able to control the full lifecycle of the content stored in the registry

Since the OCI specification also defines the image format, this image format can be closely coupled with the registry specification, allowing for API requests to be developed using known standards. For example, the registry specification defines that we can download only the manifest, specific blobs, or the entire image by chaining multiple requests. Let's explore the requests that a client might make to download a container image.

The first step is to download the image manifest. As mentioned, the manifest document holds the content-addressable files within an image. This is where we will find the configuration and layer blobs. The request is a GET sent to **https://<registry-url>/v2/<name>/manifests/<reference>**.

The **<registery-url>** portion points to an OCI-compliant registry. The **<name>** refers to the name of the image we're attempting to download. In the Dockerhub, this might be **library/ubuntu** for Ubuntu or something like **kalilinux/kali-rolling** for the Kali image. The **<reference>** can be a tag or a digest in the form of a hash that points to the specific image. For example, at the time of this writing, the latest kali-rolling digest is "sha256:9636f960465f8f3132d9d0efc95eb0ed6fe21d0bbc75b494bb2b747de5f770bb".

A request to **https://<registry-url>/v2/kalilinux/kali-rolling/manifests/sha256:9636f960465f8f3132d9d0efc95eb0ed6fe21d0bbc75b494bb2b747de5f770bb** might result in the following manifest:

```
{
  "schemaVersion": 2,
  "config": {
    "mediaType": "application/vnd.oci.image.config.v1+json",
    "digest": "sha256:275cb1bbc3f2ce36c96d7a3db49da3a430ff9d8f86218773f41fbd14f287eb96",
    "size": 2702
  },
  "layers": [
    {
      "mediaType": "application/vnd.oci.image.layer.v1.tar+gzip",
      "digest": "sha256:b84b13183d6eb598dd5e7344d061d96a660139ec220e0be3e87932ef9152e2ec",
      "size": 53467769
    }
  ]
}
```

> Listing 69 - Example Manifest for Kali

The examples displayed are slightly modified for simplicity and explanation.

In the manifest, we'll find a _config_ object and a _layers_ array. As we found in the image specification, container image content is stored in blobs that are accessed with the digest. The request to download a blob is as follows: **https://<registry-url>/v2/<name>/blobs/<digest>**. This means that to download the configuration, our request would be **https://<registry-url>/v2/kalilinux/kali-rolling/blobs/sha256:275cb1bbc3f2ce36c96d7a3db49da3a430ff9d8f86218773f41fbd14f287eb96** and the response would contain:

```
{
  "created": "2022-05-23T12:44:32.743647765+07:00",
  "architecture": "amd64",
  "os": "linux",
  "config": {
    "Env": [
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
    ],
    "Cmd": [
      "bash"
    ],
...
  },
...
}
```

> Listing 70 - Mock Configuration for Image

To download a layer, a similar request would be sent using the digest of the layer. If multiple layers are needed, multiple requests would be sent. From this point, the client should have everything needed to run the image.

In addition to pulling, many other functions are defined in the OCI specification. Below is a table from the OCI specification that shows the various endpoints and response codes.

|Method|API Endpoint|Success|Failure|
|---|---|---|---|
|GET|/v2/|200|404/401|
|GET / HEAD|/v2/<name>/blobs/<digest>|200|404|
|GET / HEAD|/v2/<name>/manifests/<reference>|200|404|
|POST|/v2/<name>/blobs/uploads/|202|404|
|POST|/v2/<name>/blobs/uploads/?digest=<digest>|201/202|404/400|
|PATCH|/v2/<name>/blobs/uploads/<reference>|202|404/416|
|PUT|/v2/<name>/blobs/uploads/<reference>?digest=<digest>|201|404/400|
|PUT|/v2/<name>/manifests/<reference>|201|404|
|GET|/v2/<name>/tags/list|200|404|
|GET|/v2/<name>/tags/list?n=<integer>&last=<integer>|200|404|
|DELETE|/v2/<name>/manifests/<reference>|202|404/400/405|
|DELETE|/v2/<name>/blobs/<digest>|202|404/405|
|POST|/v2/<name>/blobs/uploads/?mount=<digest>&from=<other_name>|201|404|

> Table 1 - OCI Distribution Endpoints

As the table shows, the OCI specification does not specify endpoints for authentication. The only comment that OCI makes about authentication is that:

[The /v2/] endpoint MAY be used for authentication/authorization purposes, but this is out of the purview of this specification.

It is up to each project to implement an effective, supported authentication mechanism.

The three specifications created by the OCI allow for a heavily-standardized container industry. This standardization will increase reliability, security, and development speed.

1

(Docker, 2017), [https://github.com/distribution/distribution/blob/5cb406d511b7b9163bff9b6439072e4892e5ae3b/docs/spec/api.md](https://github.com/distribution/distribution/blob/5cb406d511b7b9163bff9b6439072e4892e5ae3b/docs/spec/api.md) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_639-1)

2

(Open Container Initiative, 2022), [https://github.com/opencontainers/distribution-spec/blob/main/spec.md#requirements](https://github.com/opencontainers/distribution-spec/blob/main/spec.md#requirements) [↩︎](https://portal.offsec.com/learning-paths/cloud-administration-skill-path-178607/learning/containers-for-cloud-ii-39989/open-container-initiative-40035/oci-distribution-specification-39996#fnref-local_id_639-2)

#### Labs

1. Which of the OCI requirements allows for a client to list or query the content stored in the registry?

Answer

2. What is the unique identifier called that contains a hash that points to a blob?

Answer

3. True of False: The OCI distribution specification requires authentication and authorization on the registry.